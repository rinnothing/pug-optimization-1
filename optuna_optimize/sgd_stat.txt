C:\Users\rinnothing\Code\pug-optimization-1\.venv\Scripts\python.exe C:\Users\rinnothing\Code\pug-optimization-1\optuna_optimize\sgd_optuna.py
[I 2025-05-28 23:47:25,517] A new study created in memory with name: no-name-57ae6c73-a9aa-401a-9412-0156b44aedae
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    160.0 MiB    160.0 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    160.0 MiB      0.0 MiB           1           res = common.StateResult()
    27    160.0 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    160.0 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    160.0 MiB      0.0 MiB           1           count = 0
    31    160.0 MiB    -11.9 MiB         685           while (not self.stop(res) or min_count > count) and max_count > count:
    32    160.0 MiB    -11.9 MiB         684               if self.batch != 0:
    33    160.0 MiB    -11.9 MiB         684                   ran = get_n_random(self.batch, len(x))
    34    160.0 MiB    -11.9 MiB         684                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    160.0 MiB    -11.9 MiB         684               grad = 0
    40    160.0 MiB   -118.7 MiB        6840               for n in ran:
    41                                                         # just like a normal derivative
    42    160.0 MiB   -106.8 MiB        6156                   grad += self.loss_der(self.weights, x[n], y[n])
    43    160.0 MiB    -11.9 MiB         684               grad /= size
    44
    45    160.0 MiB   -928.4 MiB       53828               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    160.0 MiB   -916.6 MiB       53144                   ans = 0
    48    160.0 MiB  -9165.6 MiB      531440                   for n in ran:
    49    160.0 MiB  -8249.1 MiB      478296                       ans += self.loss(val, x[n], y[n])
    50    160.0 MiB   -916.6 MiB       53144                   ans /= size
    51
    52    160.0 MiB   -916.6 MiB       53144                   return ans
    53
    54    160.0 MiB   -928.4 MiB       53828               def optim_der(val):
    55                                                         # the same as upper
    56    160.0 MiB   -916.5 MiB       53144                   ans = 0
    57    160.0 MiB  -9164.9 MiB      531440                   for n in ran:
    58    160.0 MiB  -8248.3 MiB      478296                       ans += self.loss_der(val, x[n], y[n])
    59    160.0 MiB   -916.5 MiB       53144                   ans /= size
    60
    61    160.0 MiB   -916.5 MiB       53144                   return ans
    62
    63    160.0 MiB    -11.9 MiB         684               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    160.0 MiB    -11.9 MiB         684               res.count_of_function_calls += preres.count_call_func * size
    65    160.0 MiB    -11.9 MiB         684               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    160.0 MiB    -11.9 MiB         684               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    160.0 MiB    -11.9 MiB         684               self.weights = preres.res
    69    160.0 MiB    -11.9 MiB         684               res.add_guess(self.weights)
    70    160.0 MiB    -11.9 MiB         684               count += 1
    71
    72    160.0 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    160.0 MiB      0.0 MiB           1           res.success = True
    74    160.0 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:49:08,805] Trial 0 finished with value: 167806551695.3519 and parameters: {'l_1': 0.0005473351128911744, 'l_2': 0.003747670735974398, 'batch': 9, 'min_count': 665}. Best is trial 0 with value: 167806551695.3519.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    160.0 MiB    160.0 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    160.0 MiB      0.0 MiB           1           res = common.StateResult()
    27    160.0 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    160.0 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    160.0 MiB      0.0 MiB           1           count = 0
    31    160.0 MiB    -14.5 MiB         566           while (not self.stop(res) or min_count > count) and max_count > count:
    32    160.0 MiB    -14.4 MiB         565               if self.batch != 0:
    33    160.0 MiB    -14.4 MiB         565                   ran = get_n_random(self.batch, len(x))
    34    160.0 MiB    -14.4 MiB         565                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    160.0 MiB    -14.4 MiB         565               grad = 0
    40    160.0 MiB   -129.9 MiB        5085               for n in ran:
    41                                                         # just like a normal derivative
    42    160.0 MiB   -115.4 MiB        4520                   grad += self.loss_der(self.weights, x[n], y[n])
    43    160.0 MiB    -14.4 MiB         565               grad /= size
    44
    45    160.0 MiB  -1139.1 MiB       44307               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    160.0 MiB  -1124.6 MiB       43742                   ans = 0
    48    160.0 MiB -10121.8 MiB      393678                   for n in ran:
    49    160.0 MiB  -8997.2 MiB      349936                       ans += self.loss(val, x[n], y[n])
    50    160.0 MiB  -1124.7 MiB       43742                   ans /= size
    51
    52    160.0 MiB  -1124.7 MiB       43742                   return ans
    53
    54    160.0 MiB  -1139.1 MiB       44307               def optim_der(val):
    55                                                         # the same as upper
    56    160.0 MiB  -1124.7 MiB       43742                   ans = 0
    57    160.0 MiB -11154.7 MiB      393678                   for n in ran:
    58    160.0 MiB -10030.0 MiB      349936                       ans += self.loss_der(val, x[n], y[n])
    59    160.0 MiB  -1253.8 MiB       43742                   ans /= size
    60
    61    160.0 MiB  -1124.7 MiB       43742                   return ans
    62
    63    160.0 MiB    -14.5 MiB         565               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    160.0 MiB    -14.5 MiB         565               res.count_of_function_calls += preres.count_call_func * size
    65    160.0 MiB    -14.5 MiB         565               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    160.0 MiB    -14.5 MiB         565               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    160.0 MiB    -14.5 MiB         565               self.weights = preres.res
    69    160.0 MiB    -14.5 MiB         565               res.add_guess(self.weights)
    70    160.0 MiB    -14.5 MiB         565               count += 1
    71
    72    160.0 MiB     -0.0 MiB           1           res.add_guess(self.weights)
    73    160.0 MiB      0.0 MiB           1           res.success = True
    74    160.0 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:50:24,885] Trial 1 finished with value: 71651740789.10986 and parameters: {'l_1': 0.00018600387314587186, 'l_2': 0.006172609245271575, 'batch': 8, 'min_count': 564}. Best is trial 1 with value: 71651740789.10986.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    160.0 MiB    160.0 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    160.0 MiB      0.0 MiB           1           res = common.StateResult()
    27    160.0 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    160.0 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    160.0 MiB      0.0 MiB           1           count = 0
    31    160.0 MiB      0.0 MiB         155           while (not self.stop(res) or min_count > count) and max_count > count:
    32    160.0 MiB      0.0 MiB         154               if self.batch != 0:
    33    160.0 MiB      0.0 MiB         154                   ran = get_n_random(self.batch, len(x))
    34    160.0 MiB      0.0 MiB         154                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    160.0 MiB      0.0 MiB         154               grad = 0
    40    160.0 MiB      0.0 MiB         616               for n in ran:
    41                                                         # just like a normal derivative
    42    160.0 MiB      0.0 MiB         462                   grad += self.loss_der(self.weights, x[n], y[n])
    43    160.0 MiB      0.0 MiB         154               grad /= size
    44
    45    160.0 MiB      0.0 MiB       11890               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    160.0 MiB      0.0 MiB       11736                   ans = 0
    48    160.0 MiB      0.0 MiB       46944                   for n in ran:
    49    160.0 MiB      0.0 MiB       35208                       ans += self.loss(val, x[n], y[n])
    50    160.0 MiB      0.0 MiB       11736                   ans /= size
    51
    52    160.0 MiB      0.0 MiB       11736                   return ans
    53
    54    160.0 MiB      0.0 MiB       11890               def optim_der(val):
    55                                                         # the same as upper
    56    160.0 MiB      0.0 MiB       11736                   ans = 0
    57    160.0 MiB      0.0 MiB       46944                   for n in ran:
    58    160.0 MiB      0.0 MiB       35208                       ans += self.loss_der(val, x[n], y[n])
    59    160.0 MiB      0.0 MiB       11736                   ans /= size
    60
    61    160.0 MiB      0.0 MiB       11736                   return ans
    62
    63    160.0 MiB      0.0 MiB         154               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    160.0 MiB      0.0 MiB         154               res.count_of_function_calls += preres.count_call_func * size
    65    160.0 MiB      0.0 MiB         154               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    160.0 MiB      0.0 MiB         154               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    160.0 MiB      0.0 MiB         154               self.weights = preres.res
    69    160.0 MiB      0.0 MiB         154               res.add_guess(self.weights)
    70    160.0 MiB      0.0 MiB         154               count += 1
    71
    72    160.0 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    160.0 MiB      0.0 MiB           1           res.success = True
    74    160.0 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:50:34,037] Trial 2 finished with value: 17196941694.60722 and parameters: {'l_1': 0.006238761061037289, 'l_2': 0.004881910980647324, 'batch': 3, 'min_count': 128}. Best is trial 2 with value: 17196941694.60722.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    160.0 MiB    160.0 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    160.0 MiB      0.0 MiB           1           res = common.StateResult()
    27    160.0 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    160.0 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    160.0 MiB      0.0 MiB           1           count = 0
    31    160.0 MiB  -1065.0 MiB         922           while (not self.stop(res) or min_count > count) and max_count > count:
    32    160.0 MiB  -1062.5 MiB         921               if self.batch != 0:
    33    160.0 MiB  -1062.5 MiB         921                   ran = get_n_random(self.batch, len(x))
    34    160.0 MiB  -1062.5 MiB         921                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    160.0 MiB  -1062.5 MiB         921               grad = 0
    40    160.0 MiB -10625.2 MiB        9210               for n in ran:
    41                                                         # just like a normal derivative
    42    160.0 MiB  -9562.6 MiB        8289                   grad += self.loss_der(self.weights, x[n], y[n])
    43    160.0 MiB  -1062.5 MiB         921               grad /= size
    44
    45    160.0 MiB -82941.0 MiB       72267               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    160.0 MiB -81878.5 MiB       71346                   ans = 0
    48    160.0 MiB -818793.1 MiB      713460                   for n in ran:
    49    160.0 MiB -736914.4 MiB      642114                       ans += self.loss(val, x[n], y[n])
    50    160.0 MiB -81880.0 MiB       71346                   ans /= size
    51
    52    160.0 MiB -81880.0 MiB       71346                   return ans
    53
    54    160.0 MiB -82944.0 MiB       72267               def optim_der(val):
    55                                                         # the same as upper
    56    160.0 MiB -81881.4 MiB       71346                   ans = 0
    57    160.0 MiB -818820.5 MiB      713460                   for n in ran:
    58    160.0 MiB -736938.9 MiB      642114                       ans += self.loss_der(val, x[n], y[n])
    59    160.0 MiB -81882.4 MiB       71346                   ans /= size
    60
    61    160.0 MiB -81882.4 MiB       71346                   return ans
    62
    63    160.0 MiB  -1065.0 MiB         921               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    160.0 MiB  -1065.0 MiB         921               res.count_of_function_calls += preres.count_call_func * size
    65    160.0 MiB  -1065.0 MiB         921               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    160.0 MiB  -1065.0 MiB         921               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    160.0 MiB  -1065.0 MiB         921               self.weights = preres.res
    69    160.0 MiB  -1065.0 MiB         921               res.add_guess(self.weights)
    70    160.0 MiB  -1065.0 MiB         921               count += 1
    71
    72    157.5 MiB     -2.5 MiB           1           res.add_guess(self.weights)
    73    157.5 MiB      0.0 MiB           1           res.success = True
    74    157.5 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:52:54,504] Trial 3 finished with value: 127674379291.53891 and parameters: {'l_1': 0.005411397269942114, 'l_2': 0.0010400794494190226, 'batch': 9, 'min_count': 917}. Best is trial 2 with value: 17196941694.60722.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    157.5 MiB    157.5 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    157.5 MiB      0.0 MiB           1           res = common.StateResult()
    27    157.5 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    157.5 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    157.5 MiB      0.0 MiB           1           count = 0
    31    157.5 MiB      0.0 MiB         777           while (not self.stop(res) or min_count > count) and max_count > count:
    32    157.5 MiB      0.0 MiB         776               if self.batch != 0:
    33    157.5 MiB      0.0 MiB         776                   ran = get_n_random(self.batch, len(x))
    34    157.5 MiB      0.0 MiB         776                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    157.5 MiB      0.0 MiB         776               grad = 0
    40    157.5 MiB      0.0 MiB        4656               for n in ran:
    41                                                         # just like a normal derivative
    42    157.5 MiB      0.0 MiB        3880                   grad += self.loss_der(self.weights, x[n], y[n])
    43    157.5 MiB      0.0 MiB         776               grad /= size
    44
    45    157.5 MiB      0.0 MiB       60844               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    157.5 MiB      0.0 MiB       60068                   ans = 0
    48    157.5 MiB      0.0 MiB      360408                   for n in ran:
    49    157.5 MiB      0.0 MiB      300340                       ans += self.loss(val, x[n], y[n])
    50    157.5 MiB      0.0 MiB       60068                   ans /= size
    51
    52    157.5 MiB      0.0 MiB       60068                   return ans
    53
    54    157.5 MiB      0.0 MiB       60844               def optim_der(val):
    55                                                         # the same as upper
    56    157.5 MiB      0.0 MiB       60068                   ans = 0
    57    157.5 MiB      0.0 MiB      360408                   for n in ran:
    58    157.5 MiB      0.0 MiB      300340                       ans += self.loss_der(val, x[n], y[n])
    59    157.5 MiB      0.0 MiB       60068                   ans /= size
    60
    61    157.5 MiB      0.0 MiB       60068                   return ans
    62
    63    157.5 MiB      0.0 MiB         776               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    157.5 MiB      0.0 MiB         776               res.count_of_function_calls += preres.count_call_func * size
    65    157.5 MiB      0.0 MiB         776               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    157.5 MiB      0.0 MiB         776               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    157.5 MiB      0.0 MiB         776               self.weights = preres.res
    69    157.5 MiB      0.0 MiB         776               res.add_guess(self.weights)
    70    157.5 MiB      0.0 MiB         776               count += 1
    71
    72    157.5 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    157.5 MiB      0.0 MiB           1           res.success = True
    74    157.5 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:54:04,829] Trial 4 finished with value: 71727237284.15884 and parameters: {'l_1': 0.00036717709885319917, 'l_2': 0.0020657376489994425, 'batch': 5, 'min_count': 720}. Best is trial 2 with value: 17196941694.60722.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    157.5 MiB    157.5 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    157.5 MiB      0.0 MiB           1           res = common.StateResult()
    27    157.5 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    157.5 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    157.5 MiB      0.0 MiB           1           count = 0
    31    157.5 MiB    -13.2 MiB         695           while (not self.stop(res) or min_count > count) and max_count > count:
    32    157.5 MiB    -13.2 MiB         694               if self.batch != 0:
    33    157.5 MiB    -13.2 MiB         694                   ran = get_n_random(self.batch, len(x))
    34    157.5 MiB    -13.2 MiB         694                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    157.5 MiB    -13.2 MiB         694               grad = 0
    40    157.5 MiB    -52.7 MiB        2776               for n in ran:
    41                                                         # just like a normal derivative
    42    157.5 MiB    -39.6 MiB        2082                   grad += self.loss_der(self.weights, x[n], y[n])
    43    157.5 MiB    -13.2 MiB         694               grad /= size
    44
    45    157.5 MiB  -1026.4 MiB       54000               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    157.5 MiB  -1013.2 MiB       53306                   ans = 0
    48    157.5 MiB  -4052.8 MiB      213224                   for n in ran:
    49    157.5 MiB  -3039.6 MiB      159918                       ans += self.loss(val, x[n], y[n])
    50    157.5 MiB  -1013.2 MiB       53306                   ans /= size
    51
    52    157.5 MiB  -1013.2 MiB       53306                   return ans
    53
    54    157.5 MiB  -1026.4 MiB       54000               def optim_der(val):
    55                                                         # the same as upper
    56    157.5 MiB  -1013.2 MiB       53306                   ans = 0
    57    157.5 MiB  -4052.8 MiB      213224                   for n in ran:
    58    157.5 MiB  -3039.6 MiB      159918                       ans += self.loss_der(val, x[n], y[n])
    59    157.5 MiB  -1013.2 MiB       53306                   ans /= size
    60
    61    157.5 MiB  -1013.2 MiB       53306                   return ans
    62
    63    157.5 MiB    -13.2 MiB         694               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    157.5 MiB    -13.2 MiB         694               res.count_of_function_calls += preres.count_call_func * size
    65    157.5 MiB    -13.2 MiB         694               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    157.5 MiB    -13.2 MiB         694               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    157.5 MiB    -13.2 MiB         694               self.weights = preres.res
    69    157.5 MiB    -13.2 MiB         694               res.add_guess(self.weights)
    70    157.5 MiB    -13.2 MiB         694               count += 1
    71
    72    157.5 MiB     -0.0 MiB           1           res.add_guess(self.weights)
    73    157.5 MiB      0.0 MiB           1           res.success = True
    74    157.5 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:54:46,751] Trial 5 finished with value: 105534330945.35724 and parameters: {'l_1': 0.009140425259619537, 'l_2': 0.009005174910912982, 'batch': 3, 'min_count': 662}. Best is trial 2 with value: 17196941694.60722.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    157.5 MiB    157.5 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    157.5 MiB      0.0 MiB           1           res = common.StateResult()
    27    157.5 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    157.5 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    157.5 MiB      0.0 MiB           1           count = 0
    31    157.5 MiB      0.0 MiB         632           while (not self.stop(res) or min_count > count) and max_count > count:
    32    157.5 MiB      0.0 MiB         631               if self.batch != 0:
    33    157.5 MiB      0.0 MiB         631                   ran = get_n_random(self.batch, len(x))
    34    157.5 MiB      0.0 MiB         631                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    157.5 MiB      0.0 MiB         631               grad = 0
    40    157.5 MiB      0.0 MiB        6941               for n in ran:
    41                                                         # just like a normal derivative
    42    157.5 MiB      0.0 MiB        6310                   grad += self.loss_der(self.weights, x[n], y[n])
    43    157.5 MiB      0.0 MiB         631               grad /= size
    44
    45    157.5 MiB      0.0 MiB       49719               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    157.5 MiB      0.0 MiB       49088                   ans = 0
    48    157.5 MiB      0.0 MiB      539968                   for n in ran:
    49    157.5 MiB      0.0 MiB      490880                       ans += self.loss(val, x[n], y[n])
    50    157.5 MiB      0.0 MiB       49088                   ans /= size
    51
    52    157.5 MiB      0.0 MiB       49088                   return ans
    53
    54    157.5 MiB      0.0 MiB       49719               def optim_der(val):
    55                                                         # the same as upper
    56    157.5 MiB      0.0 MiB       49088                   ans = 0
    57    157.5 MiB      0.0 MiB      539968                   for n in ran:
    58    157.5 MiB      0.0 MiB      490880                       ans += self.loss_der(val, x[n], y[n])
    59    157.5 MiB      0.0 MiB       49088                   ans /= size
    60
    61    157.5 MiB      0.0 MiB       49088                   return ans
    62
    63    157.5 MiB      0.0 MiB         631               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    157.5 MiB      0.0 MiB         631               res.count_of_function_calls += preres.count_call_func * size
    65    157.5 MiB      0.0 MiB         631               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    157.5 MiB      0.0 MiB         631               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    157.5 MiB      0.0 MiB         631               self.weights = preres.res
    69    157.5 MiB      0.0 MiB         631               res.add_guess(self.weights)
    70    157.5 MiB      0.0 MiB         631               count += 1
    71
    72    157.5 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    157.5 MiB      0.0 MiB           1           res.success = True
    74    157.5 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:56:31,489] Trial 6 finished with value: 98844143957.2464 and parameters: {'l_1': 0.003336006857922154, 'l_2': 0.004181136633735538, 'batch': 10, 'min_count': 595}. Best is trial 2 with value: 17196941694.60722.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    157.5 MiB    157.5 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    157.5 MiB      0.0 MiB           1           res = common.StateResult()
    27    157.5 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    157.5 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    157.5 MiB      0.0 MiB           1           count = 0
    31    157.5 MiB      0.0 MiB         961           while (not self.stop(res) or min_count > count) and max_count > count:
    32    157.5 MiB      0.0 MiB         960               if self.batch != 0:
    33    157.5 MiB      0.0 MiB         960                   ran = get_n_random(self.batch, len(x))
    34    157.5 MiB      0.0 MiB         960                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    157.5 MiB      0.0 MiB         960               grad = 0
    40    157.5 MiB      0.0 MiB        1920               for n in ran:
    41                                                         # just like a normal derivative
    42    157.5 MiB      0.0 MiB         960                   grad += self.loss_der(self.weights, x[n], y[n])
    43    157.5 MiB      0.0 MiB         960               grad /= size
    44
    45    157.5 MiB      0.0 MiB       71544               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    157.5 MiB      0.0 MiB       70584                   ans = 0
    48    157.5 MiB      0.0 MiB      141168                   for n in ran:
    49    157.5 MiB      0.0 MiB       70584                       ans += self.loss(val, x[n], y[n])
    50    157.5 MiB      0.0 MiB       70584                   ans /= size
    51
    52    157.5 MiB      0.0 MiB       70584                   return ans
    53
    54    157.5 MiB      0.0 MiB       71544               def optim_der(val):
    55                                                         # the same as upper
    56    157.5 MiB      0.0 MiB       70584                   ans = 0
    57    157.5 MiB      0.0 MiB      141168                   for n in ran:
    58    157.5 MiB      0.0 MiB       70584                       ans += self.loss_der(val, x[n], y[n])
    59    157.5 MiB      0.0 MiB       70584                   ans /= size
    60
    61    157.5 MiB      0.0 MiB       70584                   return ans
    62
    63    157.5 MiB      0.0 MiB         960               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    157.5 MiB      0.0 MiB         960               res.count_of_function_calls += preres.count_call_func * size
    65    157.5 MiB      0.0 MiB         960               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    157.5 MiB      0.0 MiB         960               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    157.5 MiB      0.0 MiB         960               self.weights = preres.res
    69    157.5 MiB      0.0 MiB         960               res.add_guess(self.weights)
    70    157.5 MiB      0.0 MiB         960               count += 1
    71
    72    157.5 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    157.5 MiB      0.0 MiB           1           res.success = True
    74    157.5 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:57:02,527] Trial 7 finished with value: 10118637286.223488 and parameters: {'l_1': 0.0026501516412137612, 'l_2': 0.006106905272168856, 'batch': 1, 'min_count': 953}. Best is trial 7 with value: 10118637286.223488.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    157.5 MiB    157.5 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    157.5 MiB      0.0 MiB           1           res = common.StateResult()
    27    157.5 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    157.5 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    157.5 MiB      0.0 MiB           1           count = 0
    31    157.6 MiB      0.0 MiB         433           while (not self.stop(res) or min_count > count) and max_count > count:
    32    157.6 MiB      0.0 MiB         432               if self.batch != 0:
    33    157.6 MiB      0.0 MiB         432                   ran = get_n_random(self.batch, len(x))
    34    157.6 MiB      0.0 MiB         432                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    157.6 MiB      0.0 MiB         432               grad = 0
    40    157.6 MiB      0.0 MiB        2160               for n in ran:
    41                                                         # just like a normal derivative
    42    157.6 MiB      0.0 MiB        1728                   grad += self.loss_der(self.weights, x[n], y[n])
    43    157.6 MiB      0.0 MiB         432               grad /= size
    44
    45    157.6 MiB      0.0 MiB       33748               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    157.6 MiB      0.0 MiB       33316                   ans = 0
    48    157.6 MiB      0.0 MiB      166580                   for n in ran:
    49    157.6 MiB      0.1 MiB      133264                       ans += self.loss(val, x[n], y[n])
    50    157.6 MiB      0.0 MiB       33316                   ans /= size
    51
    52    157.6 MiB      0.0 MiB       33316                   return ans
    53
    54    157.6 MiB      0.0 MiB       33748               def optim_der(val):
    55                                                         # the same as upper
    56    157.6 MiB      0.0 MiB       33316                   ans = 0
    57    157.6 MiB      0.0 MiB      166580                   for n in ran:
    58    157.6 MiB      0.0 MiB      133264                       ans += self.loss_der(val, x[n], y[n])
    59    157.6 MiB      0.0 MiB       33316                   ans /= size
    60
    61    157.6 MiB      0.0 MiB       33316                   return ans
    62
    63    157.6 MiB      0.0 MiB         432               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    157.6 MiB      0.0 MiB         432               res.count_of_function_calls += preres.count_call_func * size
    65    157.6 MiB      0.0 MiB         432               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    157.6 MiB      0.0 MiB         432               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    157.6 MiB      0.0 MiB         432               self.weights = preres.res
    69    157.6 MiB      0.0 MiB         432               res.add_guess(self.weights)
    70    157.6 MiB      0.0 MiB         432               count += 1
    71
    72    157.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    157.6 MiB      0.0 MiB           1           res.success = True
    74    157.6 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:57:39,404] Trial 8 finished with value: 88967065127.73698 and parameters: {'l_1': 0.003917387124916364, 'l_2': 0.0035458283282134057, 'batch': 4, 'min_count': 371}. Best is trial 7 with value: 10118637286.223488.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    157.6 MiB    157.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    157.6 MiB      0.0 MiB           1           res = common.StateResult()
    27    157.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    157.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    157.6 MiB      0.0 MiB           1           count = 0
    31    157.6 MiB      0.0 MiB         201           while (not self.stop(res) or min_count > count) and max_count > count:
    32    157.6 MiB      0.0 MiB         200               if self.batch != 0:
    33    157.6 MiB      0.0 MiB         200                   ran = get_n_random(self.batch, len(x))
    34    157.6 MiB      0.0 MiB         200                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    157.6 MiB      0.0 MiB         200               grad = 0
    40    157.6 MiB      0.0 MiB        1200               for n in ran:
    41                                                         # just like a normal derivative
    42    157.6 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43    157.6 MiB      0.0 MiB         200               grad /= size
    44
    45    157.6 MiB      0.0 MiB       15596               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    157.6 MiB      0.0 MiB       15396                   ans = 0
    48    157.6 MiB      0.0 MiB       92376                   for n in ran:
    49    157.6 MiB      0.0 MiB       76980                       ans += self.loss(val, x[n], y[n])
    50    157.6 MiB      0.0 MiB       15396                   ans /= size
    51
    52    157.6 MiB      0.0 MiB       15396                   return ans
    53
    54    157.6 MiB      0.0 MiB       15596               def optim_der(val):
    55                                                         # the same as upper
    56    157.6 MiB      0.0 MiB       15396                   ans = 0
    57    157.6 MiB      0.0 MiB       92376                   for n in ran:
    58    157.6 MiB      0.0 MiB       76980                       ans += self.loss_der(val, x[n], y[n])
    59    157.6 MiB      0.0 MiB       15396                   ans /= size
    60
    61    157.6 MiB      0.0 MiB       15396                   return ans
    62
    63    157.6 MiB      0.0 MiB         200               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    157.6 MiB      0.0 MiB         200               res.count_of_function_calls += preres.count_call_func * size
    65    157.6 MiB      0.0 MiB         200               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    157.6 MiB      0.0 MiB         200               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    157.6 MiB      0.0 MiB         200               self.weights = preres.res
    69    157.6 MiB      0.0 MiB         200               res.add_guess(self.weights)
    70    157.6 MiB      0.0 MiB         200               count += 1
    71
    72    157.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    157.6 MiB      0.0 MiB           1           res.success = True
    74    157.6 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:57:58,115] Trial 9 finished with value: 15631060281.879269 and parameters: {'l_1': 0.0026489535808649463, 'l_2': 0.0030676555202107025, 'batch': 5, 'min_count': 156}. Best is trial 7 with value: 10118637286.223488.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.0 MiB    158.0 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.0 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.0 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.0 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.0 MiB      0.0 MiB           1           count = 0
    31    158.1 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.1 MiB      0.0 MiB        1000               if self.batch != 0:
    33    158.1 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34    158.1 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.1 MiB      0.0 MiB        1000               grad = 0
    40    158.1 MiB      0.0 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42    158.1 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.1 MiB      0.0 MiB        1000               grad /= size
    44
    45    158.1 MiB      0.0 MiB       75820               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.1 MiB      0.0 MiB       74820                   ans = 0
    48    158.1 MiB      0.0 MiB      149640                   for n in ran:
    49    158.1 MiB      0.1 MiB       74820                       ans += self.loss(val, x[n], y[n])
    50    158.1 MiB      0.0 MiB       74820                   ans /= size
    51
    52    158.1 MiB      0.0 MiB       74820                   return ans
    53
    54    158.1 MiB      0.0 MiB       75820               def optim_der(val):
    55                                                         # the same as upper
    56    158.1 MiB      0.0 MiB       74820                   ans = 0
    57    158.1 MiB      0.0 MiB      149640                   for n in ran:
    58    158.1 MiB      0.0 MiB       74820                       ans += self.loss_der(val, x[n], y[n])
    59    158.1 MiB      0.0 MiB       74820                   ans /= size
    60
    61    158.1 MiB      0.0 MiB       74820                   return ans
    62
    63    158.1 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.1 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65    158.1 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.1 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.1 MiB      0.0 MiB        1000               self.weights = preres.res
    69    158.1 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70    158.1 MiB      0.0 MiB        1000               count += 1
    71
    72    158.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.1 MiB      0.0 MiB           1           res.success = True
    74    158.1 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:58:30,318] Trial 10 finished with value: 289274947057.3993 and parameters: {'l_1': 0.007983853030862189, 'l_2': 0.007009230766373664, 'batch': 1, 'min_count': 998}. Best is trial 7 with value: 10118637286.223488.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.1 MiB    158.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.1 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.1 MiB      0.0 MiB           1           count = 0
    31    158.1 MiB     -6.8 MiB         363           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.1 MiB     -6.8 MiB         362               if self.batch != 0:
    33    158.1 MiB     -6.8 MiB         362                   ran = get_n_random(self.batch, len(x))
    34    158.1 MiB     -6.8 MiB         362                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.1 MiB     -6.8 MiB         362               grad = 0
    40    158.1 MiB    -13.5 MiB         724               for n in ran:
    41                                                         # just like a normal derivative
    42    158.1 MiB     -6.8 MiB         362                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.1 MiB     -6.8 MiB         362               grad /= size
    44
    45    158.1 MiB   -503.7 MiB       27222               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.1 MiB   -496.9 MiB       26860                   ans = 0
    48    158.1 MiB   -993.9 MiB       53720                   for n in ran:
    49    158.1 MiB   -496.9 MiB       26860                       ans += self.loss(val, x[n], y[n])
    50    158.1 MiB   -496.9 MiB       26860                   ans /= size
    51
    52    158.1 MiB   -496.9 MiB       26860                   return ans
    53
    54    158.1 MiB   -503.7 MiB       27222               def optim_der(val):
    55                                                         # the same as upper
    56    158.1 MiB   -497.0 MiB       26860                   ans = 0
    57    158.1 MiB   -994.0 MiB       53720                   for n in ran:
    58    158.1 MiB   -497.0 MiB       26860                       ans += self.loss_der(val, x[n], y[n])
    59    158.1 MiB   -497.0 MiB       26860                   ans /= size
    60
    61    158.1 MiB   -497.0 MiB       26860                   return ans
    62
    63    158.1 MiB     -6.8 MiB         362               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.1 MiB     -6.8 MiB         362               res.count_of_function_calls += preres.count_call_func * size
    65    158.1 MiB     -6.8 MiB         362               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.1 MiB     -6.8 MiB         362               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.1 MiB     -6.8 MiB         362               self.weights = preres.res
    69    158.1 MiB     -6.8 MiB         362               res.add_guess(self.weights)
    70    158.1 MiB     -6.8 MiB         362               count += 1
    71
    72    158.1 MiB     -0.0 MiB           1           res.add_guess(self.weights)
    73    158.1 MiB      0.0 MiB           1           res.success = True
    74    158.1 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:58:41,711] Trial 11 finished with value: 7984876739.980007 and parameters: {'l_1': 0.002514360056904314, 'l_2': 0.007573501413246397, 'batch': 1, 'min_count': 309}. Best is trial 11 with value: 7984876739.980007.
[I 2025-05-28 23:59:12,927] Trial 12 finished with value: 42179193193.59737 and parameters: {'l_1': 0.0020118178185297023, 'l_2': 0.008691065094431894, 'batch': 1, 'min_count': 349}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.1 MiB    158.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.1 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.1 MiB      0.0 MiB           1           count = 0
    31    158.1 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.1 MiB      0.0 MiB        1000               if self.batch != 0:
    33    158.1 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34    158.1 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.1 MiB      0.0 MiB        1000               grad = 0
    40    158.1 MiB      0.0 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42    158.1 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.1 MiB      0.0 MiB        1000               grad /= size
    44
    45    158.1 MiB      0.0 MiB       75112               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.1 MiB      0.0 MiB       74112                   ans = 0
    48    158.1 MiB      0.0 MiB      148224                   for n in ran:
    49    158.1 MiB      0.0 MiB       74112                       ans += self.loss(val, x[n], y[n])
    50    158.1 MiB      0.0 MiB       74112                   ans /= size
    51
    52    158.1 MiB      0.0 MiB       74112                   return ans
    53
    54    158.1 MiB      0.0 MiB       75112               def optim_der(val):
    55                                                         # the same as upper
    56    158.1 MiB      0.0 MiB       74112                   ans = 0
    57    158.1 MiB      0.0 MiB      148224                   for n in ran:
    58    158.1 MiB      0.0 MiB       74112                       ans += self.loss_der(val, x[n], y[n])
    59    158.1 MiB      0.0 MiB       74112                   ans /= size
    60
    61    158.1 MiB      0.0 MiB       74112                   return ans
    62
    63    158.1 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.1 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65    158.1 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.1 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.1 MiB      0.0 MiB        1000               self.weights = preres.res
    69    158.1 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70    158.1 MiB      0.0 MiB        1000               count += 1
    71
    72    158.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.1 MiB      0.0 MiB           1           res.success = True
    74    158.1 MiB      0.0 MiB           1           return res


Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.1 MiB    158.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.1 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.1 MiB      0.0 MiB           1           count = 0
    31    158.1 MiB      0.0 MiB         404           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.1 MiB      0.0 MiB         403               if self.batch != 0:
    33    158.1 MiB      0.0 MiB         403                   ran = get_n_random(self.batch, len(x))
    34    158.1 MiB      0.0 MiB         403                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.1 MiB      0.0 MiB         403               grad = 0
    40    158.1 MiB      0.0 MiB        1209               for n in ran:
    41                                                         # just like a normal derivative
    42    158.1 MiB      0.0 MiB         806                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.1 MiB      0.0 MiB         403               grad /= size
    44
    45    158.1 MiB      0.0 MiB       31503               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.1 MiB      0.0 MiB       31100                   ans = 0
    48    158.1 MiB      0.0 MiB       93300                   for n in ran:
    49    158.1 MiB      0.0 MiB       62200                       ans += self.loss(val, x[n], y[n])
    50    158.1 MiB      0.0 MiB       31100                   ans /= size
    51
    52    158.1 MiB      0.0 MiB       31100                   return ans
    53
    54    158.1 MiB      0.0 MiB       31503               def optim_der(val):
    55                                                         # the same as upper
    56    158.1 MiB      0.0 MiB       31100                   ans = 0
    57    158.1 MiB      0.0 MiB       93300                   for n in ran:
    58    158.1 MiB      0.0 MiB       62200                       ans += self.loss_der(val, x[n], y[n])
    59    158.1 MiB      0.0 MiB       31100                   ans /= size
    60
    61    158.1 MiB      0.0 MiB       31100                   return ans
    62
    63    158.1 MiB      0.0 MiB         403               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.1 MiB      0.0 MiB         403               res.count_of_function_calls += preres.count_call_func * size
    65    158.1 MiB      0.0 MiB         403               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.1 MiB      0.0 MiB         403               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.1 MiB      0.0 MiB         403               self.weights = preres.res
    69    158.1 MiB      0.0 MiB         403               res.add_guess(self.weights)
    70    158.1 MiB      0.0 MiB         403               count += 1
    71
    72    158.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.1 MiB      0.0 MiB           1           res.success = True
    74    158.1 MiB      0.0 MiB           1           return res


[I 2025-05-28 23:59:32,401] Trial 13 finished with value: 12547051118.48141 and parameters: {'l_1': 0.0018895777493837377, 'l_2': 0.006930256520331947, 'batch': 2, 'min_count': 388}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.1 MiB    158.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.1 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.1 MiB      0.0 MiB           1           count = 0
    31    158.1 MiB      0.0 MiB         831           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.1 MiB      0.0 MiB         830               if self.batch != 0:
    33    158.1 MiB      0.0 MiB         830                   ran = get_n_random(self.batch, len(x))
    34    158.1 MiB      0.0 MiB         830                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.1 MiB      0.0 MiB         830               grad = 0
    40    158.1 MiB      0.0 MiB        6640               for n in ran:
    41                                                         # just like a normal derivative
    42    158.1 MiB      0.0 MiB        5810                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.1 MiB      0.0 MiB         830               grad /= size
    44
    45    158.1 MiB      0.0 MiB       65230               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.1 MiB      0.0 MiB       64400                   ans = 0
    48    158.1 MiB      0.0 MiB      515200                   for n in ran:
    49    158.1 MiB      0.0 MiB      450800                       ans += self.loss(val, x[n], y[n])
    50    158.1 MiB      0.0 MiB       64400                   ans /= size
    51
    52    158.1 MiB      0.0 MiB       64400                   return ans
    53
    54    158.1 MiB      0.0 MiB       65230               def optim_der(val):
    55                                                         # the same as upper
    56    158.1 MiB      0.0 MiB       64400                   ans = 0
    57    158.1 MiB      0.0 MiB      515200                   for n in ran:
    58    158.1 MiB      0.0 MiB      450800                       ans += self.loss_der(val, x[n], y[n])
    59    158.1 MiB      0.0 MiB       64400                   ans /= size
    60
    61    158.1 MiB      0.0 MiB       64400                   return ans
    62
    63    158.1 MiB      0.0 MiB         830               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.1 MiB      0.0 MiB         830               res.count_of_function_calls += preres.count_call_func * size
    65    158.1 MiB      0.0 MiB         830               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.1 MiB      0.0 MiB         830               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.1 MiB      0.0 MiB         830               self.weights = preres.res
    69    158.1 MiB      0.0 MiB         830               res.add_guess(self.weights)
    70    158.1 MiB      0.0 MiB         830               count += 1
    71
    72    158.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.1 MiB      0.0 MiB           1           res.success = True
    74    158.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:01:18,149] Trial 14 finished with value: 98896064947.85947 and parameters: {'l_1': 0.004492105167424627, 'l_2': 0.00793991293563833, 'batch': 7, 'min_count': 809}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.1 MiB    158.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.1 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.1 MiB      0.0 MiB           1           count = 0
    31    158.1 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.1 MiB      0.0 MiB        1000               if self.batch != 0:
    33    158.1 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34    158.1 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.1 MiB      0.0 MiB        1000               grad = 0
    40    158.1 MiB      0.0 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42    158.1 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.1 MiB      0.0 MiB        1000               grad /= size
    44
    45    158.1 MiB      0.0 MiB       74136               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.1 MiB      0.0 MiB       73136                   ans = 0
    48    158.1 MiB      0.0 MiB      146272                   for n in ran:
    49    158.1 MiB      0.0 MiB       73136                       ans += self.loss(val, x[n], y[n])
    50    158.1 MiB      0.0 MiB       73136                   ans /= size
    51
    52    158.1 MiB      0.0 MiB       73136                   return ans
    53
    54    158.1 MiB      0.0 MiB       74136               def optim_der(val):
    55                                                         # the same as upper
    56    158.1 MiB      0.0 MiB       73136                   ans = 0
    57    158.1 MiB      0.0 MiB      146272                   for n in ran:
    58    158.1 MiB      0.0 MiB       73136                       ans += self.loss_der(val, x[n], y[n])
    59    158.1 MiB      0.0 MiB       73136                   ans /= size
    60
    61    158.1 MiB      0.0 MiB       73136                   return ans
    62
    63    158.1 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.1 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65    158.1 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.1 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.1 MiB      0.0 MiB        1000               self.weights = preres.res
    69    158.1 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70    158.1 MiB      0.0 MiB        1000               count += 1
    71
    72    158.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.1 MiB      0.0 MiB           1           res.success = True
    74    158.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:01:49,138] Trial 15 finished with value: 24934607136548.734 and parameters: {'l_1': 0.006277241838937433, 'l_2': 0.005831179478625005, 'batch': 1, 'min_count': 278}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.1 MiB    158.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.1 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.1 MiB      0.0 MiB           1           count = 0
    31    158.1 MiB      0.0 MiB         482           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.1 MiB      0.0 MiB         481               if self.batch != 0:
    33    158.1 MiB      0.0 MiB         481                   ran = get_n_random(self.batch, len(x))
    34    158.1 MiB      0.0 MiB         481                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.1 MiB      0.0 MiB         481               grad = 0
    40    158.1 MiB      0.0 MiB        1924               for n in ran:
    41                                                         # just like a normal derivative
    42    158.1 MiB      0.0 MiB        1443                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.1 MiB      0.0 MiB         481               grad /= size
    44
    45    158.1 MiB      0.0 MiB       37551               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.1 MiB      0.0 MiB       37070                   ans = 0
    48    158.1 MiB      0.0 MiB      148280                   for n in ran:
    49    158.1 MiB      0.0 MiB      111210                       ans += self.loss(val, x[n], y[n])
    50    158.1 MiB      0.0 MiB       37070                   ans /= size
    51
    52    158.1 MiB      0.0 MiB       37070                   return ans
    53
    54    158.1 MiB      0.0 MiB       37551               def optim_der(val):
    55                                                         # the same as upper
    56    158.1 MiB      0.0 MiB       37070                   ans = 0
    57    158.1 MiB      0.0 MiB      148280                   for n in ran:
    58    158.1 MiB      0.0 MiB      111210                       ans += self.loss_der(val, x[n], y[n])
    59    158.1 MiB      0.0 MiB       37070                   ans /= size
    60
    61    158.1 MiB      0.0 MiB       37070                   return ans
    62
    63    158.1 MiB      0.0 MiB         481               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.1 MiB      0.0 MiB         481               res.count_of_function_calls += preres.count_call_func * size
    65    158.1 MiB      0.0 MiB         481               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.1 MiB      0.0 MiB         481               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.1 MiB      0.0 MiB         481               self.weights = preres.res
    69    158.1 MiB      0.0 MiB         481               res.add_guess(self.weights)
    70    158.1 MiB      0.0 MiB         481               count += 1
    71
    72    158.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.1 MiB      0.0 MiB           1           res.success = True
    74    158.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:02:20,033] Trial 16 finished with value: 86261943133.34268 and parameters: {'l_1': 0.0020836818892618577, 'l_2': 0.009611180640018735, 'batch': 3, 'min_count': 467}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.1 MiB    158.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.1 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.1 MiB      0.0 MiB           1           count = 0
    31    158.1 MiB      0.0 MiB         247           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.1 MiB      0.0 MiB         246               if self.batch != 0:
    33    158.1 MiB      0.0 MiB         246                   ran = get_n_random(self.batch, len(x))
    34    158.1 MiB      0.0 MiB         246                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.1 MiB      0.0 MiB         246               grad = 0
    40    158.1 MiB      0.0 MiB        1722               for n in ran:
    41                                                         # just like a normal derivative
    42    158.1 MiB      0.0 MiB        1476                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.1 MiB      0.0 MiB         246               grad /= size
    44
    45    158.1 MiB      0.0 MiB       19444               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.1 MiB      0.0 MiB       19198                   ans = 0
    48    158.1 MiB      0.0 MiB      134386                   for n in ran:
    49    158.1 MiB      0.0 MiB      115188                       ans += self.loss(val, x[n], y[n])
    50    158.1 MiB      0.0 MiB       19198                   ans /= size
    51
    52    158.1 MiB      0.0 MiB       19198                   return ans
    53
    54    158.1 MiB      0.0 MiB       19444               def optim_der(val):
    55                                                         # the same as upper
    56    158.1 MiB      0.0 MiB       19198                   ans = 0
    57    158.1 MiB      0.0 MiB      134386                   for n in ran:
    58    158.1 MiB      0.0 MiB      115188                       ans += self.loss_der(val, x[n], y[n])
    59    158.1 MiB      0.0 MiB       19198                   ans /= size
    60
    61    158.1 MiB      0.0 MiB       19198                   return ans
    62
    63    158.1 MiB      0.0 MiB         246               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.1 MiB      0.0 MiB         246               res.count_of_function_calls += preres.count_call_func * size
    65    158.1 MiB      0.0 MiB         246               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.1 MiB      0.0 MiB         246               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.1 MiB      0.0 MiB         246               self.weights = preres.res
    69    158.1 MiB      0.0 MiB         246               res.add_guess(self.weights)
    70    158.1 MiB      0.0 MiB         246               count += 1
    71
    72    158.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.1 MiB      0.0 MiB           1           res.success = True
    74    158.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:02:47,028] Trial 17 finished with value: 23147948204.806778 and parameters: {'l_1': 0.0033268312053722953, 'l_2': 0.0077260189650760515, 'batch': 6, 'min_count': 221}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.1 MiB    158.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.1 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.1 MiB      0.0 MiB           1           count = 0
    31    158.1 MiB      0.0 MiB         841           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.1 MiB      0.0 MiB         840               if self.batch != 0:
    33    158.1 MiB      0.0 MiB         840                   ran = get_n_random(self.batch, len(x))
    34    158.1 MiB      0.0 MiB         840                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.1 MiB      0.0 MiB         840               grad = 0
    40    158.1 MiB      0.0 MiB        2520               for n in ran:
    41                                                         # just like a normal derivative
    42    158.1 MiB      0.0 MiB        1680                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.1 MiB      0.0 MiB         840               grad /= size
    44
    45    158.1 MiB      0.0 MiB       64970               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.1 MiB      0.0 MiB       64130                   ans = 0
    48    158.1 MiB      0.0 MiB      192390                   for n in ran:
    49    158.1 MiB      0.0 MiB      128260                       ans += self.loss(val, x[n], y[n])
    50    158.1 MiB      0.0 MiB       64130                   ans /= size
    51
    52    158.1 MiB      0.0 MiB       64130                   return ans
    53
    54    158.1 MiB      0.0 MiB       64970               def optim_der(val):
    55                                                         # the same as upper
    56    158.1 MiB      0.0 MiB       64130                   ans = 0
    57    158.1 MiB      0.0 MiB      192390                   for n in ran:
    58    158.1 MiB      0.0 MiB      128260                       ans += self.loss_der(val, x[n], y[n])
    59    158.1 MiB      0.0 MiB       64130                   ans /= size
    60
    61    158.1 MiB      0.0 MiB       64130                   return ans
    62
    63    158.1 MiB      0.0 MiB         840               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.1 MiB      0.0 MiB         840               res.count_of_function_calls += preres.count_call_func * size
    65    158.1 MiB      0.0 MiB         840               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.1 MiB      0.0 MiB         840               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.1 MiB      0.0 MiB         840               self.weights = preres.res
    69    158.1 MiB      0.0 MiB         840               res.add_guess(self.weights)
    70    158.1 MiB      0.0 MiB         840               count += 1
    71
    72    158.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.1 MiB      0.0 MiB           1           res.success = True
    74    158.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:03:26,560] Trial 18 finished with value: 171295047143.01572 and parameters: {'l_1': 0.005100998300209353, 'l_2': 0.005578885342582811, 'batch': 2, 'min_count': 812}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.1 MiB    158.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.1 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.1 MiB      0.0 MiB           1           count = 0
    31    158.1 MiB      0.0 MiB         508           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.1 MiB      0.0 MiB         507               if self.batch != 0:
    33    158.1 MiB      0.0 MiB         507                   ran = get_n_random(self.batch, len(x))
    34    158.1 MiB      0.0 MiB         507                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.1 MiB      0.0 MiB         507               grad = 0
    40    158.1 MiB      0.0 MiB        1521               for n in ran:
    41                                                         # just like a normal derivative
    42    158.1 MiB      0.0 MiB        1014                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.1 MiB      0.0 MiB         507               grad /= size
    44
    45    158.1 MiB      0.0 MiB       39297               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.1 MiB      0.0 MiB       38790                   ans = 0
    48    158.1 MiB      0.0 MiB      116370                   for n in ran:
    49    158.1 MiB      0.0 MiB       77580                       ans += self.loss(val, x[n], y[n])
    50    158.1 MiB      0.0 MiB       38790                   ans /= size
    51
    52    158.1 MiB      0.0 MiB       38790                   return ans
    53
    54    158.1 MiB      0.0 MiB       39297               def optim_der(val):
    55                                                         # the same as upper
    56    158.1 MiB      0.0 MiB       38790                   ans = 0
    57    158.1 MiB      0.0 MiB      116370                   for n in ran:
    58    158.1 MiB      0.0 MiB       77580                       ans += self.loss_der(val, x[n], y[n])
    59    158.1 MiB      0.0 MiB       38790                   ans /= size
    60
    61    158.1 MiB      0.0 MiB       38790                   return ans
    62
    63    158.1 MiB      0.0 MiB         507               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.1 MiB      0.0 MiB         507               res.count_of_function_calls += preres.count_call_func * size
    65    158.1 MiB      0.0 MiB         507               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.1 MiB      0.0 MiB         507               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.1 MiB      0.0 MiB         507               self.weights = preres.res
    69    158.1 MiB      0.0 MiB         507               res.add_guess(self.weights)
    70    158.1 MiB      0.0 MiB         507               count += 1
    71
    72    158.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.1 MiB      0.0 MiB           1           res.success = True
    74    158.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:03:50,347] Trial 19 finished with value: 40383830139.850334 and parameters: {'l_1': 0.0013548085639050389, 'l_2': 0.006810546675911597, 'batch': 2, 'min_count': 486}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.2 MiB    158.2 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.2 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.2 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.2 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.2 MiB      0.0 MiB           1           count = 0
    31    158.2 MiB      0.0 MiB         475           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.2 MiB      0.0 MiB         474               if self.batch != 0:
    33    158.2 MiB      0.0 MiB         474                   ran = get_n_random(self.batch, len(x))
    34    158.2 MiB      0.0 MiB         474                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.2 MiB      0.0 MiB         474               grad = 0
    40    158.2 MiB      0.0 MiB        2370               for n in ran:
    41                                                         # just like a normal derivative
    42    158.2 MiB      0.0 MiB        1896                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.2 MiB      0.0 MiB         474               grad /= size
    44
    45    158.2 MiB      0.0 MiB       37142               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.2 MiB      0.0 MiB       36668                   ans = 0
    48    158.2 MiB      0.0 MiB      183340                   for n in ran:
    49    158.2 MiB      0.0 MiB      146672                       ans += self.loss(val, x[n], y[n])
    50    158.2 MiB      0.0 MiB       36668                   ans /= size
    51
    52    158.2 MiB      0.0 MiB       36668                   return ans
    53
    54    158.2 MiB      0.0 MiB       37142               def optim_der(val):
    55                                                         # the same as upper
    56    158.2 MiB      0.0 MiB       36668                   ans = 0
    57    158.2 MiB      0.0 MiB      183340                   for n in ran:
    58    158.2 MiB      0.0 MiB      146672                       ans += self.loss_der(val, x[n], y[n])
    59    158.2 MiB      0.0 MiB       36668                   ans /= size
    60
    61    158.2 MiB      0.0 MiB       36668                   return ans
    62
    63    158.2 MiB      0.0 MiB         474               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.2 MiB      0.0 MiB         474               res.count_of_function_calls += preres.count_call_func * size
    65    158.2 MiB      0.0 MiB         474               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.2 MiB      0.0 MiB         474               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.2 MiB      0.0 MiB         474               self.weights = preres.res
    69    158.2 MiB      0.0 MiB         474               res.add_guess(self.weights)
    70    158.2 MiB      0.0 MiB         474               count += 1
    71
    72    158.2 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.2 MiB      0.0 MiB           1           res.success = True
    74    158.2 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:04:27,218] Trial 20 finished with value: 30108779959.83005 and parameters: {'l_1': 0.002962750082039605, 'l_2': 0.008029133248662923, 'batch': 4, 'min_count': 471}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.2 MiB    158.2 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.2 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.2 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.2 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.2 MiB      0.0 MiB           1           count = 0
    31    158.2 MiB      0.0 MiB         509           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.2 MiB      0.0 MiB         508               if self.batch != 0:
    33    158.2 MiB      0.0 MiB         508                   ran = get_n_random(self.batch, len(x))
    34    158.2 MiB      0.0 MiB         508                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.2 MiB      0.0 MiB         508               grad = 0
    40    158.2 MiB      0.0 MiB        1524               for n in ran:
    41                                                         # just like a normal derivative
    42    158.2 MiB      0.0 MiB        1016                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.2 MiB      0.0 MiB         508               grad /= size
    44
    45    158.2 MiB      0.0 MiB       39424               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.2 MiB      0.0 MiB       38916                   ans = 0
    48    158.2 MiB      0.0 MiB      116748                   for n in ran:
    49    158.2 MiB      0.0 MiB       77832                       ans += self.loss(val, x[n], y[n])
    50    158.2 MiB      0.0 MiB       38916                   ans /= size
    51
    52    158.2 MiB      0.0 MiB       38916                   return ans
    53
    54    158.2 MiB      0.0 MiB       39424               def optim_der(val):
    55                                                         # the same as upper
    56    158.2 MiB      0.0 MiB       38916                   ans = 0
    57    158.2 MiB      0.0 MiB      116748                   for n in ran:
    58    158.2 MiB      0.0 MiB       77832                       ans += self.loss_der(val, x[n], y[n])
    59    158.2 MiB      0.0 MiB       38916                   ans /= size
    60
    61    158.2 MiB      0.0 MiB       38916                   return ans
    62
    63    158.2 MiB      0.0 MiB         508               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.2 MiB      0.0 MiB         508               res.count_of_function_calls += preres.count_call_func * size
    65    158.2 MiB      0.0 MiB         508               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.2 MiB      0.0 MiB         508               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.2 MiB      0.0 MiB         508               self.weights = preres.res
    69    158.2 MiB      0.0 MiB         508               res.add_guess(self.weights)
    70    158.2 MiB      0.0 MiB         508               count += 1
    71
    72    158.2 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.2 MiB      0.0 MiB           1           res.success = True
    74    158.2 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:04:51,170] Trial 21 finished with value: 19536097783.28356 and parameters: {'l_1': 0.0015776412624409122, 'l_2': 0.0068817133286221445, 'batch': 2, 'min_count': 348}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.2 MiB    158.2 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.2 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.2 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.2 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.2 MiB      0.0 MiB           1           count = 0
    31    158.2 MiB      0.0 MiB         252           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.2 MiB      0.0 MiB         251               if self.batch != 0:
    33    158.2 MiB      0.0 MiB         251                   ran = get_n_random(self.batch, len(x))
    34    158.2 MiB      0.0 MiB         251                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.2 MiB      0.0 MiB         251               grad = 0
    40    158.2 MiB      0.0 MiB         502               for n in ran:
    41                                                         # just like a normal derivative
    42    158.2 MiB      0.0 MiB         251                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.2 MiB      0.0 MiB         251               grad /= size
    44
    45    158.2 MiB      0.0 MiB       18701               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.2 MiB      0.0 MiB       18450                   ans = 0
    48    158.2 MiB      0.0 MiB       36900                   for n in ran:
    49    158.2 MiB      0.0 MiB       18450                       ans += self.loss(val, x[n], y[n])
    50    158.2 MiB      0.0 MiB       18450                   ans /= size
    51
    52    158.2 MiB      0.0 MiB       18450                   return ans
    53
    54    158.2 MiB      0.0 MiB       18701               def optim_der(val):
    55                                                         # the same as upper
    56    158.2 MiB      0.0 MiB       18450                   ans = 0
    57    158.2 MiB      0.0 MiB       36900                   for n in ran:
    58    158.2 MiB      0.0 MiB       18450                       ans += self.loss_der(val, x[n], y[n])
    59    158.2 MiB      0.0 MiB       18450                   ans /= size
    60
    61    158.2 MiB      0.0 MiB       18450                   return ans
    62
    63    158.2 MiB      0.0 MiB         251               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.2 MiB      0.0 MiB         251               res.count_of_function_calls += preres.count_call_func * size
    65    158.2 MiB      0.0 MiB         251               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.2 MiB      0.0 MiB         251               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.2 MiB      0.0 MiB         251               self.weights = preres.res
    69    158.2 MiB      0.0 MiB         251               res.add_guess(self.weights)
    70    158.2 MiB      0.0 MiB         251               count += 1
    71
    72    158.2 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.2 MiB      0.0 MiB           1           res.success = True
    74    158.2 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:04:58,814] Trial 22 finished with value: 19992903782.903545 and parameters: {'l_1': 0.004100292737278173, 'l_2': 0.004763027449233202, 'batch': 1, 'min_count': 244}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.2 MiB    158.2 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.2 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.2 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.2 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.2 MiB      0.0 MiB           1           count = 0
    31    158.2 MiB      0.0 MiB         470           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.2 MiB      0.0 MiB         469               if self.batch != 0:
    33    158.2 MiB      0.0 MiB         469                   ran = get_n_random(self.batch, len(x))
    34    158.2 MiB      0.0 MiB         469                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.2 MiB      0.0 MiB         469               grad = 0
    40    158.2 MiB      0.0 MiB        1407               for n in ran:
    41                                                         # just like a normal derivative
    42    158.2 MiB      0.0 MiB         938                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.2 MiB      0.0 MiB         469               grad /= size
    44
    45    158.2 MiB      0.0 MiB       36653               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.2 MiB      0.0 MiB       36184                   ans = 0
    48    158.2 MiB      0.0 MiB      108552                   for n in ran:
    49    158.2 MiB      0.0 MiB       72368                       ans += self.loss(val, x[n], y[n])
    50    158.2 MiB      0.0 MiB       36184                   ans /= size
    51
    52    158.2 MiB      0.0 MiB       36184                   return ans
    53
    54    158.2 MiB      0.0 MiB       36653               def optim_der(val):
    55                                                         # the same as upper
    56    158.2 MiB      0.0 MiB       36184                   ans = 0
    57    158.2 MiB      0.0 MiB      108552                   for n in ran:
    58    158.2 MiB      0.0 MiB       72368                       ans += self.loss_der(val, x[n], y[n])
    59    158.2 MiB      0.0 MiB       36184                   ans /= size
    60
    61    158.2 MiB      0.0 MiB       36184                   return ans
    62
    63    158.2 MiB      0.0 MiB         469               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.2 MiB      0.0 MiB         469               res.count_of_function_calls += preres.count_call_func * size
    65    158.2 MiB      0.0 MiB         469               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.2 MiB      0.0 MiB         469               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.2 MiB      0.0 MiB         469               self.weights = preres.res
    69    158.2 MiB      0.0 MiB         469               res.add_guess(self.weights)
    70    158.2 MiB      0.0 MiB         469               count += 1
    71
    72    158.2 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.2 MiB      0.0 MiB           1           res.success = True
    74    158.2 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:05:21,025] Trial 23 finished with value: 30482601690.35314 and parameters: {'l_1': 0.0011760020918164774, 'l_2': 0.00624770690257422, 'batch': 2, 'min_count': 425}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.3 MiB    158.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.3 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.3 MiB      0.0 MiB           1           count = 0
    31    158.3 MiB      0.0 MiB         342           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.3 MiB      0.0 MiB         341               if self.batch != 0:
    33    158.3 MiB      0.0 MiB         341                   ran = get_n_random(self.batch, len(x))
    34    158.3 MiB      0.0 MiB         341                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.3 MiB      0.0 MiB         341               grad = 0
    40    158.3 MiB      0.0 MiB         682               for n in ran:
    41                                                         # just like a normal derivative
    42    158.3 MiB      0.0 MiB         341                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.3 MiB      0.0 MiB         341               grad /= size
    44
    45    158.3 MiB      0.0 MiB       25135               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.3 MiB      0.0 MiB       24794                   ans = 0
    48    158.3 MiB      0.0 MiB       49588                   for n in ran:
    49    158.3 MiB      0.0 MiB       24794                       ans += self.loss(val, x[n], y[n])
    50    158.3 MiB      0.0 MiB       24794                   ans /= size
    51
    52    158.3 MiB      0.0 MiB       24794                   return ans
    53
    54    158.3 MiB      0.0 MiB       25135               def optim_der(val):
    55                                                         # the same as upper
    56    158.3 MiB      0.0 MiB       24794                   ans = 0
    57    158.3 MiB      0.0 MiB       49588                   for n in ran:
    58    158.3 MiB      0.0 MiB       24794                       ans += self.loss_der(val, x[n], y[n])
    59    158.3 MiB      0.0 MiB       24794                   ans /= size
    60
    61    158.3 MiB      0.0 MiB       24794                   return ans
    62
    63    158.3 MiB      0.0 MiB         341               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.3 MiB      0.0 MiB         341               res.count_of_function_calls += preres.count_call_func * size
    65    158.3 MiB      0.0 MiB         341               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.3 MiB      0.0 MiB         341               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.3 MiB      0.0 MiB         341               self.weights = preres.res
    69    158.3 MiB      0.0 MiB         341               res.add_guess(self.weights)
    70    158.3 MiB      0.0 MiB         341               count += 1
    71
    72    158.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.3 MiB      0.0 MiB           1           res.success = True
    74    158.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:05:31,529] Trial 24 finished with value: 11978022164.916082 and parameters: {'l_1': 0.002453846139543244, 'l_2': 0.007536365220149391, 'batch': 1, 'min_count': 306}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.3 MiB    158.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.3 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.3 MiB      0.0 MiB           1           count = 0
    31    158.3 MiB      0.0 MiB         270           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.3 MiB      0.0 MiB         269               if self.batch != 0:
    33    158.3 MiB      0.0 MiB         269                   ran = get_n_random(self.batch, len(x))
    34    158.3 MiB      0.0 MiB         269                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.3 MiB      0.0 MiB         269               grad = 0
    40    158.3 MiB      0.0 MiB        1345               for n in ran:
    41                                                         # just like a normal derivative
    42    158.3 MiB      0.0 MiB        1076                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.3 MiB      0.0 MiB         269               grad /= size
    44
    45    158.3 MiB      0.0 MiB       21391               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.3 MiB      0.0 MiB       21122                   ans = 0
    48    158.3 MiB      0.0 MiB      105610                   for n in ran:
    49    158.3 MiB      0.0 MiB       84488                       ans += self.loss(val, x[n], y[n])
    50    158.3 MiB      0.0 MiB       21122                   ans /= size
    51
    52    158.3 MiB      0.0 MiB       21122                   return ans
    53
    54    158.3 MiB      0.0 MiB       21391               def optim_der(val):
    55                                                         # the same as upper
    56    158.3 MiB      0.0 MiB       21122                   ans = 0
    57    158.3 MiB      0.0 MiB      105610                   for n in ran:
    58    158.3 MiB      0.0 MiB       84488                       ans += self.loss_der(val, x[n], y[n])
    59    158.3 MiB      0.0 MiB       21122                   ans /= size
    60
    61    158.3 MiB      0.0 MiB       21122                   return ans
    62
    63    158.3 MiB      0.0 MiB         269               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.3 MiB      0.0 MiB         269               res.count_of_function_calls += preres.count_call_func * size
    65    158.3 MiB      0.0 MiB         269               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.3 MiB      0.0 MiB         269               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.3 MiB      0.0 MiB         269               self.weights = preres.res
    69    158.3 MiB      0.0 MiB         269               res.add_guess(self.weights)
    70    158.3 MiB      0.0 MiB         269               count += 1
    71
    72    158.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.3 MiB      0.0 MiB           1           res.success = True
    74    158.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:05:53,086] Trial 25 finished with value: 53929238629.52187 and parameters: {'l_1': 0.002626986914611507, 'l_2': 0.008709696963932943, 'batch': 4, 'min_count': 194}. Best is trial 11 with value: 7984876739.980007.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.3 MiB    158.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.3 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.3 MiB      0.0 MiB           1           count = 0
    31    158.3 MiB      0.0 MiB         450           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.3 MiB      0.0 MiB         449               if self.batch != 0:
    33    158.3 MiB      0.0 MiB         449                   ran = get_n_random(self.batch, len(x))
    34    158.3 MiB      0.0 MiB         449                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.3 MiB      0.0 MiB         449               grad = 0
    40    158.3 MiB      0.0 MiB         898               for n in ran:
    41                                                         # just like a normal derivative
    42    158.3 MiB      0.0 MiB         449                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.3 MiB      0.0 MiB         449               grad /= size
    44
    45    158.3 MiB      0.0 MiB       33505               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.3 MiB      0.0 MiB       33056                   ans = 0
    48    158.3 MiB      0.0 MiB       66112                   for n in ran:
    49    158.3 MiB      0.0 MiB       33056                       ans += self.loss(val, x[n], y[n])
    50    158.3 MiB      0.0 MiB       33056                   ans /= size
    51
    52    158.3 MiB      0.0 MiB       33056                   return ans
    53
    54    158.3 MiB      0.0 MiB       33505               def optim_der(val):
    55                                                         # the same as upper
    56    158.3 MiB      0.0 MiB       33056                   ans = 0
    57    158.3 MiB      0.0 MiB       66112                   for n in ran:
    58    158.3 MiB      0.0 MiB       33056                       ans += self.loss_der(val, x[n], y[n])
    59    158.3 MiB      0.0 MiB       33056                   ans /= size
    60
    61    158.3 MiB      0.0 MiB       33056                   return ans
    62
    63    158.3 MiB      0.0 MiB         449               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.3 MiB      0.0 MiB         449               res.count_of_function_calls += preres.count_call_func * size
    65    158.3 MiB      0.0 MiB         449               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.3 MiB      0.0 MiB         449               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.3 MiB      0.0 MiB         449               self.weights = preres.res
    69    158.3 MiB      0.0 MiB         449               res.add_guess(self.weights)
    70    158.3 MiB      0.0 MiB         449               count += 1
    71
    72    158.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.3 MiB      0.0 MiB           1           res.success = True
    74    158.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:06:07,159] Trial 26 finished with value: 4652100560.737796 and parameters: {'l_1': 0.0037824862798109603, 'l_2': 0.007673710105000955, 'batch': 1, 'min_count': 317}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.3 MiB    158.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.3 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.3 MiB      0.0 MiB           1           count = 0
    31    158.3 MiB      0.0 MiB         549           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.3 MiB      0.0 MiB         548               if self.batch != 0:
    33    158.3 MiB      0.0 MiB         548                   ran = get_n_random(self.batch, len(x))
    34    158.3 MiB      0.0 MiB         548                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.3 MiB      0.0 MiB         548               grad = 0
    40    158.3 MiB      0.0 MiB        2192               for n in ran:
    41                                                         # just like a normal derivative
    42    158.3 MiB      0.0 MiB        1644                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.3 MiB      0.0 MiB         548               grad /= size
    44
    45    158.3 MiB      0.0 MiB       42886               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.3 MiB      0.0 MiB       42338                   ans = 0
    48    158.3 MiB      0.0 MiB      169352                   for n in ran:
    49    158.3 MiB      0.0 MiB      127014                       ans += self.loss(val, x[n], y[n])
    50    158.3 MiB      0.0 MiB       42338                   ans /= size
    51
    52    158.3 MiB      0.0 MiB       42338                   return ans
    53
    54    158.3 MiB      0.0 MiB       42886               def optim_der(val):
    55                                                         # the same as upper
    56    158.3 MiB      0.0 MiB       42338                   ans = 0
    57    158.3 MiB      0.0 MiB      169352                   for n in ran:
    58    158.3 MiB      0.0 MiB      127014                       ans += self.loss_der(val, x[n], y[n])
    59    158.3 MiB      0.0 MiB       42338                   ans /= size
    60
    61    158.3 MiB      0.0 MiB       42338                   return ans
    62
    63    158.3 MiB      0.0 MiB         548               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.3 MiB      0.0 MiB         548               res.count_of_function_calls += preres.count_call_func * size
    65    158.3 MiB      0.0 MiB         548               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.3 MiB      0.0 MiB         548               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.3 MiB      0.0 MiB         548               self.weights = preres.res
    69    158.3 MiB      0.0 MiB         548               res.add_guess(self.weights)
    70    158.3 MiB      0.0 MiB         548               count += 1
    71
    72    158.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.3 MiB      0.0 MiB           1           res.success = True
    74    158.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:06:42,126] Trial 27 finished with value: 30738381250.24748 and parameters: {'l_1': 0.005946422819993348, 'l_2': 0.005295099970181518, 'batch': 3, 'min_count': 525}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.3 MiB    158.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.3 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.3 MiB      0.0 MiB           1           count = 0
    31    158.3 MiB      0.0 MiB         330           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.3 MiB      0.0 MiB         329               if self.batch != 0:
    33    158.3 MiB      0.0 MiB         329                   ran = get_n_random(self.batch, len(x))
    34    158.3 MiB      0.0 MiB         329                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.3 MiB      0.0 MiB         329               grad = 0
    40    158.3 MiB      0.0 MiB         658               for n in ran:
    41                                                         # just like a normal derivative
    42    158.3 MiB      0.0 MiB         329                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.3 MiB      0.0 MiB         329               grad /= size
    44
    45    158.3 MiB      0.0 MiB       24727               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.3 MiB      0.0 MiB       24398                   ans = 0
    48    158.3 MiB      0.0 MiB       48796                   for n in ran:
    49    158.3 MiB      0.0 MiB       24398                       ans += self.loss(val, x[n], y[n])
    50    158.3 MiB      0.0 MiB       24398                   ans /= size
    51
    52    158.3 MiB      0.0 MiB       24398                   return ans
    53
    54    158.3 MiB      0.0 MiB       24727               def optim_der(val):
    55                                                         # the same as upper
    56    158.3 MiB      0.0 MiB       24398                   ans = 0
    57    158.3 MiB      0.0 MiB       48796                   for n in ran:
    58    158.3 MiB      0.0 MiB       24398                       ans += self.loss_der(val, x[n], y[n])
    59    158.3 MiB      0.0 MiB       24398                   ans /= size
    60
    61    158.3 MiB      0.0 MiB       24398                   return ans
    62
    63    158.3 MiB      0.0 MiB         329               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.3 MiB      0.0 MiB         329               res.count_of_function_calls += preres.count_call_func * size
    65    158.3 MiB      0.0 MiB         329               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.3 MiB      0.0 MiB         329               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.3 MiB      0.0 MiB         329               self.weights = preres.res
    69    158.3 MiB      0.0 MiB         329               res.add_guess(self.weights)
    70    158.3 MiB      0.0 MiB         329               count += 1
    71
    72    158.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.3 MiB      0.0 MiB           1           res.success = True
    74    158.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:06:52,442] Trial 28 finished with value: 8196347531.358262 and parameters: {'l_1': 0.0037553041665248405, 'l_2': 0.009855293266811479, 'batch': 1, 'min_count': 293}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.3 MiB    158.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.3 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.3 MiB      0.0 MiB           1           count = 0
    31    158.3 MiB      0.0 MiB         310           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.3 MiB      0.0 MiB         309               if self.batch != 0:
    33    158.3 MiB      0.0 MiB         309                   ran = get_n_random(self.batch, len(x))
    34    158.3 MiB      0.0 MiB         309                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.3 MiB      0.0 MiB         309               grad = 0
    40    158.3 MiB      0.0 MiB         927               for n in ran:
    41                                                         # just like a normal derivative
    42    158.3 MiB      0.0 MiB         618                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.3 MiB      0.0 MiB         309               grad /= size
    44
    45    158.3 MiB      0.0 MiB       24271               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.3 MiB      0.0 MiB       23962                   ans = 0
    48    158.3 MiB      0.0 MiB       71886                   for n in ran:
    49    158.3 MiB      0.0 MiB       47924                       ans += self.loss(val, x[n], y[n])
    50    158.3 MiB      0.0 MiB       23962                   ans /= size
    51
    52    158.3 MiB      0.0 MiB       23962                   return ans
    53
    54    158.3 MiB      0.0 MiB       24271               def optim_der(val):
    55                                                         # the same as upper
    56    158.3 MiB      0.0 MiB       23962                   ans = 0
    57    158.3 MiB      0.0 MiB       71886                   for n in ran:
    58    158.3 MiB      0.0 MiB       47924                       ans += self.loss_der(val, x[n], y[n])
    59    158.3 MiB      0.0 MiB       23962                   ans /= size
    60
    61    158.3 MiB      0.0 MiB       23962                   return ans
    62
    63    158.3 MiB      0.0 MiB         309               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.3 MiB      0.0 MiB         309               res.count_of_function_calls += preres.count_call_func * size
    65    158.3 MiB      0.0 MiB         309               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.3 MiB      0.0 MiB         309               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.3 MiB      0.0 MiB         309               self.weights = preres.res
    69    158.3 MiB      0.0 MiB         309               res.add_guess(self.weights)
    70    158.3 MiB      0.0 MiB         309               count += 1
    71
    72    158.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.3 MiB      0.0 MiB           1           res.success = True
    74    158.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:07:07,260] Trial 29 finished with value: 11702412078.021835 and parameters: {'l_1': 0.004607653881917668, 'l_2': 0.009792182695246992, 'batch': 2, 'min_count': 297}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.3 MiB    158.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.3 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.3 MiB      0.0 MiB           1           count = 0
    31    158.3 MiB      0.0 MiB         230           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.3 MiB      0.0 MiB         229               if self.batch != 0:
    33    158.3 MiB      0.0 MiB         229                   ran = get_n_random(self.batch, len(x))
    34    158.3 MiB      0.0 MiB         229                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.3 MiB      0.0 MiB         229               grad = 0
    40    158.3 MiB      0.0 MiB        1603               for n in ran:
    41                                                         # just like a normal derivative
    42    158.3 MiB      0.0 MiB        1374                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.3 MiB      0.0 MiB         229               grad /= size
    44
    45    158.3 MiB      0.0 MiB       17725               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.3 MiB      0.0 MiB       17496                   ans = 0
    48    158.3 MiB      0.0 MiB      122472                   for n in ran:
    49    158.3 MiB      0.0 MiB      104976                       ans += self.loss(val, x[n], y[n])
    50    158.3 MiB      0.0 MiB       17496                   ans /= size
    51
    52    158.3 MiB      0.0 MiB       17496                   return ans
    53
    54    158.3 MiB      0.0 MiB       17725               def optim_der(val):
    55                                                         # the same as upper
    56    158.3 MiB      0.0 MiB       17496                   ans = 0
    57    158.3 MiB      0.0 MiB      122472                   for n in ran:
    58    158.3 MiB      0.0 MiB      104976                       ans += self.loss_der(val, x[n], y[n])
    59    158.3 MiB      0.0 MiB       17496                   ans /= size
    60
    61    158.3 MiB      0.0 MiB       17496                   return ans
    62
    63    158.3 MiB      0.0 MiB         229               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.3 MiB      0.0 MiB         229               res.count_of_function_calls += preres.count_call_func * size
    65    158.3 MiB      0.0 MiB         229               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.3 MiB      0.0 MiB         229               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.3 MiB      0.0 MiB         229               self.weights = preres.res
    69    158.3 MiB      0.0 MiB         229               res.add_guess(self.weights)
    70    158.3 MiB      0.0 MiB         229               count += 1
    71
    72    158.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.3 MiB      0.0 MiB           1           res.success = True
    74    158.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:07:31,930] Trial 30 finished with value: 22927251029.160137 and parameters: {'l_1': 0.007214706551183608, 'l_2': 0.009972847888613995, 'batch': 6, 'min_count': 128}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.3 MiB    158.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.3 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.3 MiB      0.0 MiB           1           count = 0
    31    158.3 MiB      0.0 MiB         721           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.3 MiB      0.0 MiB         720               if self.batch != 0:
    33    158.3 MiB      0.0 MiB         720                   ran = get_n_random(self.batch, len(x))
    34    158.3 MiB      0.0 MiB         720                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.3 MiB      0.0 MiB         720               grad = 0
    40    158.3 MiB      0.0 MiB        1440               for n in ran:
    41                                                         # just like a normal derivative
    42    158.3 MiB      0.0 MiB         720                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.3 MiB      0.0 MiB         720               grad /= size
    44
    45    158.3 MiB      0.0 MiB       53746               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.3 MiB      0.0 MiB       53026                   ans = 0
    48    158.3 MiB      0.0 MiB      106052                   for n in ran:
    49    158.3 MiB      0.1 MiB       53026                       ans += self.loss(val, x[n], y[n])
    50    158.3 MiB      0.0 MiB       53026                   ans /= size
    51
    52    158.3 MiB      0.0 MiB       53026                   return ans
    53
    54    158.3 MiB      0.0 MiB       53746               def optim_der(val):
    55                                                         # the same as upper
    56    158.3 MiB      0.0 MiB       53026                   ans = 0
    57    158.3 MiB      0.0 MiB      106052                   for n in ran:
    58    158.3 MiB      0.0 MiB       53026                       ans += self.loss_der(val, x[n], y[n])
    59    158.3 MiB      0.0 MiB       53026                   ans /= size
    60
    61    158.3 MiB      0.0 MiB       53026                   return ans
    62
    63    158.3 MiB      0.0 MiB         720               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.3 MiB      0.0 MiB         720               res.count_of_function_calls += preres.count_call_func * size
    65    158.3 MiB      0.0 MiB         720               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.3 MiB      0.0 MiB         720               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.3 MiB      0.0 MiB         720               self.weights = preres.res
    69    158.3 MiB      0.0 MiB         720               res.add_guess(self.weights)
    70    158.3 MiB      0.0 MiB         720               count += 1
    71
    72    158.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.3 MiB      0.0 MiB           1           res.success = True
    74    158.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:07:54,226] Trial 31 finished with value: 6834380159.96418 and parameters: {'l_1': 0.003730880131903913, 'l_2': 0.008481979639506117, 'batch': 1, 'min_count': 630}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.3 MiB    158.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.3 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.3 MiB      0.0 MiB           1           count = 0
    31    158.3 MiB      0.0 MiB         811           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.3 MiB      0.0 MiB         810               if self.batch != 0:
    33    158.3 MiB      0.0 MiB         810                   ran = get_n_random(self.batch, len(x))
    34    158.3 MiB      0.0 MiB         810                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.3 MiB      0.0 MiB         810               grad = 0
    40    158.3 MiB      0.0 MiB        1620               for n in ran:
    41                                                         # just like a normal derivative
    42    158.3 MiB      0.0 MiB         810                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.3 MiB      0.0 MiB         810               grad /= size
    44
    45    158.3 MiB      0.0 MiB       60816               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.3 MiB      0.0 MiB       60006                   ans = 0
    48    158.3 MiB      0.0 MiB      120012                   for n in ran:
    49    158.3 MiB      0.0 MiB       60006                       ans += self.loss(val, x[n], y[n])
    50    158.3 MiB      0.0 MiB       60006                   ans /= size
    51
    52    158.3 MiB      0.0 MiB       60006                   return ans
    53
    54    158.3 MiB      0.0 MiB       60816               def optim_der(val):
    55                                                         # the same as upper
    56    158.3 MiB      0.0 MiB       60006                   ans = 0
    57    158.3 MiB      0.0 MiB      120012                   for n in ran:
    58    158.3 MiB      0.0 MiB       60006                       ans += self.loss_der(val, x[n], y[n])
    59    158.3 MiB      0.0 MiB       60006                   ans /= size
    60
    61    158.3 MiB      0.0 MiB       60006                   return ans
    62
    63    158.3 MiB      0.0 MiB         810               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.3 MiB      0.0 MiB         810               res.count_of_function_calls += preres.count_call_func * size
    65    158.3 MiB      0.0 MiB         810               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.3 MiB      0.0 MiB         810               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.3 MiB      0.0 MiB         810               self.weights = preres.res
    69    158.3 MiB      0.0 MiB         810               res.add_guess(self.weights)
    70    158.3 MiB      0.0 MiB         810               count += 1
    71
    72    158.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.3 MiB      0.0 MiB           1           res.success = True
    74    158.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:08:19,218] Trial 32 finished with value: 72959858847.98952 and parameters: {'l_1': 0.003941410035869716, 'l_2': 0.009108087735233666, 'batch': 1, 'min_count': 636}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.4 MiB    158.4 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.4 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.4 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.4 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.4 MiB      0.0 MiB           1           count = 0
    31    158.4 MiB      0.0 MiB         590           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.4 MiB      0.0 MiB         589               if self.batch != 0:
    33    158.4 MiB      0.0 MiB         589                   ran = get_n_random(self.batch, len(x))
    34    158.4 MiB      0.0 MiB         589                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.4 MiB      0.0 MiB         589               grad = 0
    40    158.4 MiB      0.0 MiB        1767               for n in ran:
    41                                                         # just like a normal derivative
    42    158.4 MiB      0.0 MiB        1178                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.4 MiB      0.0 MiB         589               grad /= size
    44
    45    158.4 MiB      0.0 MiB       45893               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.4 MiB      0.0 MiB       45304                   ans = 0
    48    158.4 MiB      0.0 MiB      135912                   for n in ran:
    49    158.4 MiB      0.0 MiB       90608                       ans += self.loss(val, x[n], y[n])
    50    158.4 MiB      0.0 MiB       45304                   ans /= size
    51
    52    158.4 MiB      0.0 MiB       45304                   return ans
    53
    54    158.4 MiB      0.0 MiB       45893               def optim_der(val):
    55                                                         # the same as upper
    56    158.4 MiB      0.0 MiB       45304                   ans = 0
    57    158.4 MiB      0.0 MiB      135912                   for n in ran:
    58    158.4 MiB      0.0 MiB       90608                       ans += self.loss_der(val, x[n], y[n])
    59    158.4 MiB      0.0 MiB       45304                   ans /= size
    60
    61    158.4 MiB      0.0 MiB       45304                   return ans
    62
    63    158.4 MiB      0.0 MiB         589               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.4 MiB      0.0 MiB         589               res.count_of_function_calls += preres.count_call_func * size
    65    158.4 MiB      0.0 MiB         589               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.4 MiB      0.0 MiB         589               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.4 MiB      0.0 MiB         589               self.weights = preres.res
    69    158.4 MiB      0.0 MiB         589               res.add_guess(self.weights)
    70    158.4 MiB      0.0 MiB         589               count += 1
    71
    72    158.4 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.4 MiB      0.0 MiB           1           res.success = True
    74    158.4 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:08:46,998] Trial 33 finished with value: 202208949703.9145 and parameters: {'l_1': 0.0034035352749645837, 'l_2': 0.008307072631555031, 'batch': 2, 'min_count': 567}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.4 MiB    158.4 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.4 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.4 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.4 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.4 MiB      0.0 MiB           1           count = 0
    31    158.4 MiB      0.0 MiB         756           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.4 MiB      0.0 MiB         755               if self.batch != 0:
    33    158.4 MiB      0.0 MiB         755                   ran = get_n_random(self.batch, len(x))
    34    158.4 MiB      0.0 MiB         755                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.4 MiB      0.0 MiB         755               grad = 0
    40    158.4 MiB      0.0 MiB        3020               for n in ran:
    41                                                         # just like a normal derivative
    42    158.4 MiB      0.0 MiB        2265                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.4 MiB      0.0 MiB         755               grad /= size
    44
    45    158.4 MiB      0.0 MiB       58801               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.4 MiB      0.0 MiB       58046                   ans = 0
    48    158.4 MiB      0.0 MiB      232184                   for n in ran:
    49    158.4 MiB      0.0 MiB      174138                       ans += self.loss(val, x[n], y[n])
    50    158.4 MiB      0.0 MiB       58046                   ans /= size
    51
    52    158.4 MiB      0.0 MiB       58046                   return ans
    53
    54    158.4 MiB      0.0 MiB       58801               def optim_der(val):
    55                                                         # the same as upper
    56    158.4 MiB      0.0 MiB       58046                   ans = 0
    57    158.4 MiB      0.0 MiB      232184                   for n in ran:
    58    158.4 MiB      0.0 MiB      174138                       ans += self.loss_der(val, x[n], y[n])
    59    158.4 MiB      0.0 MiB       58046                   ans /= size
    60
    61    158.4 MiB      0.0 MiB       58046                   return ans
    62
    63    158.4 MiB      0.0 MiB         755               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.4 MiB      0.0 MiB         755               res.count_of_function_calls += preres.count_call_func * size
    65    158.4 MiB      0.0 MiB         755               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.4 MiB      0.0 MiB         755               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.4 MiB      0.0 MiB         755               self.weights = preres.res
    69    158.4 MiB      0.0 MiB         755               res.add_guess(self.weights)
    70    158.4 MiB      0.0 MiB         755               count += 1
    71
    72    158.4 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.4 MiB      0.0 MiB           1           res.success = True
    74    158.4 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:09:34,169] Trial 34 finished with value: 43963494476.10929 and parameters: {'l_1': 0.005673636949393681, 'l_2': 0.009313453678940353, 'batch': 3, 'min_count': 738}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.4 MiB    158.4 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.4 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.4 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.4 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.4 MiB      0.0 MiB           1           count = 0
    31    158.4 MiB      0.0 MiB         185           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.4 MiB      0.0 MiB         184               if self.batch != 0:
    33    158.4 MiB      0.0 MiB         184                   ran = get_n_random(self.batch, len(x))
    34    158.4 MiB      0.0 MiB         184                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.4 MiB      0.0 MiB         184               grad = 0
    40    158.4 MiB      0.0 MiB        1656               for n in ran:
    41                                                         # just like a normal derivative
    42    158.4 MiB      0.0 MiB        1472                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.4 MiB      0.0 MiB         184               grad /= size
    44
    45    158.4 MiB      0.0 MiB       14538               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.4 MiB      0.0 MiB       14354                   ans = 0
    48    158.4 MiB      0.0 MiB      129186                   for n in ran:
    49    158.4 MiB      0.0 MiB      114832                       ans += self.loss(val, x[n], y[n])
    50    158.4 MiB      0.0 MiB       14354                   ans /= size
    51
    52    158.4 MiB      0.0 MiB       14354                   return ans
    53
    54    158.4 MiB      0.0 MiB       14538               def optim_der(val):
    55                                                         # the same as upper
    56    158.4 MiB      0.0 MiB       14354                   ans = 0
    57    158.4 MiB      0.0 MiB      129186                   for n in ran:
    58    158.4 MiB      0.0 MiB      114832                       ans += self.loss_der(val, x[n], y[n])
    59    158.4 MiB      0.0 MiB       14354                   ans /= size
    60
    61    158.4 MiB      0.0 MiB       14354                   return ans
    62
    63    158.4 MiB      0.0 MiB         184               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.4 MiB      0.0 MiB         184               res.count_of_function_calls += preres.count_call_func * size
    65    158.4 MiB      0.0 MiB         184               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.4 MiB      0.0 MiB         184               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.4 MiB      0.0 MiB         184               self.weights = preres.res
    69    158.4 MiB      0.0 MiB         184               res.add_guess(self.weights)
    70    158.4 MiB      0.0 MiB         184               count += 1
    71
    72    158.4 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.4 MiB      0.0 MiB           1           res.success = True
    74    158.4 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:10:00,517] Trial 35 finished with value: 24068862396.908283 and parameters: {'l_1': 0.004696487388730987, 'l_2': 0.008306370608697183, 'batch': 8, 'min_count': 173}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.4 MiB    158.4 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.4 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.4 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.4 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.4 MiB      0.0 MiB           1           count = 0
    31    158.4 MiB      0.0 MiB         672           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.4 MiB      0.0 MiB         671               if self.batch != 0:
    33    158.4 MiB      0.0 MiB         671                   ran = get_n_random(self.batch, len(x))
    34    158.4 MiB      0.0 MiB         671                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.4 MiB      0.0 MiB         671               grad = 0
    40    158.4 MiB      0.0 MiB        1342               for n in ran:
    41                                                         # just like a normal derivative
    42    158.4 MiB      0.0 MiB         671                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.4 MiB      0.0 MiB         671               grad /= size
    44
    45    158.4 MiB      0.0 MiB       50121               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.4 MiB      0.0 MiB       49450                   ans = 0
    48    158.4 MiB      0.0 MiB       98900                   for n in ran:
    49    158.4 MiB      0.0 MiB       49450                       ans += self.loss(val, x[n], y[n])
    50    158.4 MiB      0.0 MiB       49450                   ans /= size
    51
    52    158.4 MiB      0.0 MiB       49450                   return ans
    53
    54    158.4 MiB      0.0 MiB       50121               def optim_der(val):
    55                                                         # the same as upper
    56    158.4 MiB      0.0 MiB       49450                   ans = 0
    57    158.4 MiB      0.0 MiB       98900                   for n in ran:
    58    158.4 MiB      0.0 MiB       49450                       ans += self.loss_der(val, x[n], y[n])
    59    158.4 MiB      0.0 MiB       49450                   ans /= size
    60
    61    158.4 MiB      0.0 MiB       49450                   return ans
    62
    63    158.4 MiB      0.0 MiB         671               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.4 MiB      0.0 MiB         671               res.count_of_function_calls += preres.count_call_func * size
    65    158.4 MiB      0.0 MiB         671               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.4 MiB      0.0 MiB         671               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.4 MiB      0.0 MiB         671               self.weights = preres.res
    69    158.4 MiB      0.0 MiB         671               res.add_guess(self.weights)
    70    158.4 MiB      0.0 MiB         671               count += 1
    71
    72    158.4 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.4 MiB      0.0 MiB           1           res.success = True
    74    158.4 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:10:20,893] Trial 36 finished with value: 18924250653.54023 and parameters: {'l_1': 0.0008393663697317472, 'l_2': 0.008721731058476189, 'batch': 1, 'min_count': 422}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.4 MiB    158.4 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.4 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.4 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.4 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.4 MiB      0.0 MiB           1           count = 0
    31    158.4 MiB      0.0 MiB         695           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.4 MiB      0.0 MiB         694               if self.batch != 0:
    33    158.4 MiB      0.0 MiB         694                   ran = get_n_random(self.batch, len(x))
    34    158.4 MiB      0.0 MiB         694                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.4 MiB      0.0 MiB         694               grad = 0
    40    158.4 MiB      0.0 MiB        2082               for n in ran:
    41                                                         # just like a normal derivative
    42    158.4 MiB      0.0 MiB        1388                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.4 MiB      0.0 MiB         694               grad /= size
    44
    45    158.4 MiB      0.0 MiB       53914               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.4 MiB      0.0 MiB       53220                   ans = 0
    48    158.4 MiB      0.0 MiB      159660                   for n in ran:
    49    158.4 MiB      0.0 MiB      106440                       ans += self.loss(val, x[n], y[n])
    50    158.4 MiB      0.0 MiB       53220                   ans /= size
    51
    52    158.4 MiB      0.0 MiB       53220                   return ans
    53
    54    158.4 MiB      0.0 MiB       53914               def optim_der(val):
    55                                                         # the same as upper
    56    158.4 MiB      0.0 MiB       53220                   ans = 0
    57    158.4 MiB      0.0 MiB      159660                   for n in ran:
    58    158.4 MiB      0.0 MiB      106440                       ans += self.loss_der(val, x[n], y[n])
    59    158.4 MiB      0.0 MiB       53220                   ans /= size
    60
    61    158.4 MiB      0.0 MiB       53220                   return ans
    62
    63    158.4 MiB      0.0 MiB         694               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.4 MiB      0.0 MiB         694               res.count_of_function_calls += preres.count_call_func * size
    65    158.4 MiB      0.0 MiB         694               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.4 MiB      0.0 MiB         694               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.4 MiB      0.0 MiB         694               self.weights = preres.res
    69    158.4 MiB      0.0 MiB         694               res.add_guess(self.weights)
    70    158.4 MiB      0.0 MiB         694               count += 1
    71
    72    158.4 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.4 MiB      0.0 MiB           1           res.success = True
    74    158.4 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:10:53,314] Trial 37 finished with value: 21042533610.24987 and parameters: {'l_1': 0.003619460683668771, 'l_2': 0.00746920479557398, 'batch': 2, 'min_count': 254}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.4 MiB    158.4 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.4 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.4 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.4 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.4 MiB      0.0 MiB           1           count = 0
    31    158.4 MiB      0.0 MiB         779           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.4 MiB      0.0 MiB         778               if self.batch != 0:
    33    158.4 MiB      0.0 MiB         778                   ran = get_n_random(self.batch, len(x))
    34    158.4 MiB      0.0 MiB         778                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.4 MiB      0.0 MiB         778               grad = 0
    40    158.4 MiB      0.0 MiB        3112               for n in ran:
    41                                                         # just like a normal derivative
    42    158.4 MiB      0.0 MiB        2334                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.4 MiB      0.0 MiB         778               grad /= size
    44
    45    158.4 MiB      0.0 MiB       60968               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.4 MiB      0.0 MiB       60190                   ans = 0
    48    158.4 MiB      0.0 MiB      240760                   for n in ran:
    49    158.4 MiB      0.0 MiB      180570                       ans += self.loss(val, x[n], y[n])
    50    158.4 MiB      0.0 MiB       60190                   ans /= size
    51
    52    158.4 MiB      0.0 MiB       60190                   return ans
    53
    54    158.4 MiB      0.0 MiB       60968               def optim_der(val):
    55                                                         # the same as upper
    56    158.4 MiB      0.0 MiB       60190                   ans = 0
    57    158.4 MiB      0.0 MiB      240760                   for n in ran:
    58    158.4 MiB      0.0 MiB      180570                       ans += self.loss_der(val, x[n], y[n])
    59    158.4 MiB      0.0 MiB       60190                   ans /= size
    60
    61    158.4 MiB      0.0 MiB       60190                   return ans
    62
    63    158.4 MiB      0.0 MiB         778               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.4 MiB      0.0 MiB         778               res.count_of_function_calls += preres.count_call_func * size
    65    158.4 MiB      0.0 MiB         778               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.4 MiB      0.0 MiB         778               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.4 MiB      0.0 MiB         778               self.weights = preres.res
    69    158.4 MiB      0.0 MiB         778               res.add_guess(self.weights)
    70    158.4 MiB      0.0 MiB         778               count += 1
    71
    72    158.4 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.4 MiB      0.0 MiB           1           res.success = True
    74    158.4 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:11:41,384] Trial 38 finished with value: 35187814700.80622 and parameters: {'l_1': 0.00015412805469466797, 'l_2': 0.009353741653007454, 'batch': 3, 'min_count': 710}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.6 MiB    158.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.6 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.6 MiB      0.0 MiB           1           count = 0
    31    158.6 MiB      0.0 MiB         613           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.6 MiB      0.0 MiB         612               if self.batch != 0:
    33    158.6 MiB      0.0 MiB         612                   ran = get_n_random(self.batch, len(x))
    34    158.6 MiB      0.0 MiB         612                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.6 MiB      0.0 MiB         612               grad = 0
    40    158.6 MiB      0.0 MiB        1224               for n in ran:
    41                                                         # just like a normal derivative
    42    158.6 MiB      0.0 MiB         612                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.6 MiB      0.0 MiB         612               grad /= size
    44
    45    158.6 MiB      0.0 MiB       45930               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.6 MiB      0.0 MiB       45318                   ans = 0
    48    158.6 MiB      0.0 MiB       90636                   for n in ran:
    49    158.6 MiB      0.0 MiB       45318                       ans += self.loss(val, x[n], y[n])
    50    158.6 MiB      0.0 MiB       45318                   ans /= size
    51
    52    158.6 MiB      0.0 MiB       45318                   return ans
    53
    54    158.6 MiB      0.0 MiB       45930               def optim_der(val):
    55                                                         # the same as upper
    56    158.6 MiB      0.0 MiB       45318                   ans = 0
    57    158.6 MiB      0.0 MiB       90636                   for n in ran:
    58    158.6 MiB      0.0 MiB       45318                       ans += self.loss_der(val, x[n], y[n])
    59    158.6 MiB      0.0 MiB       45318                   ans /= size
    60
    61    158.6 MiB      0.0 MiB       45318                   return ans
    62
    63    158.6 MiB      0.0 MiB         612               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.6 MiB      0.0 MiB         612               res.count_of_function_calls += preres.count_call_func * size
    65    158.6 MiB      0.0 MiB         612               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.6 MiB      0.0 MiB         612               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.6 MiB      0.0 MiB         612               self.weights = preres.res
    69    158.6 MiB      0.0 MiB         612               res.add_guess(self.weights)
    70    158.6 MiB      0.0 MiB         612               count += 1
    71
    72    158.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.6 MiB      0.0 MiB           1           res.success = True
    74    158.6 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:11:59,999] Trial 39 finished with value: 21848440397.81696 and parameters: {'l_1': 0.005198841223773699, 'l_2': 0.0005354138913777218, 'batch': 1, 'min_count': 604}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.6 MiB    158.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.6 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.6 MiB      0.0 MiB           1           count = 0
    31    158.6 MiB      0.0 MiB         178           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.6 MiB      0.0 MiB         177               if self.batch != 0:
    33    158.6 MiB      0.0 MiB         177                   ran = get_n_random(self.batch, len(x))
    34    158.6 MiB      0.0 MiB         177                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.6 MiB      0.0 MiB         177               grad = 0
    40    158.6 MiB      0.0 MiB        1947               for n in ran:
    41                                                         # just like a normal derivative
    42    158.6 MiB      0.0 MiB        1770                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.6 MiB      0.0 MiB         177               grad /= size
    44
    45    158.6 MiB      0.0 MiB       13941               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.6 MiB      0.0 MiB       13764                   ans = 0
    48    158.6 MiB      0.0 MiB      151404                   for n in ran:
    49    158.6 MiB      0.0 MiB      137640                       ans += self.loss(val, x[n], y[n])
    50    158.6 MiB      0.0 MiB       13764                   ans /= size
    51
    52    158.6 MiB      0.0 MiB       13764                   return ans
    53
    54    158.6 MiB      0.0 MiB       13941               def optim_der(val):
    55                                                         # the same as upper
    56    158.6 MiB      0.0 MiB       13764                   ans = 0
    57    158.6 MiB      0.0 MiB      151404                   for n in ran:
    58    158.6 MiB      0.0 MiB      137640                       ans += self.loss_der(val, x[n], y[n])
    59    158.6 MiB      0.0 MiB       13764                   ans /= size
    60
    61    158.6 MiB      0.0 MiB       13764                   return ans
    62
    63    158.6 MiB      0.0 MiB         177               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.6 MiB      0.0 MiB         177               res.count_of_function_calls += preres.count_call_func * size
    65    158.6 MiB      0.0 MiB         177               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.6 MiB      0.0 MiB         177               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.6 MiB      0.0 MiB         177               self.weights = preres.res
    69    158.6 MiB      0.0 MiB         177               res.add_guess(self.weights)
    70    158.6 MiB      0.0 MiB         177               count += 1
    71
    72    158.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.6 MiB      0.0 MiB           1           res.success = True
    74    158.6 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:12:29,483] Trial 40 finished with value: 30750656971.648502 and parameters: {'l_1': 0.006705497020495752, 'l_2': 0.007282645866671493, 'batch': 10, 'min_count': 112}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.6 MiB    158.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.6 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.6 MiB      0.0 MiB           1           count = 0
    31    158.6 MiB      0.0 MiB         939           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.6 MiB      0.0 MiB         938               if self.batch != 0:
    33    158.6 MiB      0.0 MiB         938                   ran = get_n_random(self.batch, len(x))
    34    158.6 MiB      0.0 MiB         938                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.6 MiB      0.0 MiB         938               grad = 0
    40    158.6 MiB      0.0 MiB        1876               for n in ran:
    41                                                         # just like a normal derivative
    42    158.6 MiB      0.0 MiB         938                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.6 MiB      0.0 MiB         938               grad /= size
    44
    45    158.6 MiB      0.0 MiB       71380               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.6 MiB      0.0 MiB       70442                   ans = 0
    48    158.6 MiB      0.0 MiB      140884                   for n in ran:
    49    158.6 MiB      0.1 MiB       70442                       ans += self.loss(val, x[n], y[n])
    50    158.6 MiB      0.0 MiB       70442                   ans /= size
    51
    52    158.6 MiB      0.0 MiB       70442                   return ans
    53
    54    158.6 MiB      0.0 MiB       71380               def optim_der(val):
    55                                                         # the same as upper
    56    158.6 MiB      0.0 MiB       70442                   ans = 0
    57    158.6 MiB      0.0 MiB      140884                   for n in ran:
    58    158.6 MiB      0.0 MiB       70442                       ans += self.loss_der(val, x[n], y[n])
    59    158.6 MiB      0.0 MiB       70442                   ans /= size
    60
    61    158.6 MiB      0.0 MiB       70442                   return ans
    62
    63    158.6 MiB      0.0 MiB         938               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.6 MiB      0.0 MiB         938               res.count_of_function_calls += preres.count_call_func * size
    65    158.6 MiB      0.0 MiB         938               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.6 MiB      0.0 MiB         938               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.6 MiB      0.0 MiB         938               self.weights = preres.res
    69    158.6 MiB      0.0 MiB         938               res.add_guess(self.weights)
    70    158.6 MiB      0.0 MiB         938               count += 1
    71
    72    158.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.6 MiB      0.0 MiB           1           res.success = True
    74    158.6 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:12:58,721] Trial 41 finished with value: 8706090718.082119 and parameters: {'l_1': 0.0028996692194188543, 'l_2': 0.0062217571890612385, 'batch': 1, 'min_count': 890}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.6 MiB    158.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.6 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.6 MiB      0.0 MiB           1           count = 0
    31    158.6 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.6 MiB      0.0 MiB        1000               if self.batch != 0:
    33    158.6 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34    158.6 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.6 MiB      0.0 MiB        1000               grad = 0
    40    158.6 MiB      0.0 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42    158.6 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.6 MiB      0.0 MiB        1000               grad /= size
    44
    45    158.6 MiB      0.0 MiB       75354               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.6 MiB      0.0 MiB       74354                   ans = 0
    48    158.6 MiB      0.0 MiB      148708                   for n in ran:
    49    158.6 MiB      0.0 MiB       74354                       ans += self.loss(val, x[n], y[n])
    50    158.6 MiB      0.0 MiB       74354                   ans /= size
    51
    52    158.6 MiB      0.0 MiB       74354                   return ans
    53
    54    158.6 MiB      0.0 MiB       75354               def optim_der(val):
    55                                                         # the same as upper
    56    158.6 MiB      0.0 MiB       74354                   ans = 0
    57    158.6 MiB      0.0 MiB      148708                   for n in ran:
    58    158.6 MiB      0.0 MiB       74354                       ans += self.loss_der(val, x[n], y[n])
    59    158.6 MiB      0.0 MiB       74354                   ans /= size
    60
    61    158.6 MiB      0.0 MiB       74354                   return ans
    62
    63    158.6 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.6 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65    158.6 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.6 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.6 MiB      0.0 MiB        1000               self.weights = preres.res
    69    158.6 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70    158.6 MiB      0.0 MiB        1000               count += 1
    71
    72    158.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.6 MiB      0.0 MiB           1           res.success = True
    74    158.6 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:13:29,605] Trial 42 finished with value: 10545432618.607834 and parameters: {'l_1': 0.003135725704298336, 'l_2': 0.006677814254230805, 'batch': 1, 'min_count': 892}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.6 MiB    158.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.6 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.6 MiB      0.0 MiB           1           count = 0
    31    158.6 MiB      0.0 MiB         836           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.6 MiB      0.0 MiB         835               if self.batch != 0:
    33    158.6 MiB      0.0 MiB         835                   ran = get_n_random(self.batch, len(x))
    34    158.6 MiB      0.0 MiB         835                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.6 MiB      0.0 MiB         835               grad = 0
    40    158.6 MiB      0.0 MiB        2505               for n in ran:
    41                                                         # just like a normal derivative
    42    158.6 MiB      0.0 MiB        1670                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.6 MiB      0.0 MiB         835               grad /= size
    44
    45    158.6 MiB      0.0 MiB       64733               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.6 MiB      0.0 MiB       63898                   ans = 0
    48    158.6 MiB      0.0 MiB      191694                   for n in ran:
    49    158.6 MiB      0.0 MiB      127796                       ans += self.loss(val, x[n], y[n])
    50    158.6 MiB      0.0 MiB       63898                   ans /= size
    51
    52    158.6 MiB      0.0 MiB       63898                   return ans
    53
    54    158.6 MiB      0.0 MiB       64733               def optim_der(val):
    55                                                         # the same as upper
    56    158.6 MiB      0.0 MiB       63898                   ans = 0
    57    158.6 MiB      0.0 MiB      191694                   for n in ran:
    58    158.6 MiB      0.0 MiB      127796                       ans += self.loss_der(val, x[n], y[n])
    59    158.6 MiB      0.0 MiB       63898                   ans /= size
    60
    61    158.6 MiB      0.0 MiB       63898                   return ans
    62
    63    158.6 MiB      0.0 MiB         835               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.6 MiB      0.0 MiB         835               res.count_of_function_calls += preres.count_call_func * size
    65    158.6 MiB      0.0 MiB         835               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.6 MiB      0.0 MiB         835               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.6 MiB      0.0 MiB         835               self.weights = preres.res
    69    158.6 MiB      0.0 MiB         835               res.add_guess(self.weights)
    70    158.6 MiB      0.0 MiB         835               count += 1
    71
    72    158.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.6 MiB      0.0 MiB           1           res.success = True
    74    158.6 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:14:08,407] Trial 43 finished with value: 24342944133.774452 and parameters: {'l_1': 0.003988451421241673, 'l_2': 0.0063305272926980636, 'batch': 2, 'min_count': 831}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.6 MiB    158.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.6 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.6 MiB      0.0 MiB           1           count = 0
    31    158.6 MiB      0.0 MiB         554           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.6 MiB      0.0 MiB         553               if self.batch != 0:
    33    158.6 MiB      0.0 MiB         553                   ran = get_n_random(self.batch, len(x))
    34    158.6 MiB      0.0 MiB         553                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.6 MiB      0.0 MiB         553               grad = 0
    40    158.6 MiB      0.0 MiB        1106               for n in ran:
    41                                                         # just like a normal derivative
    42    158.6 MiB      0.0 MiB         553                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.6 MiB      0.0 MiB         553               grad /= size
    44
    45    158.6 MiB      0.0 MiB       41665               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.6 MiB      0.0 MiB       41112                   ans = 0
    48    158.6 MiB      0.0 MiB       82224                   for n in ran:
    49    158.6 MiB      0.0 MiB       41112                       ans += self.loss(val, x[n], y[n])
    50    158.6 MiB      0.0 MiB       41112                   ans /= size
    51
    52    158.6 MiB      0.0 MiB       41112                   return ans
    53
    54    158.6 MiB      0.0 MiB       41665               def optim_der(val):
    55                                                         # the same as upper
    56    158.6 MiB      0.0 MiB       41112                   ans = 0
    57    158.6 MiB      0.0 MiB       82224                   for n in ran:
    58    158.6 MiB      0.0 MiB       41112                       ans += self.loss_der(val, x[n], y[n])
    59    158.6 MiB      0.0 MiB       41112                   ans /= size
    60
    61    158.6 MiB      0.0 MiB       41112                   return ans
    62
    63    158.6 MiB      0.0 MiB         553               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.6 MiB      0.0 MiB         553               res.count_of_function_calls += preres.count_call_func * size
    65    158.6 MiB      0.0 MiB         553               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.6 MiB      0.0 MiB         553               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.6 MiB      0.0 MiB         553               self.weights = preres.res
    69    158.6 MiB      0.0 MiB         553               res.add_guess(self.weights)
    70    158.6 MiB      0.0 MiB         553               count += 1
    71
    72    158.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.6 MiB      0.0 MiB           1           res.success = True
    74    158.6 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:14:25,178] Trial 44 finished with value: 36663178747.61747 and parameters: {'l_1': 0.004362570472627964, 'l_2': 0.008274442303771428, 'batch': 1, 'min_count': 532}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.6 MiB    158.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.6 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.6 MiB      0.0 MiB           1           count = 0
    31    158.6 MiB      0.0 MiB         514           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.6 MiB      0.0 MiB         513               if self.batch != 0:
    33    158.6 MiB      0.0 MiB         513                   ran = get_n_random(self.batch, len(x))
    34    158.6 MiB      0.0 MiB         513                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.6 MiB      0.0 MiB         513               grad = 0
    40    158.6 MiB      0.0 MiB        2052               for n in ran:
    41                                                         # just like a normal derivative
    42    158.6 MiB      0.0 MiB        1539                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.6 MiB      0.0 MiB         513               grad /= size
    44
    45    158.6 MiB      0.0 MiB       40079               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.6 MiB      0.0 MiB       39566                   ans = 0
    48    158.6 MiB      0.0 MiB      158264                   for n in ran:
    49    158.6 MiB      0.0 MiB      118698                       ans += self.loss(val, x[n], y[n])
    50    158.6 MiB      0.0 MiB       39566                   ans /= size
    51
    52    158.6 MiB      0.0 MiB       39566                   return ans
    53
    54    158.6 MiB      0.0 MiB       40079               def optim_der(val):
    55                                                         # the same as upper
    56    158.6 MiB      0.0 MiB       39566                   ans = 0
    57    158.6 MiB      0.0 MiB      158264                   for n in ran:
    58    158.6 MiB      0.0 MiB      118698                       ans += self.loss_der(val, x[n], y[n])
    59    158.6 MiB      0.0 MiB       39566                   ans /= size
    60
    61    158.6 MiB      0.0 MiB       39566                   return ans
    62
    63    158.6 MiB      0.0 MiB         513               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.6 MiB      0.0 MiB         513               res.count_of_function_calls += preres.count_call_func * size
    65    158.6 MiB      0.0 MiB         513               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.6 MiB      0.0 MiB         513               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.6 MiB      0.0 MiB         513               self.weights = preres.res
    69    158.6 MiB      0.0 MiB         513               res.add_guess(self.weights)
    70    158.6 MiB      0.0 MiB         513               count += 1
    71
    72    158.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.6 MiB      0.0 MiB           1           res.success = True
    74    158.6 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:14:57,215] Trial 45 finished with value: 75138820913.45782 and parameters: {'l_1': 0.002290715567843474, 'l_2': 0.0020287514413033043, 'batch': 3, 'min_count': 336}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.6 MiB    158.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.6 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.6 MiB      0.0 MiB           1           count = 0
    31    158.6 MiB      0.0 MiB         455           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.6 MiB      0.0 MiB         454               if self.batch != 0:
    33    158.6 MiB      0.0 MiB         454                   ran = get_n_random(self.batch, len(x))
    34    158.6 MiB      0.0 MiB         454                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.6 MiB      0.0 MiB         454               grad = 0
    40    158.6 MiB      0.0 MiB         908               for n in ran:
    41                                                         # just like a normal derivative
    42    158.6 MiB      0.0 MiB         454                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.6 MiB      0.0 MiB         454               grad /= size
    44
    45    158.6 MiB      0.0 MiB       34306               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.6 MiB      0.0 MiB       33852                   ans = 0
    48    158.6 MiB      0.0 MiB       67704                   for n in ran:
    49    158.6 MiB      0.0 MiB       33852                       ans += self.loss(val, x[n], y[n])
    50    158.6 MiB      0.0 MiB       33852                   ans /= size
    51
    52    158.6 MiB      0.0 MiB       33852                   return ans
    53
    54    158.6 MiB      0.0 MiB       34306               def optim_der(val):
    55                                                         # the same as upper
    56    158.6 MiB      0.0 MiB       33852                   ans = 0
    57    158.6 MiB      0.0 MiB       67704                   for n in ran:
    58    158.6 MiB      0.0 MiB       33852                       ans += self.loss_der(val, x[n], y[n])
    59    158.6 MiB      0.0 MiB       33852                   ans /= size
    60
    61    158.6 MiB      0.0 MiB       33852                   return ans
    62
    63    158.6 MiB      0.0 MiB         454               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.6 MiB      0.0 MiB         454               res.count_of_function_calls += preres.count_call_func * size
    65    158.6 MiB      0.0 MiB         454               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.6 MiB      0.0 MiB         454               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.6 MiB      0.0 MiB         454               self.weights = preres.res
    69    158.6 MiB      0.0 MiB         454               res.add_guess(self.weights)
    70    158.6 MiB      0.0 MiB         454               count += 1
    71
    72    158.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.6 MiB      0.0 MiB           1           res.success = True
    74    158.6 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:15:11,200] Trial 46 finished with value: 30730970516.567142 and parameters: {'l_1': 0.00975054425383505, 'l_2': 0.008961755687979099, 'batch': 1, 'min_count': 413}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.7 MiB    158.7 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.7 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.7 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.7 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.7 MiB      0.0 MiB           1           count = 0
    31    158.7 MiB      0.0 MiB         930           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.7 MiB      0.0 MiB         929               if self.batch != 0:
    33    158.7 MiB      0.0 MiB         929                   ran = get_n_random(self.batch, len(x))
    34    158.7 MiB      0.0 MiB         929                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.7 MiB      0.0 MiB         929               grad = 0
    40    158.7 MiB      0.0 MiB        9290               for n in ran:
    41                                                         # just like a normal derivative
    42    158.7 MiB      0.0 MiB        8361                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.7 MiB      0.0 MiB         929               grad /= size
    44
    45    158.7 MiB      0.0 MiB       73111               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.7 MiB      0.0 MiB       72182                   ans = 0
    48    158.7 MiB      0.0 MiB      721820                   for n in ran:
    49    158.7 MiB      0.0 MiB      649638                       ans += self.loss(val, x[n], y[n])
    50    158.7 MiB      0.0 MiB       72182                   ans /= size
    51
    52    158.7 MiB      0.0 MiB       72182                   return ans
    53
    54    158.7 MiB      0.0 MiB       73111               def optim_der(val):
    55                                                         # the same as upper
    56    158.7 MiB      0.0 MiB       72182                   ans = 0
    57    158.7 MiB      0.0 MiB      721820                   for n in ran:
    58    158.7 MiB      0.0 MiB      649638                       ans += self.loss_der(val, x[n], y[n])
    59    158.7 MiB      0.0 MiB       72182                   ans /= size
    60
    61    158.7 MiB      0.0 MiB       72182                   return ans
    62
    63    158.7 MiB      0.0 MiB         929               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.7 MiB      0.0 MiB         929               res.count_of_function_calls += preres.count_call_func * size
    65    158.7 MiB      0.0 MiB         929               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.7 MiB      0.0 MiB         929               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.7 MiB      0.0 MiB         929               self.weights = preres.res
    69    158.7 MiB      0.0 MiB         929               res.add_guess(self.weights)
    70    158.7 MiB      0.0 MiB         929               count += 1
    71
    72    158.7 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.7 MiB      0.0 MiB           1           res.success = True
    74    158.7 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:17:34,703] Trial 47 finished with value: 136575950145.09067 and parameters: {'l_1': 0.0027926547972083257, 'l_2': 0.007263571723055302, 'batch': 9, 'min_count': 914}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.7 MiB    158.7 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.7 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.7 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.7 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.7 MiB      0.0 MiB           1           count = 0
    31    158.7 MiB      0.0 MiB         922           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.7 MiB      0.0 MiB         921               if self.batch != 0:
    33    158.7 MiB      0.0 MiB         921                   ran = get_n_random(self.batch, len(x))
    34    158.7 MiB      0.0 MiB         921                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.7 MiB      0.0 MiB         921               grad = 0
    40    158.7 MiB      0.0 MiB        4605               for n in ran:
    41                                                         # just like a normal derivative
    42    158.7 MiB      0.0 MiB        3684                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.7 MiB      0.0 MiB         921               grad /= size
    44
    45    158.7 MiB      0.0 MiB       72221               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.7 MiB      0.0 MiB       71300                   ans = 0
    48    158.7 MiB      0.0 MiB      356500                   for n in ran:
    49    158.7 MiB      0.0 MiB      285200                       ans += self.loss(val, x[n], y[n])
    50    158.7 MiB      0.0 MiB       71300                   ans /= size
    51
    52    158.7 MiB      0.0 MiB       71300                   return ans
    53
    54    158.7 MiB      0.0 MiB       72221               def optim_der(val):
    55                                                         # the same as upper
    56    158.7 MiB      0.0 MiB       71300                   ans = 0
    57    158.7 MiB      0.0 MiB      356500                   for n in ran:
    58    158.7 MiB      0.0 MiB      285200                       ans += self.loss_der(val, x[n], y[n])
    59    158.7 MiB      0.0 MiB       71300                   ans /= size
    60
    61    158.7 MiB      0.0 MiB       71300                   return ans
    62
    63    158.7 MiB      0.0 MiB         921               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.7 MiB      0.0 MiB         921               res.count_of_function_calls += preres.count_call_func * size
    65    158.7 MiB      0.0 MiB         921               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.7 MiB      0.0 MiB         921               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.7 MiB      0.0 MiB         921               self.weights = preres.res
    69    158.7 MiB      0.0 MiB         921               res.add_guess(self.weights)
    70    158.7 MiB      0.0 MiB         921               count += 1
    71
    72    158.7 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.7 MiB      0.0 MiB           1           res.success = True
    74    158.7 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:18:46,582] Trial 48 finished with value: 315770658675.80896 and parameters: {'l_1': 0.003740393291678418, 'l_2': 0.007766095878230437, 'batch': 4, 'min_count': 758}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.7 MiB    158.7 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.7 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.7 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.7 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.7 MiB      0.0 MiB           1           count = 0
    31    158.7 MiB      0.0 MiB         714           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.7 MiB      0.0 MiB         713               if self.batch != 0:
    33    158.7 MiB      0.0 MiB         713                   ran = get_n_random(self.batch, len(x))
    34    158.7 MiB      0.0 MiB         713                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.7 MiB      0.0 MiB         713               grad = 0
    40    158.7 MiB      0.0 MiB        2139               for n in ran:
    41                                                         # just like a normal derivative
    42    158.7 MiB      0.0 MiB        1426                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.7 MiB      0.0 MiB         713               grad /= size
    44
    45    158.7 MiB      0.0 MiB       55359               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.7 MiB      0.0 MiB       54646                   ans = 0
    48    158.7 MiB      0.0 MiB      163938                   for n in ran:
    49    158.7 MiB      0.0 MiB      109292                       ans += self.loss(val, x[n], y[n])
    50    158.7 MiB      0.0 MiB       54646                   ans /= size
    51
    52    158.7 MiB      0.0 MiB       54646                   return ans
    53
    54    158.7 MiB      0.0 MiB       55359               def optim_der(val):
    55                                                         # the same as upper
    56    158.7 MiB      0.0 MiB       54646                   ans = 0
    57    158.7 MiB      0.0 MiB      163938                   for n in ran:
    58    158.7 MiB      0.0 MiB      109292                       ans += self.loss_der(val, x[n], y[n])
    59    158.7 MiB      0.0 MiB       54646                   ans /= size
    60
    61    158.7 MiB      0.0 MiB       54646                   return ans
    62
    63    158.7 MiB      0.0 MiB         713               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.7 MiB      0.0 MiB         713               res.count_of_function_calls += preres.count_call_func * size
    65    158.7 MiB      0.0 MiB         713               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.7 MiB      0.0 MiB         713               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.7 MiB      0.0 MiB         713               self.weights = preres.res
    69    158.7 MiB      0.0 MiB         713               res.add_guess(self.weights)
    70    158.7 MiB      0.0 MiB         713               count += 1
    71
    72    158.7 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.7 MiB      0.0 MiB           1           res.success = True
    74    158.7 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:19:19,919] Trial 49 finished with value: 123353228725.81087 and parameters: {'l_1': 0.0018093586645584196, 'l_2': 0.004417158794825701, 'batch': 2, 'min_count': 684}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.7 MiB    158.7 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.7 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.7 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.7 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.7 MiB      0.0 MiB           1           count = 0
    31    158.7 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.7 MiB      0.0 MiB        1000               if self.batch != 0:
    33    158.7 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34    158.7 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.7 MiB      0.0 MiB        1000               grad = 0
    40    158.7 MiB      0.0 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42    158.7 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.7 MiB      0.0 MiB        1000               grad /= size
    44
    45    158.7 MiB      0.0 MiB       75180               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.7 MiB      0.0 MiB       74180                   ans = 0
    48    158.7 MiB      0.0 MiB      148360                   for n in ran:
    49    158.7 MiB      0.0 MiB       74180                       ans += self.loss(val, x[n], y[n])
    50    158.7 MiB      0.0 MiB       74180                   ans /= size
    51
    52    158.7 MiB      0.0 MiB       74180                   return ans
    53
    54    158.7 MiB      0.0 MiB       75180               def optim_der(val):
    55                                                         # the same as upper
    56    158.7 MiB      0.0 MiB       74180                   ans = 0
    57    158.7 MiB      0.0 MiB      148360                   for n in ran:
    58    158.7 MiB      0.0 MiB       74180                       ans += self.loss_der(val, x[n], y[n])
    59    158.7 MiB      0.0 MiB       74180                   ans /= size
    60
    61    158.7 MiB      0.0 MiB       74180                   return ans
    62
    63    158.7 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.7 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65    158.7 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.7 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.7 MiB      0.0 MiB        1000               self.weights = preres.res
    69    158.7 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70    158.7 MiB      0.0 MiB        1000               count += 1
    71
    72    158.7 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73    158.7 MiB      0.0 MiB           1           res.success = True
    74    158.7 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:19:50,660] Trial 50 finished with value: 478745859020.81415 and parameters: {'l_1': 0.00483678948442688, 'l_2': 0.006478304957764652, 'batch': 1, 'min_count': 860}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24    158.7 MiB    158.7 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26    158.7 MiB      0.0 MiB           1           res = common.StateResult()
    27    158.7 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28    158.7 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30    158.7 MiB      0.0 MiB           1           count = 0
    31    158.7 MiB -63372.8 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32    158.7 MiB -63276.8 MiB        1000               if self.batch != 0:
    33    158.7 MiB -63276.8 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34    158.7 MiB -63276.8 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39    158.7 MiB -63276.8 MiB        1000               grad = 0
    40    158.7 MiB -126553.6 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42    158.7 MiB -63276.8 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43    158.7 MiB -63276.8 MiB        1000               grad /= size
    44
    45    158.7 MiB -4802599.9 MiB       75924               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47    158.7 MiB -4739326.8 MiB       74924                   ans = 0
    48    158.7 MiB -9478684.1 MiB      149848                   for n in ran:
    49    158.7 MiB -4739349.5 MiB       74924                       ans += self.loss(val, x[n], y[n])
    50    158.7 MiB -4739358.2 MiB       74924                   ans /= size
    51
    52    158.7 MiB -4739362.0 MiB       74924                   return ans
    53
    54    158.7 MiB -4802686.9 MiB       75924               def optim_der(val):
    55                                                         # the same as upper
    56    158.7 MiB -4739413.5 MiB       74924                   ans = 0
    57    158.7 MiB -9478858.0 MiB      149848                   for n in ran:
    58    158.7 MiB -4739437.4 MiB       74924                       ans += self.loss_der(val, x[n], y[n])
    59    158.7 MiB -4739446.0 MiB       74924                   ans /= size
    60
    61    158.7 MiB -4739449.3 MiB       74924                   return ans
    62
    63    158.7 MiB -63372.8 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64    158.7 MiB -63372.8 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65    158.7 MiB -63372.8 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66    158.7 MiB -63372.8 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68    158.7 MiB -63372.8 MiB        1000               self.weights = preres.res
    69    158.7 MiB -63372.8 MiB        1000               res.add_guess(self.weights)
    70    158.7 MiB -63372.8 MiB        1000               count += 1
    71
    72     62.7 MiB    -96.0 MiB           1           res.add_guess(self.weights)
    73     62.7 MiB      0.0 MiB           1           res.success = True
    74     62.7 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:20:21,849] Trial 51 finished with value: 2208894038851.868 and parameters: {'l_1': 0.0030897661289566447, 'l_2': 0.005890089197893185, 'batch': 1, 'min_count': 976}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     62.8 MiB     62.8 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     62.8 MiB      0.0 MiB           1           res = common.StateResult()
    27     62.8 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     62.8 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     62.8 MiB      0.0 MiB           1           count = 0
    31     62.8 MiB      0.0 MiB         987           while (not self.stop(res) or min_count > count) and max_count > count:
    32     62.8 MiB      0.0 MiB         986               if self.batch != 0:
    33     62.8 MiB      0.0 MiB         986                   ran = get_n_random(self.batch, len(x))
    34     62.8 MiB      0.0 MiB         986                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     62.8 MiB      0.0 MiB         986               grad = 0
    40     62.8 MiB      0.0 MiB        1972               for n in ran:
    41                                                         # just like a normal derivative
    42     62.8 MiB      0.0 MiB         986                   grad += self.loss_der(self.weights, x[n], y[n])
    43     62.8 MiB      0.0 MiB         986               grad /= size
    44
    45     62.8 MiB      0.0 MiB       74230               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     62.8 MiB      0.0 MiB       73244                   ans = 0
    48     62.8 MiB      0.0 MiB      146488                   for n in ran:
    49     62.8 MiB      0.0 MiB       73244                       ans += self.loss(val, x[n], y[n])
    50     62.8 MiB      0.0 MiB       73244                   ans /= size
    51
    52     62.8 MiB      0.0 MiB       73244                   return ans
    53
    54     62.8 MiB      0.0 MiB       74230               def optim_der(val):
    55                                                         # the same as upper
    56     62.8 MiB      0.0 MiB       73244                   ans = 0
    57     62.8 MiB      0.0 MiB      146488                   for n in ran:
    58     62.8 MiB      0.0 MiB       73244                       ans += self.loss_der(val, x[n], y[n])
    59     62.8 MiB      0.0 MiB       73244                   ans /= size
    60
    61     62.8 MiB      0.0 MiB       73244                   return ans
    62
    63     62.8 MiB      0.0 MiB         986               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     62.8 MiB      0.0 MiB         986               res.count_of_function_calls += preres.count_call_func * size
    65     62.8 MiB      0.0 MiB         986               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     62.8 MiB      0.0 MiB         986               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     62.8 MiB      0.0 MiB         986               self.weights = preres.res
    69     62.8 MiB      0.0 MiB         986               res.add_guess(self.weights)
    70     62.8 MiB      0.0 MiB         986               count += 1
    71
    72     62.8 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     62.8 MiB      0.0 MiB           1           res.success = True
    74     62.8 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:20:51,999] Trial 52 finished with value: 68809986253.05537 and parameters: {'l_1': 0.003567354178538263, 'l_2': 0.00517209602241275, 'batch': 1, 'min_count': 934}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     62.8 MiB     62.8 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     62.8 MiB      0.0 MiB           1           res = common.StateResult()
    27     62.8 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     62.8 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     62.8 MiB      0.0 MiB           1           count = 0
    31     62.8 MiB      0.0 MiB         564           while (not self.stop(res) or min_count > count) and max_count > count:
    32     62.8 MiB      0.0 MiB         563               if self.batch != 0:
    33     62.8 MiB      0.0 MiB         563                   ran = get_n_random(self.batch, len(x))
    34     62.8 MiB      0.0 MiB         563                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     62.8 MiB      0.0 MiB         563               grad = 0
    40     62.8 MiB      0.0 MiB        1689               for n in ran:
    41                                                         # just like a normal derivative
    42     62.8 MiB      0.0 MiB        1126                   grad += self.loss_der(self.weights, x[n], y[n])
    43     62.8 MiB      0.0 MiB         563               grad /= size
    44
    45     62.8 MiB      0.0 MiB       43697               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     62.8 MiB      0.0 MiB       43134                   ans = 0
    48     62.8 MiB      0.0 MiB      129402                   for n in ran:
    49     62.8 MiB      0.0 MiB       86268                       ans += self.loss(val, x[n], y[n])
    50     62.8 MiB      0.0 MiB       43134                   ans /= size
    51
    52     62.8 MiB      0.0 MiB       43134                   return ans
    53
    54     62.8 MiB      0.0 MiB       43697               def optim_der(val):
    55                                                         # the same as upper
    56     62.8 MiB      0.0 MiB       43134                   ans = 0
    57     62.8 MiB      0.0 MiB      129402                   for n in ran:
    58     62.8 MiB      0.0 MiB       86268                       ans += self.loss_der(val, x[n], y[n])
    59     62.8 MiB      0.0 MiB       43134                   ans /= size
    60
    61     62.8 MiB      0.0 MiB       43134                   return ans
    62
    63     62.8 MiB      0.0 MiB         563               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     62.8 MiB      0.0 MiB         563               res.count_of_function_calls += preres.count_call_func * size
    65     62.8 MiB      0.0 MiB         563               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     62.8 MiB      0.0 MiB         563               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     62.8 MiB      0.0 MiB         563               self.weights = preres.res
    69     62.8 MiB      0.0 MiB         563               res.add_guess(self.weights)
    70     62.8 MiB      0.0 MiB         563               count += 1
    71
    72     62.8 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     62.8 MiB      0.0 MiB           1           res.success = True
    74     62.8 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:21:18,384] Trial 53 finished with value: 21565392002.050728 and parameters: {'l_1': 0.0042428302926995716, 'l_2': 0.005667668728918565, 'batch': 2, 'min_count': 378}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     62.8 MiB     62.8 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     62.8 MiB      0.0 MiB           1           res = common.StateResult()
    27     62.8 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     62.8 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     62.8 MiB      0.0 MiB           1           count = 0
    31     62.8 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32     62.8 MiB      0.0 MiB        1000               if self.batch != 0:
    33     62.8 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34     62.8 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     62.8 MiB      0.0 MiB        1000               grad = 0
    40     62.8 MiB      0.0 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42     62.8 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43     62.8 MiB      0.0 MiB        1000               grad /= size
    44
    45     62.8 MiB      0.0 MiB       75504               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     62.8 MiB      0.0 MiB       74504                   ans = 0
    48     62.8 MiB      0.0 MiB      149008                   for n in ran:
    49     62.8 MiB      0.0 MiB       74504                       ans += self.loss(val, x[n], y[n])
    50     62.8 MiB      0.0 MiB       74504                   ans /= size
    51
    52     62.8 MiB      0.0 MiB       74504                   return ans
    53
    54     62.8 MiB      0.0 MiB       75504               def optim_der(val):
    55                                                         # the same as upper
    56     62.8 MiB      0.0 MiB       74504                   ans = 0
    57     62.8 MiB      0.0 MiB      149008                   for n in ran:
    58     62.8 MiB      0.0 MiB       74504                       ans += self.loss_der(val, x[n], y[n])
    59     62.8 MiB      0.0 MiB       74504                   ans /= size
    60
    61     62.8 MiB      0.0 MiB       74504                   return ans
    62
    63     62.8 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     62.8 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65     62.8 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     62.8 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     62.8 MiB      0.0 MiB        1000               self.weights = preres.res
    69     62.8 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70     62.8 MiB      0.0 MiB        1000               count += 1
    71
    72     62.8 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     62.8 MiB      0.0 MiB           1           res.success = True
    74     62.8 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:21:49,293] Trial 54 finished with value: 5044182988509.278 and parameters: {'l_1': 0.002315892591530794, 'l_2': 0.006046502233483711, 'batch': 1, 'min_count': 964}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     62.8 MiB     62.8 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     62.8 MiB      0.0 MiB           1           res = common.StateResult()
    27     62.8 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     62.8 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     62.8 MiB      0.0 MiB           1           count = 0
    31     62.8 MiB      0.0 MiB         910           while (not self.stop(res) or min_count > count) and max_count > count:
    32     62.8 MiB      0.0 MiB         909               if self.batch != 0:
    33     62.8 MiB      0.0 MiB         909                   ran = get_n_random(self.batch, len(x))
    34     62.8 MiB      0.0 MiB         909                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     62.8 MiB      0.0 MiB         909               grad = 0
    40     62.8 MiB      0.0 MiB        2727               for n in ran:
    41                                                         # just like a normal derivative
    42     62.8 MiB      0.0 MiB        1818                   grad += self.loss_der(self.weights, x[n], y[n])
    43     62.8 MiB      0.0 MiB         909               grad /= size
    44
    45     62.8 MiB      0.0 MiB       70279               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     62.8 MiB      0.0 MiB       69370                   ans = 0
    48     62.8 MiB      0.0 MiB      208110                   for n in ran:
    49     62.8 MiB      0.0 MiB      138740                       ans += self.loss(val, x[n], y[n])
    50     62.8 MiB      0.0 MiB       69370                   ans /= size
    51
    52     62.8 MiB      0.0 MiB       69370                   return ans
    53
    54     62.8 MiB      0.0 MiB       70279               def optim_der(val):
    55                                                         # the same as upper
    56     62.8 MiB      0.0 MiB       69370                   ans = 0
    57     62.8 MiB      0.0 MiB      208110                   for n in ran:
    58     62.8 MiB      0.0 MiB      138740                       ans += self.loss_der(val, x[n], y[n])
    59     62.8 MiB      0.0 MiB       69370                   ans /= size
    60
    61     62.8 MiB      0.0 MiB       69370                   return ans
    62
    63     62.8 MiB      0.0 MiB         909               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     62.8 MiB      0.0 MiB         909               res.count_of_function_calls += preres.count_call_func * size
    65     62.8 MiB      0.0 MiB         909               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     62.8 MiB      0.0 MiB         909               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     62.8 MiB      0.0 MiB         909               self.weights = preres.res
    69     62.8 MiB      0.0 MiB         909               res.add_guess(self.weights)
    70     62.8 MiB      0.0 MiB         909               count += 1
    71
    72     62.8 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     62.8 MiB      0.0 MiB           1           res.success = True
    74     62.8 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:22:31,065] Trial 55 finished with value: 36724507415.415565 and parameters: {'l_1': 0.002836872516861222, 'l_2': 0.007086524999888383, 'batch': 2, 'min_count': 871}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     62.8 MiB     62.8 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     62.8 MiB      0.0 MiB           1           res = common.StateResult()
    27     62.8 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     62.8 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     62.8 MiB      0.0 MiB           1           count = 0
    31     62.9 MiB      0.0 MiB         919           while (not self.stop(res) or min_count > count) and max_count > count:
    32     62.9 MiB      0.0 MiB         918               if self.batch != 0:
    33     62.9 MiB      0.0 MiB         918                   ran = get_n_random(self.batch, len(x))
    34     62.9 MiB      0.0 MiB         918                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     62.9 MiB      0.0 MiB         918               grad = 0
    40     62.9 MiB      0.0 MiB        3672               for n in ran:
    41                                                         # just like a normal derivative
    42     62.9 MiB      0.0 MiB        2754                   grad += self.loss_der(self.weights, x[n], y[n])
    43     62.9 MiB      0.0 MiB         918               grad /= size
    44
    45     62.9 MiB      0.0 MiB       71944               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     62.9 MiB      0.0 MiB       71026                   ans = 0
    48     62.9 MiB      0.0 MiB      284104                   for n in ran:
    49     62.9 MiB      0.0 MiB      213078                       ans += self.loss(val, x[n], y[n])
    50     62.9 MiB      0.0 MiB       71026                   ans /= size
    51
    52     62.9 MiB      0.0 MiB       71026                   return ans
    53
    54     62.9 MiB      0.0 MiB       71944               def optim_der(val):
    55                                                         # the same as upper
    56     62.9 MiB      0.0 MiB       71026                   ans = 0
    57     62.9 MiB      0.0 MiB      284104                   for n in ran:
    58     62.9 MiB      0.0 MiB      213078                       ans += self.loss_der(val, x[n], y[n])
    59     62.9 MiB      0.0 MiB       71026                   ans /= size
    60
    61     62.9 MiB      0.0 MiB       71026                   return ans
    62
    63     62.9 MiB      0.0 MiB         918               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     62.9 MiB      0.0 MiB         918               res.count_of_function_calls += preres.count_call_func * size
    65     62.9 MiB      0.0 MiB         918               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     62.9 MiB      0.0 MiB         918               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     62.9 MiB      0.0 MiB         918               self.weights = preres.res
    69     62.9 MiB      0.0 MiB         918               res.add_guess(self.weights)
    70     62.9 MiB      0.0 MiB         918               count += 1
    71
    72     62.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     62.9 MiB      0.0 MiB           1           res.success = True
    74     62.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:23:28,110] Trial 56 finished with value: 271314606141.5385 and parameters: {'l_1': 0.0032652111324826135, 'l_2': 0.003501126497304944, 'batch': 3, 'min_count': 783}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     62.9 MiB     62.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     62.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     62.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     62.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     62.9 MiB      0.0 MiB           1           count = 0
    31     62.9 MiB      0.0 MiB         449           while (not self.stop(res) or min_count > count) and max_count > count:
    32     62.9 MiB      0.0 MiB         448               if self.batch != 0:
    33     62.9 MiB      0.0 MiB         448                   ran = get_n_random(self.batch, len(x))
    34     62.9 MiB      0.0 MiB         448                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     62.9 MiB      0.0 MiB         448               grad = 0
    40     62.9 MiB      0.0 MiB         896               for n in ran:
    41                                                         # just like a normal derivative
    42     62.9 MiB      0.0 MiB         448                   grad += self.loss_der(self.weights, x[n], y[n])
    43     62.9 MiB      0.0 MiB         448               grad /= size
    44
    45     62.9 MiB      0.0 MiB       33594               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     62.9 MiB      0.0 MiB       33146                   ans = 0
    48     62.9 MiB      0.0 MiB       66292                   for n in ran:
    49     62.9 MiB      0.0 MiB       33146                       ans += self.loss(val, x[n], y[n])
    50     62.9 MiB      0.0 MiB       33146                   ans /= size
    51
    52     62.9 MiB      0.0 MiB       33146                   return ans
    53
    54     62.9 MiB      0.0 MiB       33594               def optim_der(val):
    55                                                         # the same as upper
    56     62.9 MiB      0.0 MiB       33146                   ans = 0
    57     62.9 MiB      0.0 MiB       66292                   for n in ran:
    58     62.9 MiB      0.0 MiB       33146                       ans += self.loss_der(val, x[n], y[n])
    59     62.9 MiB      0.0 MiB       33146                   ans /= size
    60
    61     62.9 MiB      0.0 MiB       33146                   return ans
    62
    63     62.9 MiB      0.0 MiB         448               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     62.9 MiB      0.0 MiB         448               res.count_of_function_calls += preres.count_call_func * size
    65     62.9 MiB      0.0 MiB         448               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     62.9 MiB      0.0 MiB         448               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     62.9 MiB      0.0 MiB         448               self.weights = preres.res
    69     62.9 MiB      0.0 MiB         448               res.add_guess(self.weights)
    70     62.9 MiB      0.0 MiB         448               count += 1
    71
    72     62.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     62.9 MiB      0.0 MiB           1           res.success = True
    74     62.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:23:41,525] Trial 57 finished with value: 45692144311.86314 and parameters: {'l_1': 0.0007468893149377555, 'l_2': 0.007981005815379796, 'batch': 1, 'min_count': 226}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     62.9 MiB     62.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     62.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     62.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     62.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     62.9 MiB      0.0 MiB           1           count = 0
    31     62.9 MiB      0.0 MiB         327           while (not self.stop(res) or min_count > count) and max_count > count:
    32     62.9 MiB      0.0 MiB         326               if self.batch != 0:
    33     62.9 MiB      0.0 MiB         326                   ran = get_n_random(self.batch, len(x))
    34     62.9 MiB      0.0 MiB         326                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     62.9 MiB      0.0 MiB         326               grad = 0
    40     62.9 MiB      0.0 MiB         978               for n in ran:
    41                                                         # just like a normal derivative
    42     62.9 MiB      0.0 MiB         652                   grad += self.loss_der(self.weights, x[n], y[n])
    43     62.9 MiB      0.0 MiB         326               grad /= size
    44
    45     62.9 MiB      0.0 MiB       25534               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     62.9 MiB      0.0 MiB       25208                   ans = 0
    48     62.9 MiB      0.0 MiB       75624                   for n in ran:
    49     62.9 MiB      0.0 MiB       50416                       ans += self.loss(val, x[n], y[n])
    50     62.9 MiB      0.0 MiB       25208                   ans /= size
    51
    52     62.9 MiB      0.0 MiB       25208                   return ans
    53
    54     62.9 MiB      0.0 MiB       25534               def optim_der(val):
    55                                                         # the same as upper
    56     62.9 MiB      0.0 MiB       25208                   ans = 0
    57     62.9 MiB      0.0 MiB       75624                   for n in ran:
    58     62.9 MiB      0.0 MiB       50416                       ans += self.loss_der(val, x[n], y[n])
    59     62.9 MiB      0.0 MiB       25208                   ans /= size
    60
    61     62.9 MiB      0.0 MiB       25208                   return ans
    62
    63     62.9 MiB      0.0 MiB         326               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     62.9 MiB      0.0 MiB         326               res.count_of_function_calls += preres.count_call_func * size
    65     62.9 MiB      0.0 MiB         326               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     62.9 MiB      0.0 MiB         326               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     62.9 MiB      0.0 MiB         326               self.weights = preres.res
    69     62.9 MiB      0.0 MiB         326               res.add_guess(self.weights)
    70     62.9 MiB      0.0 MiB         326               count += 1
    71
    72     62.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     62.9 MiB      0.0 MiB           1           res.success = True
    74     62.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:23:56,649] Trial 58 finished with value: 13335309563.106218 and parameters: {'l_1': 0.0014572204404829044, 'l_2': 0.008541313394524925, 'batch': 2, 'min_count': 317}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     62.9 MiB     62.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     62.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     62.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     62.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     62.9 MiB      0.0 MiB           1           count = 0
    31     62.9 MiB      0.0 MiB         695           while (not self.stop(res) or min_count > count) and max_count > count:
    32     62.9 MiB      0.0 MiB         694               if self.batch != 0:
    33     62.9 MiB      0.0 MiB         694                   ran = get_n_random(self.batch, len(x))
    34     62.9 MiB      0.0 MiB         694                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     62.9 MiB      0.0 MiB         694               grad = 0
    40     62.9 MiB      0.0 MiB        1388               for n in ran:
    41                                                         # just like a normal derivative
    42     62.9 MiB      0.0 MiB         694                   grad += self.loss_der(self.weights, x[n], y[n])
    43     62.9 MiB      0.0 MiB         694               grad /= size
    44
    45     62.9 MiB      0.0 MiB       52394               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     62.9 MiB      0.0 MiB       51700                   ans = 0
    48     62.9 MiB      0.0 MiB      103400                   for n in ran:
    49     62.9 MiB      0.0 MiB       51700                       ans += self.loss(val, x[n], y[n])
    50     62.9 MiB      0.0 MiB       51700                   ans /= size
    51
    52     62.9 MiB      0.0 MiB       51700                   return ans
    53
    54     62.9 MiB      0.0 MiB       52394               def optim_der(val):
    55                                                         # the same as upper
    56     62.9 MiB      0.0 MiB       51700                   ans = 0
    57     62.9 MiB      0.0 MiB      103400                   for n in ran:
    58     62.9 MiB      0.0 MiB       51700                       ans += self.loss_der(val, x[n], y[n])
    59     62.9 MiB      0.0 MiB       51700                   ans /= size
    60
    61     62.9 MiB      0.0 MiB       51700                   return ans
    62
    63     62.9 MiB      0.0 MiB         694               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     62.9 MiB      0.0 MiB         694               res.count_of_function_calls += preres.count_call_func * size
    65     62.9 MiB      0.0 MiB         694               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     62.9 MiB      0.0 MiB         694               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     62.9 MiB      0.0 MiB         694               self.weights = preres.res
    69     62.9 MiB      0.0 MiB         694               res.add_guess(self.weights)
    70     62.9 MiB      0.0 MiB         694               count += 1
    71
    72     62.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     62.9 MiB      0.0 MiB           1           res.success = True
    74     62.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:24:17,522] Trial 59 finished with value: 45395940885.10722 and parameters: {'l_1': 0.0019973545501339026, 'l_2': 0.009568132836772766, 'batch': 1, 'min_count': 268}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     63.0 MiB     63.0 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     63.0 MiB      0.0 MiB           1           res = common.StateResult()
    27     63.0 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     63.0 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     63.0 MiB      0.0 MiB           1           count = 0
    31     63.0 MiB    -42.8 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32     63.0 MiB    -40.6 MiB        1000               if self.batch != 0:
    33     63.0 MiB    -40.6 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34     63.0 MiB    -40.6 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     63.0 MiB    -40.6 MiB        1000               grad = 0
    40     63.0 MiB   -121.7 MiB        3000               for n in ran:
    41                                                         # just like a normal derivative
    42     63.0 MiB    -81.1 MiB        2000                   grad += self.loss_der(self.weights, x[n], y[n])
    43     63.0 MiB    -40.6 MiB        1000               grad /= size
    44
    45     63.0 MiB  -3388.1 MiB       77198               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     63.0 MiB  -3347.6 MiB       76198                   ans = 0
    48     63.0 MiB -10043.8 MiB      228594                   for n in ran:
    49     63.0 MiB  -6696.1 MiB      152396                       ans += self.loss(val, x[n], y[n])
    50     63.0 MiB  -3348.4 MiB       76198                   ans /= size
    51
    52     63.0 MiB  -3348.4 MiB       76198                   return ans
    53
    54     63.0 MiB  -3390.0 MiB       77198               def optim_der(val):
    55                                                         # the same as upper
    56     63.0 MiB  -3349.4 MiB       76198                   ans = 0
    57     63.0 MiB -10049.7 MiB      228594                   for n in ran:
    58     63.0 MiB  -6700.1 MiB      152396                       ans += self.loss_der(val, x[n], y[n])
    59     63.0 MiB  -3350.4 MiB       76198                   ans /= size
    60
    61     63.0 MiB  -3350.4 MiB       76198                   return ans
    62
    63     63.0 MiB    -42.8 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     63.0 MiB    -42.8 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65     63.0 MiB    -42.8 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     63.0 MiB    -42.8 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     63.0 MiB    -42.8 MiB        1000               self.weights = preres.res
    69     63.0 MiB    -42.8 MiB        1000               res.add_guess(self.weights)
    70     63.0 MiB    -42.8 MiB        1000               count += 1
    71
    72     60.7 MiB     -2.3 MiB           1           res.add_guess(self.weights)
    73     60.7 MiB      0.0 MiB           1           res.success = True
    74     60.7 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:25:03,070] Trial 60 finished with value: 29376047052.65397 and parameters: {'l_1': 0.0025638712599798143, 'l_2': 0.004729539391577425, 'batch': 2, 'min_count': 997}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     60.8 MiB     60.8 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     60.8 MiB      0.0 MiB           1           res = common.StateResult()
    27     60.8 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     60.8 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     60.8 MiB      0.0 MiB           1           count = 0
    31     60.9 MiB      0.0 MiB         991           while (not self.stop(res) or min_count > count) and max_count > count:
    32     60.9 MiB      0.0 MiB         990               if self.batch != 0:
    33     60.9 MiB      0.0 MiB         990                   ran = get_n_random(self.batch, len(x))
    34     60.9 MiB      0.0 MiB         990                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     60.9 MiB      0.0 MiB         990               grad = 0
    40     60.9 MiB      0.0 MiB        1980               for n in ran:
    41                                                         # just like a normal derivative
    42     60.9 MiB      0.0 MiB         990                   grad += self.loss_der(self.weights, x[n], y[n])
    43     60.9 MiB      0.0 MiB         990               grad /= size
    44
    45     60.9 MiB      0.0 MiB       74388               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     60.9 MiB      0.0 MiB       73398                   ans = 0
    48     60.9 MiB      0.0 MiB      146796                   for n in ran:
    49     60.9 MiB      0.0 MiB       73398                       ans += self.loss(val, x[n], y[n])
    50     60.9 MiB      0.0 MiB       73398                   ans /= size
    51
    52     60.9 MiB      0.0 MiB       73398                   return ans
    53
    54     60.9 MiB      0.0 MiB       74388               def optim_der(val):
    55                                                         # the same as upper
    56     60.9 MiB      0.0 MiB       73398                   ans = 0
    57     60.9 MiB      0.0 MiB      146796                   for n in ran:
    58     60.9 MiB      0.0 MiB       73398                       ans += self.loss_der(val, x[n], y[n])
    59     60.9 MiB      0.0 MiB       73398                   ans /= size
    60
    61     60.9 MiB      0.0 MiB       73398                   return ans
    62
    63     60.9 MiB      0.0 MiB         990               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     60.9 MiB      0.0 MiB         990               res.count_of_function_calls += preres.count_call_func * size
    65     60.9 MiB      0.0 MiB         990               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     60.9 MiB      0.0 MiB         990               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     60.9 MiB      0.0 MiB         990               self.weights = preres.res
    69     60.9 MiB      0.0 MiB         990               res.add_guess(self.weights)
    70     60.9 MiB      0.0 MiB         990               count += 1
    71
    72     60.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     60.9 MiB      0.0 MiB           1           res.success = True
    74     60.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:25:33,135] Trial 61 finished with value: 11649007729.005392 and parameters: {'l_1': 0.003086705443330603, 'l_2': 0.006704560295493542, 'batch': 1, 'min_count': 903}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     60.9 MiB     60.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     60.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     60.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     60.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     60.9 MiB      0.0 MiB           1           count = 0
    31     60.9 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32     60.9 MiB      0.0 MiB        1000               if self.batch != 0:
    33     60.9 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34     60.9 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     60.9 MiB      0.0 MiB        1000               grad = 0
    40     60.9 MiB      0.0 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42     60.9 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43     60.9 MiB      0.0 MiB        1000               grad /= size
    44
    45     60.9 MiB      0.0 MiB       75080               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     60.9 MiB      0.0 MiB       74080                   ans = 0
    48     60.9 MiB      0.0 MiB      148160                   for n in ran:
    49     60.9 MiB      0.0 MiB       74080                       ans += self.loss(val, x[n], y[n])
    50     60.9 MiB      0.0 MiB       74080                   ans /= size
    51
    52     60.9 MiB      0.0 MiB       74080                   return ans
    53
    54     60.9 MiB      0.0 MiB       75080               def optim_der(val):
    55                                                         # the same as upper
    56     60.9 MiB      0.0 MiB       74080                   ans = 0
    57     60.9 MiB      0.0 MiB      148160                   for n in ran:
    58     60.9 MiB      0.0 MiB       74080                       ans += self.loss_der(val, x[n], y[n])
    59     60.9 MiB      0.0 MiB       74080                   ans /= size
    60
    61     60.9 MiB      0.0 MiB       74080                   return ans
    62
    63     60.9 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     60.9 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65     60.9 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     60.9 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     60.9 MiB      0.0 MiB        1000               self.weights = preres.res
    69     60.9 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70     60.9 MiB      0.0 MiB        1000               count += 1
    71
    72     60.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     60.9 MiB      0.0 MiB           1           res.success = True
    74     60.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:26:03,195] Trial 62 finished with value: 84023425372.93945 and parameters: {'l_1': 0.003749535630277297, 'l_2': 0.006660912686531271, 'batch': 1, 'min_count': 867}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     60.9 MiB     60.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     60.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     60.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     60.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     60.9 MiB      0.0 MiB           1           count = 0
    31     60.9 MiB      0.0 MiB         939           while (not self.stop(res) or min_count > count) and max_count > count:
    32     60.9 MiB      0.0 MiB         938               if self.batch != 0:
    33     60.9 MiB      0.0 MiB         938                   ran = get_n_random(self.batch, len(x))
    34     60.9 MiB      0.0 MiB         938                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     60.9 MiB      0.0 MiB         938               grad = 0
    40     60.9 MiB      0.0 MiB        5628               for n in ran:
    41                                                         # just like a normal derivative
    42     60.9 MiB      0.0 MiB        4690                   grad += self.loss_der(self.weights, x[n], y[n])
    43     60.9 MiB      0.0 MiB         938               grad /= size
    44
    45     60.9 MiB      0.0 MiB       73244               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     60.9 MiB      0.0 MiB       72306                   ans = 0
    48     60.9 MiB      0.0 MiB      433836                   for n in ran:
    49     60.9 MiB      0.0 MiB      361530                       ans += self.loss(val, x[n], y[n])
    50     60.9 MiB      0.0 MiB       72306                   ans /= size
    51
    52     60.9 MiB      0.0 MiB       72306                   return ans
    53
    54     60.9 MiB      0.0 MiB       73244               def optim_der(val):
    55                                                         # the same as upper
    56     60.9 MiB      0.0 MiB       72306                   ans = 0
    57     60.9 MiB      0.0 MiB      433836                   for n in ran:
    58     60.9 MiB      0.0 MiB      361530                       ans += self.loss_der(val, x[n], y[n])
    59     60.9 MiB      0.0 MiB       72306                   ans /= size
    60
    61     60.9 MiB      0.0 MiB       72306                   return ans
    62
    63     60.9 MiB      0.0 MiB         938               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     60.9 MiB      0.0 MiB         938               res.count_of_function_calls += preres.count_call_func * size
    65     60.9 MiB      0.0 MiB         938               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     60.9 MiB      0.0 MiB         938               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     60.9 MiB      0.0 MiB         938               self.weights = preres.res
    69     60.9 MiB      0.0 MiB         938               res.add_guess(self.weights)
    70     60.9 MiB      0.0 MiB         938               count += 1
    71
    72     60.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     60.9 MiB      0.0 MiB           1           res.success = True
    74     60.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:27:27,958] Trial 63 finished with value: 187087152798.8312 and parameters: {'l_1': 0.0031123824226461493, 'l_2': 0.007518119071542559, 'batch': 5, 'min_count': 936}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     60.9 MiB     60.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     60.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     60.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     60.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     60.9 MiB      0.0 MiB           1           count = 0
    31     60.9 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32     60.9 MiB      0.0 MiB        1000               if self.batch != 0:
    33     60.9 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34     60.9 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     60.9 MiB      0.0 MiB        1000               grad = 0
    40     60.9 MiB      0.0 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42     60.9 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43     60.9 MiB      0.0 MiB        1000               grad /= size
    44
    45     60.9 MiB      0.0 MiB       75176               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     60.9 MiB      0.0 MiB       74176                   ans = 0
    48     60.9 MiB      0.0 MiB      148352                   for n in ran:
    49     60.9 MiB      0.0 MiB       74176                       ans += self.loss(val, x[n], y[n])
    50     60.9 MiB      0.0 MiB       74176                   ans /= size
    51
    52     60.9 MiB      0.0 MiB       74176                   return ans
    53
    54     60.9 MiB      0.0 MiB       75176               def optim_der(val):
    55                                                         # the same as upper
    56     60.9 MiB      0.0 MiB       74176                   ans = 0
    57     60.9 MiB      0.0 MiB      148352                   for n in ran:
    58     60.9 MiB      0.0 MiB       74176                       ans += self.loss_der(val, x[n], y[n])
    59     60.9 MiB      0.0 MiB       74176                   ans /= size
    60
    61     60.9 MiB      0.0 MiB       74176                   return ans
    62
    63     60.9 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     60.9 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65     60.9 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     60.9 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     60.9 MiB      0.0 MiB        1000               self.weights = preres.res
    69     60.9 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70     60.9 MiB      0.0 MiB        1000               count += 1
    71
    72     60.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     60.9 MiB      0.0 MiB           1           res.success = True
    74     60.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:27:58,271] Trial 64 finished with value: 15668742762.15506 and parameters: {'l_1': 0.0022713231466008703, 'l_2': 0.00553163010403011, 'batch': 1, 'min_count': 500}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     61.0 MiB     61.0 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     61.0 MiB      0.0 MiB           1           res = common.StateResult()
    27     61.0 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     61.0 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     61.0 MiB      0.0 MiB           1           count = 0
    31     61.0 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32     61.0 MiB      0.0 MiB        1000               if self.batch != 0:
    33     61.0 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34     61.0 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     61.0 MiB      0.0 MiB        1000               grad = 0
    40     61.0 MiB      0.0 MiB        3000               for n in ran:
    41                                                         # just like a normal derivative
    42     61.0 MiB      0.0 MiB        2000                   grad += self.loss_der(self.weights, x[n], y[n])
    43     61.0 MiB      0.0 MiB        1000               grad /= size
    44
    45     61.0 MiB      0.0 MiB       77890               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     61.0 MiB      0.0 MiB       76890                   ans = 0
    48     61.0 MiB      0.0 MiB      230670                   for n in ran:
    49     61.0 MiB      0.0 MiB      153780                       ans += self.loss(val, x[n], y[n])
    50     61.0 MiB      0.0 MiB       76890                   ans /= size
    51
    52     61.0 MiB      0.0 MiB       76890                   return ans
    53
    54     61.0 MiB      0.0 MiB       77890               def optim_der(val):
    55                                                         # the same as upper
    56     61.0 MiB      0.0 MiB       76890                   ans = 0
    57     61.0 MiB      0.0 MiB      230670                   for n in ran:
    58     61.0 MiB      0.0 MiB      153780                       ans += self.loss_der(val, x[n], y[n])
    59     61.0 MiB      0.0 MiB       76890                   ans /= size
    60
    61     61.0 MiB      0.0 MiB       76890                   return ans
    62
    63     61.0 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     61.0 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65     61.0 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     61.0 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     61.0 MiB      0.0 MiB        1000               self.weights = preres.res
    69     61.0 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70     61.0 MiB      0.0 MiB        1000               count += 1
    71
    72     61.0 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     61.0 MiB      0.0 MiB           1           res.success = True
    74     61.0 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:28:45,122] Trial 65 finished with value: 115328845660.96715 and parameters: {'l_1': 0.004179526541168273, 'l_2': 0.0064281608011588914, 'batch': 2, 'min_count': 893}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     61.0 MiB     61.0 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     61.0 MiB      0.0 MiB           1           res = common.StateResult()
    27     61.0 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     61.0 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     61.0 MiB      0.0 MiB           1           count = 0
    31     61.0 MiB      0.0 MiB         701           while (not self.stop(res) or min_count > count) and max_count > count:
    32     61.0 MiB      0.0 MiB         700               if self.batch != 0:
    33     61.0 MiB      0.0 MiB         700                   ran = get_n_random(self.batch, len(x))
    34     61.0 MiB      0.0 MiB         700                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     61.0 MiB      0.0 MiB         700               grad = 0
    40     61.0 MiB      0.0 MiB        1400               for n in ran:
    41                                                         # just like a normal derivative
    42     61.0 MiB      0.0 MiB         700                   grad += self.loss_der(self.weights, x[n], y[n])
    43     61.0 MiB      0.0 MiB         700               grad /= size
    44
    45     61.0 MiB      0.0 MiB       52964               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     61.0 MiB      0.0 MiB       52264                   ans = 0
    48     61.0 MiB      0.0 MiB      104528                   for n in ran:
    49     61.0 MiB      0.0 MiB       52264                       ans += self.loss(val, x[n], y[n])
    50     61.0 MiB      0.0 MiB       52264                   ans /= size
    51
    52     61.0 MiB      0.0 MiB       52264                   return ans
    53
    54     61.0 MiB      0.0 MiB       52964               def optim_der(val):
    55                                                         # the same as upper
    56     61.0 MiB      0.0 MiB       52264                   ans = 0
    57     61.0 MiB      0.0 MiB      104528                   for n in ran:
    58     61.0 MiB      0.0 MiB       52264                       ans += self.loss_der(val, x[n], y[n])
    59     61.0 MiB      0.0 MiB       52264                   ans /= size
    60
    61     61.0 MiB      0.0 MiB       52264                   return ans
    62
    63     61.0 MiB      0.0 MiB         700               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     61.0 MiB      0.0 MiB         700               res.count_of_function_calls += preres.count_call_func * size
    65     61.0 MiB      0.0 MiB         700               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     61.0 MiB      0.0 MiB         700               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     61.0 MiB      0.0 MiB         700               self.weights = preres.res
    69     61.0 MiB      0.0 MiB         700               res.add_guess(self.weights)
    70     61.0 MiB      0.0 MiB         700               count += 1
    71
    72     61.0 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     61.0 MiB      0.0 MiB           1           res.success = True
    74     61.0 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:29:06,685] Trial 66 finished with value: 60662422569.38403 and parameters: {'l_1': 0.0034271563760787877, 'l_2': 0.00703698732671521, 'batch': 1, 'min_count': 446}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     61.1 MiB     61.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     61.1 MiB      0.0 MiB           1           res = common.StateResult()
    27     61.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     61.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     61.1 MiB      0.0 MiB           1           count = 0
    31     61.1 MiB      0.0 MiB         932           while (not self.stop(res) or min_count > count) and max_count > count:
    32     61.1 MiB      0.0 MiB         931               if self.batch != 0:
    33     61.1 MiB      0.0 MiB         931                   ran = get_n_random(self.batch, len(x))
    34     61.1 MiB      0.0 MiB         931                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     61.1 MiB      0.0 MiB         931               grad = 0
    40     61.1 MiB      0.0 MiB        2793               for n in ran:
    41                                                         # just like a normal derivative
    42     61.1 MiB      0.0 MiB        1862                   grad += self.loss_der(self.weights, x[n], y[n])
    43     61.1 MiB      0.0 MiB         931               grad /= size
    44
    45     61.1 MiB      0.0 MiB       72415               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     61.1 MiB      0.0 MiB       71484                   ans = 0
    48     61.1 MiB      0.0 MiB      214452                   for n in ran:
    49     61.1 MiB      0.0 MiB      142968                       ans += self.loss(val, x[n], y[n])
    50     61.1 MiB      0.0 MiB       71484                   ans /= size
    51
    52     61.1 MiB      0.0 MiB       71484                   return ans
    53
    54     61.1 MiB      0.0 MiB       72415               def optim_der(val):
    55                                                         # the same as upper
    56     61.1 MiB      0.0 MiB       71484                   ans = 0
    57     61.1 MiB      0.0 MiB      214452                   for n in ran:
    58     61.1 MiB      0.0 MiB      142968                       ans += self.loss_der(val, x[n], y[n])
    59     61.1 MiB      0.0 MiB       71484                   ans /= size
    60
    61     61.1 MiB      0.0 MiB       71484                   return ans
    62
    63     61.1 MiB      0.0 MiB         931               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     61.1 MiB      0.0 MiB         931               res.count_of_function_calls += preres.count_call_func * size
    65     61.1 MiB      0.0 MiB         931               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     61.1 MiB      0.0 MiB         931               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     61.1 MiB      0.0 MiB         931               self.weights = preres.res
    69     61.1 MiB      0.0 MiB         931               res.add_guess(self.weights)
    70     61.1 MiB      0.0 MiB         931               count += 1
    71
    72     61.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     61.1 MiB      0.0 MiB           1           res.success = True
    74     61.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:29:49,601] Trial 67 finished with value: 114388144909.95981 and parameters: {'l_1': 0.002905111141838367, 'l_2': 0.006109208089866894, 'batch': 2, 'min_count': 816}. Best is trial 26 with value: 4652100560.737796.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     61.1 MiB     61.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     61.1 MiB      0.0 MiB           1           res = common.StateResult()
    27     61.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     61.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     61.1 MiB      0.0 MiB           1           count = 0
    31     61.1 MiB      0.0 MiB         229           while (not self.stop(res) or min_count > count) and max_count > count:
    32     61.1 MiB      0.0 MiB         228               if self.batch != 0:
    33     61.1 MiB      0.0 MiB         228                   ran = get_n_random(self.batch, len(x))
    34     61.1 MiB      0.0 MiB         228                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     61.1 MiB      0.0 MiB         228               grad = 0
    40     61.1 MiB      0.0 MiB         456               for n in ran:
    41                                                         # just like a normal derivative
    42     61.1 MiB      0.0 MiB         228                   grad += self.loss_der(self.weights, x[n], y[n])
    43     61.1 MiB      0.0 MiB         228               grad /= size
    44
    45     61.1 MiB      0.0 MiB       17032               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     61.1 MiB      0.0 MiB       16804                   ans = 0
    48     61.1 MiB      0.0 MiB       33608                   for n in ran:
    49     61.1 MiB      0.0 MiB       16804                       ans += self.loss(val, x[n], y[n])
    50     61.1 MiB      0.0 MiB       16804                   ans /= size
    51
    52     61.1 MiB      0.0 MiB       16804                   return ans
    53
    54     61.1 MiB      0.0 MiB       17032               def optim_der(val):
    55                                                         # the same as upper
    56     61.1 MiB      0.0 MiB       16804                   ans = 0
    57     61.1 MiB      0.0 MiB       33608                   for n in ran:
    58     61.1 MiB      0.0 MiB       16804                       ans += self.loss_der(val, x[n], y[n])
    59     61.1 MiB      0.0 MiB       16804                   ans /= size
    60
    61     61.1 MiB      0.0 MiB       16804                   return ans
    62
    63     61.1 MiB      0.0 MiB         228               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     61.1 MiB      0.0 MiB         228               res.count_of_function_calls += preres.count_call_func * size
    65     61.1 MiB      0.0 MiB         228               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     61.1 MiB      0.0 MiB         228               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     61.1 MiB      0.0 MiB         228               self.weights = preres.res
    69     61.1 MiB      0.0 MiB         228               res.add_guess(self.weights)
    70     61.1 MiB      0.0 MiB         228               count += 1
    71
    72     61.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     61.1 MiB      0.0 MiB           1           res.success = True
    74     61.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:29:56,314] Trial 68 finished with value: 3002477570.026244 and parameters: {'l_1': 0.0011155790464793206, 'l_2': 0.008327823447788912, 'batch': 1, 'min_count': 188}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     61.1 MiB     61.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     61.1 MiB      0.0 MiB           1           res = common.StateResult()
    27     61.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     61.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     61.1 MiB      0.0 MiB           1           count = 0
    31     61.1 MiB   -222.9 MiB         177           while (not self.stop(res) or min_count > count) and max_count > count:
    32     61.1 MiB   -221.1 MiB         176               if self.batch != 0:
    33     61.1 MiB   -221.1 MiB         176                   ran = get_n_random(self.batch, len(x))
    34     61.1 MiB   -221.1 MiB         176                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     61.1 MiB   -221.1 MiB         176               grad = 0
    40     61.1 MiB  -1769.0 MiB        1408               for n in ran:
    41                                                         # just like a normal derivative
    42     61.1 MiB  -1547.9 MiB        1232                   grad += self.loss_der(self.weights, x[n], y[n])
    43     61.1 MiB   -221.1 MiB         176               grad /= size
    44
    45     61.1 MiB -17715.3 MiB       13928               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     61.1 MiB -17494.2 MiB       13752                   ans = 0
    48     61.1 MiB -139955.7 MiB      110016                   for n in ran:
    49     61.1 MiB -122461.3 MiB       96264                       ans += self.loss(val, x[n], y[n])
    50     61.1 MiB -17494.9 MiB       13752                   ans /= size
    51
    52     61.1 MiB -17495.0 MiB       13752                   return ans
    53
    54     61.1 MiB -17717.8 MiB       13928               def optim_der(val):
    55                                                         # the same as upper
    56     61.1 MiB -17496.7 MiB       13752                   ans = 0
    57     61.1 MiB -139977.1 MiB      110016                   for n in ran:
    58     61.1 MiB -122480.3 MiB       96264                       ans += self.loss_der(val, x[n], y[n])
    59     61.1 MiB -17497.5 MiB       13752                   ans /= size
    60
    61     61.1 MiB -17497.6 MiB       13752                   return ans
    62
    63     61.1 MiB   -222.9 MiB         176               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     61.1 MiB   -222.9 MiB         176               res.count_of_function_calls += preres.count_call_func * size
    65     61.1 MiB   -222.9 MiB         176               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     61.1 MiB   -222.9 MiB         176               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     61.1 MiB   -222.9 MiB         176               self.weights = preres.res
    69     61.1 MiB   -222.9 MiB         176               res.add_guess(self.weights)
    70     61.1 MiB   -222.9 MiB         176               count += 1
    71
    72     59.3 MiB     -1.8 MiB           1           res.add_guess(self.weights)
    73     59.3 MiB      0.0 MiB           1           res.success = True
    74     59.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:30:17,668] Trial 69 finished with value: 19937009844.30158 and parameters: {'l_1': 0.0017399692766802335, 'l_2': 0.008941165477388584, 'batch': 7, 'min_count': 159}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.3 MiB     59.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.3 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.3 MiB      0.0 MiB           1           count = 0
    31     59.3 MiB      0.0 MiB         301           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.3 MiB      0.0 MiB         300               if self.batch != 0:
    33     59.3 MiB      0.0 MiB         300                   ran = get_n_random(self.batch, len(x))
    34     59.3 MiB      0.0 MiB         300                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.3 MiB      0.0 MiB         300               grad = 0
    40     59.3 MiB      0.0 MiB         900               for n in ran:
    41                                                         # just like a normal derivative
    42     59.3 MiB      0.0 MiB         600                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.3 MiB      0.0 MiB         300               grad /= size
    44
    45     59.3 MiB      0.0 MiB       23286               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.3 MiB      0.0 MiB       22986                   ans = 0
    48     59.3 MiB      0.0 MiB       68958                   for n in ran:
    49     59.3 MiB      0.0 MiB       45972                       ans += self.loss(val, x[n], y[n])
    50     59.3 MiB      0.0 MiB       22986                   ans /= size
    51
    52     59.3 MiB      0.0 MiB       22986                   return ans
    53
    54     59.3 MiB      0.0 MiB       23286               def optim_der(val):
    55                                                         # the same as upper
    56     59.3 MiB      0.0 MiB       22986                   ans = 0
    57     59.3 MiB      0.0 MiB       68958                   for n in ran:
    58     59.3 MiB      0.0 MiB       45972                       ans += self.loss_der(val, x[n], y[n])
    59     59.3 MiB      0.0 MiB       22986                   ans /= size
    60
    61     59.3 MiB      0.0 MiB       22986                   return ans
    62
    63     59.3 MiB      0.0 MiB         300               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.3 MiB      0.0 MiB         300               res.count_of_function_calls += preres.count_call_func * size
    65     59.3 MiB      0.0 MiB         300               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.3 MiB      0.0 MiB         300               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.3 MiB      0.0 MiB         300               self.weights = preres.res
    69     59.3 MiB      0.0 MiB         300               res.add_guess(self.weights)
    70     59.3 MiB      0.0 MiB         300               count += 1
    71
    72     59.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.3 MiB      0.0 MiB           1           res.success = True
    74     59.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:30:31,403] Trial 70 finished with value: 11528054401.449081 and parameters: {'l_1': 0.00048444212306547607, 'l_2': 0.008163173198770532, 'batch': 2, 'min_count': 192}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.3 MiB     59.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.3 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.3 MiB      0.0 MiB           1           count = 0
    31     59.3 MiB      0.0 MiB         380           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.3 MiB      0.0 MiB         379               if self.batch != 0:
    33     59.3 MiB      0.0 MiB         379                   ran = get_n_random(self.batch, len(x))
    34     59.3 MiB      0.0 MiB         379                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.3 MiB      0.0 MiB         379               grad = 0
    40     59.3 MiB      0.0 MiB         758               for n in ran:
    41                                                         # just like a normal derivative
    42     59.3 MiB      0.0 MiB         379                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.3 MiB      0.0 MiB         379               grad /= size
    44
    45     59.3 MiB      0.0 MiB       28535               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.3 MiB      0.0 MiB       28156                   ans = 0
    48     59.3 MiB      0.0 MiB       56312                   for n in ran:
    49     59.3 MiB      0.0 MiB       28156                       ans += self.loss(val, x[n], y[n])
    50     59.3 MiB      0.0 MiB       28156                   ans /= size
    51
    52     59.3 MiB      0.0 MiB       28156                   return ans
    53
    54     59.3 MiB      0.0 MiB       28535               def optim_der(val):
    55                                                         # the same as upper
    56     59.3 MiB      0.0 MiB       28156                   ans = 0
    57     59.3 MiB      0.0 MiB       56312                   for n in ran:
    58     59.3 MiB      0.0 MiB       28156                       ans += self.loss_der(val, x[n], y[n])
    59     59.3 MiB      0.0 MiB       28156                   ans /= size
    60
    61     59.3 MiB      0.0 MiB       28156                   return ans
    62
    63     59.3 MiB      0.0 MiB         379               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.3 MiB      0.0 MiB         379               res.count_of_function_calls += preres.count_call_func * size
    65     59.3 MiB      0.0 MiB         379               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.3 MiB      0.0 MiB         379               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.3 MiB      0.0 MiB         379               self.weights = preres.res
    69     59.3 MiB      0.0 MiB         379               res.add_guess(self.weights)
    70     59.3 MiB      0.0 MiB         379               count += 1
    71
    72     59.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.3 MiB      0.0 MiB           1           res.success = True
    74     59.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:30:42,788] Trial 71 finished with value: 7838322868.031185 and parameters: {'l_1': 0.001095140020994089, 'l_2': 0.0077793576039008145, 'batch': 1, 'min_count': 292}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.4 MiB     59.4 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.4 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.4 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.4 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.4 MiB      0.0 MiB           1           count = 0
    31     59.4 MiB      0.0 MiB         363           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.4 MiB      0.0 MiB         362               if self.batch != 0:
    33     59.4 MiB      0.0 MiB         362                   ran = get_n_random(self.batch, len(x))
    34     59.4 MiB      0.0 MiB         362                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.4 MiB      0.0 MiB         362               grad = 0
    40     59.4 MiB      0.0 MiB         724               for n in ran:
    41                                                         # just like a normal derivative
    42     59.4 MiB      0.0 MiB         362                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.4 MiB      0.0 MiB         362               grad /= size
    44
    45     59.4 MiB      0.0 MiB       26908               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.4 MiB      0.0 MiB       26546                   ans = 0
    48     59.4 MiB      0.0 MiB       53092                   for n in ran:
    49     59.4 MiB      0.0 MiB       26546                       ans += self.loss(val, x[n], y[n])
    50     59.4 MiB      0.0 MiB       26546                   ans /= size
    51
    52     59.4 MiB      0.0 MiB       26546                   return ans
    53
    54     59.4 MiB      0.0 MiB       26908               def optim_der(val):
    55                                                         # the same as upper
    56     59.4 MiB      0.0 MiB       26546                   ans = 0
    57     59.4 MiB      0.0 MiB       53092                   for n in ran:
    58     59.4 MiB      0.0 MiB       26546                       ans += self.loss_der(val, x[n], y[n])
    59     59.4 MiB      0.0 MiB       26546                   ans /= size
    60
    61     59.4 MiB      0.0 MiB       26546                   return ans
    62
    63     59.4 MiB      0.0 MiB         362               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.4 MiB      0.0 MiB         362               res.count_of_function_calls += preres.count_call_func * size
    65     59.4 MiB      0.0 MiB         362               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.4 MiB      0.0 MiB         362               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.4 MiB      0.0 MiB         362               self.weights = preres.res
    69     59.4 MiB      0.0 MiB         362               res.add_guess(self.weights)
    70     59.4 MiB      0.0 MiB         362               count += 1
    71
    72     59.4 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.4 MiB      0.0 MiB           1           res.success = True
    74     59.4 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:30:53,557] Trial 72 finished with value: 13013078455.997025 and parameters: {'l_1': 0.001095414589808609, 'l_2': 0.007764410758774646, 'batch': 1, 'min_count': 224}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.6 MiB     59.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.6 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.6 MiB      0.0 MiB           1           count = 0
    31     59.6 MiB      0.0 MiB         399           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.6 MiB      0.0 MiB         398               if self.batch != 0:
    33     59.6 MiB      0.0 MiB         398                   ran = get_n_random(self.batch, len(x))
    34     59.6 MiB      0.0 MiB         398                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.6 MiB      0.0 MiB         398               grad = 0
    40     59.6 MiB      0.0 MiB         796               for n in ran:
    41                                                         # just like a normal derivative
    42     59.6 MiB      0.0 MiB         398                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.6 MiB      0.0 MiB         398               grad /= size
    44
    45     59.6 MiB      0.0 MiB       29782               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.6 MiB      0.0 MiB       29384                   ans = 0
    48     59.6 MiB      0.0 MiB       58768                   for n in ran:
    49     59.6 MiB      0.0 MiB       29384                       ans += self.loss(val, x[n], y[n])
    50     59.6 MiB      0.0 MiB       29384                   ans /= size
    51
    52     59.6 MiB      0.0 MiB       29384                   return ans
    53
    54     59.6 MiB      0.0 MiB       29782               def optim_der(val):
    55                                                         # the same as upper
    56     59.6 MiB      0.0 MiB       29384                   ans = 0
    57     59.6 MiB      0.0 MiB       58768                   for n in ran:
    58     59.6 MiB      0.0 MiB       29384                       ans += self.loss_der(val, x[n], y[n])
    59     59.6 MiB      0.0 MiB       29384                   ans /= size
    60
    61     59.6 MiB      0.0 MiB       29384                   return ans
    62
    63     59.6 MiB      0.0 MiB         398               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.6 MiB      0.0 MiB         398               res.count_of_function_calls += preres.count_call_func * size
    65     59.6 MiB      0.0 MiB         398               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.6 MiB      0.0 MiB         398               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.6 MiB      0.0 MiB         398               self.weights = preres.res
    69     59.6 MiB      0.0 MiB         398               res.add_guess(self.weights)
    70     59.6 MiB      0.0 MiB         398               count += 1
    71
    72     59.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.6 MiB      0.0 MiB           1           res.success = True
    74     59.6 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:31:05,298] Trial 73 finished with value: 12182862382.766819 and parameters: {'l_1': 0.00021890664252671992, 'l_2': 0.008718182605778693, 'batch': 1, 'min_count': 286}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.7 MiB     59.7 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.7 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.7 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.7 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.7 MiB      0.0 MiB           1           count = 0
    31     59.7 MiB      0.0 MiB         642           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.7 MiB      0.0 MiB         641               if self.batch != 0:
    33     59.7 MiB      0.0 MiB         641                   ran = get_n_random(self.batch, len(x))
    34     59.7 MiB      0.0 MiB         641                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.7 MiB      0.0 MiB         641               grad = 0
    40     59.7 MiB      0.0 MiB        1282               for n in ran:
    41                                                         # just like a normal derivative
    42     59.7 MiB      0.0 MiB         641                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.7 MiB      0.0 MiB         641               grad /= size
    44
    45     59.7 MiB      0.0 MiB       48051               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.7 MiB      0.0 MiB       47410                   ans = 0
    48     59.7 MiB      0.0 MiB       94820                   for n in ran:
    49     59.7 MiB      0.0 MiB       47410                       ans += self.loss(val, x[n], y[n])
    50     59.7 MiB      0.0 MiB       47410                   ans /= size
    51
    52     59.7 MiB      0.0 MiB       47410                   return ans
    53
    54     59.7 MiB      0.0 MiB       48051               def optim_der(val):
    55                                                         # the same as upper
    56     59.7 MiB      0.0 MiB       47410                   ans = 0
    57     59.7 MiB      0.0 MiB       94820                   for n in ran:
    58     59.7 MiB      0.0 MiB       47410                       ans += self.loss_der(val, x[n], y[n])
    59     59.7 MiB      0.0 MiB       47410                   ans /= size
    60
    61     59.7 MiB      0.0 MiB       47410                   return ans
    62
    63     59.7 MiB      0.0 MiB         641               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.7 MiB      0.0 MiB         641               res.count_of_function_calls += preres.count_call_func * size
    65     59.7 MiB      0.0 MiB         641               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.7 MiB      0.0 MiB         641               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.7 MiB      0.0 MiB         641               self.weights = preres.res
    69     59.7 MiB      0.0 MiB         641               res.add_guess(self.weights)
    70     59.7 MiB      0.0 MiB         641               count += 1
    71
    72     59.7 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.7 MiB      0.0 MiB           1           res.success = True
    74     59.7 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:31:24,157] Trial 74 finished with value: 128486403885.2725 and parameters: {'l_1': 0.0010728095171061713, 'l_2': 0.008498098235393194, 'batch': 1, 'min_count': 329}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.7 MiB     59.7 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.7 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.7 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.7 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.7 MiB      0.0 MiB           1           count = 0
    31     59.7 MiB      0.0 MiB         428           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.7 MiB      0.0 MiB         427               if self.batch != 0:
    33     59.7 MiB      0.0 MiB         427                   ran = get_n_random(self.batch, len(x))
    34     59.7 MiB      0.0 MiB         427                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.7 MiB      0.0 MiB         427               grad = 0
    40     59.7 MiB      0.0 MiB         854               for n in ran:
    41                                                         # just like a normal derivative
    42     59.7 MiB      0.0 MiB         427                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.7 MiB      0.0 MiB         427               grad /= size
    44
    45     59.7 MiB      0.0 MiB       32285               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.7 MiB      0.0 MiB       31858                   ans = 0
    48     59.7 MiB      0.0 MiB       63716                   for n in ran:
    49     59.7 MiB      0.0 MiB       31858                       ans += self.loss(val, x[n], y[n])
    50     59.7 MiB      0.0 MiB       31858                   ans /= size
    51
    52     59.7 MiB      0.0 MiB       31858                   return ans
    53
    54     59.7 MiB      0.0 MiB       32285               def optim_der(val):
    55                                                         # the same as upper
    56     59.7 MiB      0.0 MiB       31858                   ans = 0
    57     59.7 MiB      0.0 MiB       63716                   for n in ran:
    58     59.7 MiB      0.0 MiB       31858                       ans += self.loss_der(val, x[n], y[n])
    59     59.7 MiB      0.0 MiB       31858                   ans /= size
    60
    61     59.7 MiB      0.0 MiB       31858                   return ans
    62
    63     59.7 MiB      0.0 MiB         427               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.7 MiB      0.0 MiB         427               res.count_of_function_calls += preres.count_call_func * size
    65     59.7 MiB      0.0 MiB         427               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.7 MiB      0.0 MiB         427               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.7 MiB      0.0 MiB         427               self.weights = preres.res
    69     59.7 MiB      0.0 MiB         427               res.add_guess(self.weights)
    70     59.7 MiB      0.0 MiB         427               count += 1
    71
    72     59.7 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.7 MiB      0.0 MiB           1           res.success = True
    74     59.7 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:31:36,830] Trial 75 finished with value: 5097811880.954137 and parameters: {'l_1': 0.0015895649320530388, 'l_2': 0.009994991685578627, 'batch': 1, 'min_count': 370}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.8 MiB     59.8 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.8 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.8 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.8 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.8 MiB      0.0 MiB           1           count = 0
    31     59.8 MiB      0.0 MiB         476           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.8 MiB      0.0 MiB         475               if self.batch != 0:
    33     59.8 MiB      0.0 MiB         475                   ran = get_n_random(self.batch, len(x))
    34     59.8 MiB      0.0 MiB         475                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.8 MiB      0.0 MiB         475               grad = 0
    40     59.8 MiB      0.0 MiB         950               for n in ran:
    41                                                         # just like a normal derivative
    42     59.8 MiB      0.0 MiB         475                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.8 MiB      0.0 MiB         475               grad /= size
    44
    45     59.8 MiB      0.0 MiB       35687               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.8 MiB      0.0 MiB       35212                   ans = 0
    48     59.8 MiB      0.0 MiB       70424                   for n in ran:
    49     59.8 MiB      0.0 MiB       35212                       ans += self.loss(val, x[n], y[n])
    50     59.8 MiB      0.0 MiB       35212                   ans /= size
    51
    52     59.8 MiB      0.0 MiB       35212                   return ans
    53
    54     59.8 MiB      0.0 MiB       35687               def optim_der(val):
    55                                                         # the same as upper
    56     59.8 MiB      0.0 MiB       35212                   ans = 0
    57     59.8 MiB      0.0 MiB       70424                   for n in ran:
    58     59.8 MiB      0.0 MiB       35212                       ans += self.loss_der(val, x[n], y[n])
    59     59.8 MiB      0.0 MiB       35212                   ans /= size
    60
    61     59.8 MiB      0.0 MiB       35212                   return ans
    62
    63     59.8 MiB      0.0 MiB         475               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.8 MiB      0.0 MiB         475               res.count_of_function_calls += preres.count_call_func * size
    65     59.8 MiB      0.0 MiB         475               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.8 MiB      0.0 MiB         475               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.8 MiB      0.0 MiB         475               self.weights = preres.res
    69     59.8 MiB      0.0 MiB         475               res.add_guess(self.weights)
    70     59.8 MiB      0.0 MiB         475               count += 1
    71
    72     59.8 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.8 MiB      0.0 MiB           1           res.success = True
    74     59.8 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:31:50,891] Trial 76 finished with value: 24329297449.02394 and parameters: {'l_1': 0.0016796570863750787, 'l_2': 0.009281013585153238, 'batch': 1, 'min_count': 362}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.8 MiB     59.8 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.8 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.8 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.8 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.8 MiB      0.0 MiB           1           count = 0
    31     59.8 MiB      0.0 MiB         461           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.8 MiB      0.0 MiB         460               if self.batch != 0:
    33     59.8 MiB      0.0 MiB         460                   ran = get_n_random(self.batch, len(x))
    34     59.8 MiB      0.0 MiB         460                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.8 MiB      0.0 MiB         460               grad = 0
    40     59.8 MiB      0.0 MiB        1380               for n in ran:
    41                                                         # just like a normal derivative
    42     59.8 MiB      0.0 MiB         920                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.8 MiB      0.0 MiB         460               grad /= size
    44
    45     59.8 MiB      0.0 MiB       35754               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.8 MiB      0.0 MiB       35294                   ans = 0
    48     59.8 MiB      0.0 MiB      105882                   for n in ran:
    49     59.8 MiB      0.0 MiB       70588                       ans += self.loss(val, x[n], y[n])
    50     59.8 MiB      0.0 MiB       35294                   ans /= size
    51
    52     59.8 MiB      0.0 MiB       35294                   return ans
    53
    54     59.8 MiB      0.0 MiB       35754               def optim_der(val):
    55                                                         # the same as upper
    56     59.8 MiB      0.0 MiB       35294                   ans = 0
    57     59.8 MiB      0.0 MiB      105882                   for n in ran:
    58     59.8 MiB      0.0 MiB       70588                       ans += self.loss_der(val, x[n], y[n])
    59     59.8 MiB      0.0 MiB       35294                   ans /= size
    60
    61     59.8 MiB      0.0 MiB       35294                   return ans
    62
    63     59.8 MiB      0.0 MiB         460               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.8 MiB      0.0 MiB         460               res.count_of_function_calls += preres.count_call_func * size
    65     59.8 MiB      0.0 MiB         460               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.8 MiB      0.0 MiB         460               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.8 MiB      0.0 MiB         460               self.weights = preres.res
    69     59.8 MiB      0.0 MiB         460               res.add_guess(self.weights)
    70     59.8 MiB      0.0 MiB         460               count += 1
    71
    72     59.8 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.8 MiB      0.0 MiB           1           res.success = True
    74     59.8 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:32:11,797] Trial 77 finished with value: 17892111552.377316 and parameters: {'l_1': 0.0012482284098962577, 'l_2': 0.00994521772146499, 'batch': 2, 'min_count': 397}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.9 MiB     59.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.9 MiB      0.0 MiB           1           count = 0
    31     59.9 MiB      0.0 MiB         451           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.9 MiB      0.0 MiB         450               if self.batch != 0:
    33     59.9 MiB      0.0 MiB         450                   ran = get_n_random(self.batch, len(x))
    34     59.9 MiB      0.0 MiB         450                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.9 MiB      0.0 MiB         450               grad = 0
    40     59.9 MiB      0.0 MiB         900               for n in ran:
    41                                                         # just like a normal derivative
    42     59.9 MiB      0.0 MiB         450                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.9 MiB      0.0 MiB         450               grad /= size
    44
    45     59.9 MiB      0.0 MiB       33748               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.9 MiB      0.0 MiB       33298                   ans = 0
    48     59.9 MiB      0.0 MiB       66596                   for n in ran:
    49     59.9 MiB      0.0 MiB       33298                       ans += self.loss(val, x[n], y[n])
    50     59.9 MiB      0.0 MiB       33298                   ans /= size
    51
    52     59.9 MiB      0.0 MiB       33298                   return ans
    53
    54     59.9 MiB      0.0 MiB       33748               def optim_der(val):
    55                                                         # the same as upper
    56     59.9 MiB      0.0 MiB       33298                   ans = 0
    57     59.9 MiB      0.0 MiB       66596                   for n in ran:
    58     59.9 MiB      0.0 MiB       33298                       ans += self.loss_der(val, x[n], y[n])
    59     59.9 MiB      0.0 MiB       33298                   ans /= size
    60
    61     59.9 MiB      0.0 MiB       33298                   return ans
    62
    63     59.9 MiB      0.0 MiB         450               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.9 MiB      0.0 MiB         450               res.count_of_function_calls += preres.count_call_func * size
    65     59.9 MiB      0.0 MiB         450               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.9 MiB      0.0 MiB         450               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.9 MiB      0.0 MiB         450               self.weights = preres.res
    69     59.9 MiB      0.0 MiB         450               res.add_guess(self.weights)
    70     59.9 MiB      0.0 MiB         450               count += 1
    71
    72     59.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.9 MiB      0.0 MiB           1           res.success = True
    74     59.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:32:24,997] Trial 78 finished with value: 8215708502.382947 and parameters: {'l_1': 0.0008793376738118694, 'l_2': 0.009589869659046235, 'batch': 1, 'min_count': 305}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.9 MiB     59.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.9 MiB      0.0 MiB           1           count = 0
    31     59.9 MiB      0.0 MiB         293           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.9 MiB      0.0 MiB         292               if self.batch != 0:
    33     59.9 MiB      0.0 MiB         292                   ran = get_n_random(self.batch, len(x))
    34     59.9 MiB      0.0 MiB         292                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.9 MiB      0.0 MiB         292               grad = 0
    40     59.9 MiB      0.0 MiB         876               for n in ran:
    41                                                         # just like a normal derivative
    42     59.9 MiB      0.0 MiB         584                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.9 MiB      0.0 MiB         292               grad /= size
    44
    45     59.9 MiB      0.0 MiB       22710               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.9 MiB      0.0 MiB       22418                   ans = 0
    48     59.9 MiB      0.0 MiB       67254                   for n in ran:
    49     59.9 MiB      0.0 MiB       44836                       ans += self.loss(val, x[n], y[n])
    50     59.9 MiB      0.0 MiB       22418                   ans /= size
    51
    52     59.9 MiB      0.0 MiB       22418                   return ans
    53
    54     59.9 MiB      0.0 MiB       22710               def optim_der(val):
    55                                                         # the same as upper
    56     59.9 MiB      0.0 MiB       22418                   ans = 0
    57     59.9 MiB      0.0 MiB       67254                   for n in ran:
    58     59.9 MiB      0.0 MiB       44836                       ans += self.loss_der(val, x[n], y[n])
    59     59.9 MiB      0.0 MiB       22418                   ans /= size
    60
    61     59.9 MiB      0.0 MiB       22418                   return ans
    62
    63     59.9 MiB      0.0 MiB         292               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.9 MiB      0.0 MiB         292               res.count_of_function_calls += preres.count_call_func * size
    65     59.9 MiB      0.0 MiB         292               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.9 MiB      0.0 MiB         292               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.9 MiB      0.0 MiB         292               self.weights = preres.res
    69     59.9 MiB      0.0 MiB         292               res.add_guess(self.weights)
    70     59.9 MiB      0.0 MiB         292               count += 1
    71
    72     59.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.9 MiB      0.0 MiB           1           res.success = True
    74     59.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:32:38,336] Trial 79 finished with value: 9855643234.823298 and parameters: {'l_1': 0.0006410657970035392, 'l_2': 0.00932048292799029, 'batch': 2, 'min_count': 262}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.9 MiB     59.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.9 MiB      0.0 MiB           1           count = 0
    31     59.9 MiB      0.0 MiB         335           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.9 MiB      0.0 MiB         334               if self.batch != 0:
    33     59.9 MiB      0.0 MiB         334                   ran = get_n_random(self.batch, len(x))
    34     59.9 MiB      0.0 MiB         334                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.9 MiB      0.0 MiB         334               grad = 0
    40     59.9 MiB      0.0 MiB        3006               for n in ran:
    41                                                         # just like a normal derivative
    42     59.9 MiB      0.0 MiB        2672                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.9 MiB      0.0 MiB         334               grad /= size
    44
    45     59.9 MiB      0.0 MiB       26332               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.9 MiB      0.0 MiB       25998                   ans = 0
    48     59.9 MiB      0.0 MiB      233982                   for n in ran:
    49     59.9 MiB      0.0 MiB      207984                       ans += self.loss(val, x[n], y[n])
    50     59.9 MiB      0.0 MiB       25998                   ans /= size
    51
    52     59.9 MiB      0.0 MiB       25998                   return ans
    53
    54     59.9 MiB      0.0 MiB       26332               def optim_der(val):
    55                                                         # the same as upper
    56     59.9 MiB      0.0 MiB       25998                   ans = 0
    57     59.9 MiB      0.0 MiB      233982                   for n in ran:
    58     59.9 MiB      0.0 MiB      207984                       ans += self.loss_der(val, x[n], y[n])
    59     59.9 MiB      0.0 MiB       25998                   ans /= size
    60
    61     59.9 MiB      0.0 MiB       25998                   return ans
    62
    63     59.9 MiB      0.0 MiB         334               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.9 MiB      0.0 MiB         334               res.count_of_function_calls += preres.count_call_func * size
    65     59.9 MiB      0.0 MiB         334               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.9 MiB      0.0 MiB         334               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.9 MiB      0.0 MiB         334               self.weights = preres.res
    69     59.9 MiB      0.0 MiB         334               res.add_guess(self.weights)
    70     59.9 MiB      0.0 MiB         334               count += 1
    71
    72     59.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.9 MiB      0.0 MiB           1           res.success = True
    74     59.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:33:23,531] Trial 80 finished with value: 41202004289.839676 and parameters: {'l_1': 0.0009309276507481286, 'l_2': 0.009536710036645905, 'batch': 8, 'min_count': 297}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.9 MiB     59.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.9 MiB      0.0 MiB           1           count = 0
    31     59.9 MiB      0.0 MiB         387           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.9 MiB      0.0 MiB         386               if self.batch != 0:
    33     59.9 MiB      0.0 MiB         386                   ran = get_n_random(self.batch, len(x))
    34     59.9 MiB      0.0 MiB         386                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.9 MiB      0.0 MiB         386               grad = 0
    40     59.9 MiB      0.0 MiB         772               for n in ran:
    41                                                         # just like a normal derivative
    42     59.9 MiB      0.0 MiB         386                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.9 MiB      0.0 MiB         386               grad /= size
    44
    45     59.9 MiB      0.0 MiB       28806               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.9 MiB      0.0 MiB       28420                   ans = 0
    48     59.9 MiB      0.0 MiB       56840                   for n in ran:
    49     59.9 MiB      0.0 MiB       28420                       ans += self.loss(val, x[n], y[n])
    50     59.9 MiB      0.0 MiB       28420                   ans /= size
    51
    52     59.9 MiB      0.0 MiB       28420                   return ans
    53
    54     59.9 MiB      0.0 MiB       28806               def optim_der(val):
    55                                                         # the same as upper
    56     59.9 MiB      0.0 MiB       28420                   ans = 0
    57     59.9 MiB      0.0 MiB       56840                   for n in ran:
    58     59.9 MiB      0.0 MiB       28420                       ans += self.loss_der(val, x[n], y[n])
    59     59.9 MiB      0.0 MiB       28420                   ans /= size
    60
    61     59.9 MiB      0.0 MiB       28420                   return ans
    62
    63     59.9 MiB      0.0 MiB         386               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.9 MiB      0.0 MiB         386               res.count_of_function_calls += preres.count_call_func * size
    65     59.9 MiB      0.0 MiB         386               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.9 MiB      0.0 MiB         386               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.9 MiB      0.0 MiB         386               self.weights = preres.res
    69     59.9 MiB      0.0 MiB         386               res.add_guess(self.weights)
    70     59.9 MiB      0.0 MiB         386               count += 1
    71
    72     59.9 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.9 MiB      0.0 MiB           1           res.success = True
    74     59.9 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:33:34,895] Trial 81 finished with value: 7061486240.630857 and parameters: {'l_1': 0.0013737724943704518, 'l_2': 0.009770450832143812, 'batch': 1, 'min_count': 353}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.9 MiB     59.9 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.9 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.9 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.9 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.9 MiB      0.0 MiB           1           count = 0
    31     60.0 MiB      0.0 MiB         588           while (not self.stop(res) or min_count > count) and max_count > count:
    32     60.0 MiB      0.0 MiB         587               if self.batch != 0:
    33     60.0 MiB      0.0 MiB         587                   ran = get_n_random(self.batch, len(x))
    34     60.0 MiB      0.0 MiB         587                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     60.0 MiB      0.0 MiB         587               grad = 0
    40     60.0 MiB      0.0 MiB        1174               for n in ran:
    41                                                         # just like a normal derivative
    42     60.0 MiB      0.0 MiB         587                   grad += self.loss_der(self.weights, x[n], y[n])
    43     60.0 MiB      0.0 MiB         587               grad /= size
    44
    45     60.0 MiB      0.0 MiB       43997               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     60.0 MiB      0.0 MiB       43410                   ans = 0
    48     60.0 MiB      0.0 MiB       86820                   for n in ran:
    49     60.0 MiB      0.0 MiB       43410                       ans += self.loss(val, x[n], y[n])
    50     60.0 MiB      0.0 MiB       43410                   ans /= size
    51
    52     60.0 MiB      0.0 MiB       43410                   return ans
    53
    54     60.0 MiB      0.0 MiB       43997               def optim_der(val):
    55                                                         # the same as upper
    56     60.0 MiB      0.0 MiB       43410                   ans = 0
    57     60.0 MiB      0.0 MiB       86820                   for n in ran:
    58     60.0 MiB      0.0 MiB       43410                       ans += self.loss_der(val, x[n], y[n])
    59     60.0 MiB      0.0 MiB       43410                   ans /= size
    60
    61     60.0 MiB      0.0 MiB       43410                   return ans
    62
    63     60.0 MiB      0.0 MiB         587               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     60.0 MiB      0.0 MiB         587               res.count_of_function_calls += preres.count_call_func * size
    65     60.0 MiB      0.0 MiB         587               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     60.0 MiB      0.0 MiB         587               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     60.0 MiB      0.0 MiB         587               self.weights = preres.res
    69     60.0 MiB      0.0 MiB         587               res.add_guess(self.weights)
    70     60.0 MiB      0.0 MiB         587               count += 1
    71
    72     60.0 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     60.0 MiB      0.0 MiB           1           res.success = True
    74     60.0 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:33:52,250] Trial 82 finished with value: 6621021350.0365715 and parameters: {'l_1': 0.0013000671007970697, 'l_2': 0.00973630650657404, 'batch': 1, 'min_count': 357}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     60.0 MiB     60.0 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     60.0 MiB      0.0 MiB           1           res = common.StateResult()
    27     60.0 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     60.0 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     60.0 MiB      0.0 MiB           1           count = 0
    31     60.0 MiB      0.0 MiB         653           while (not self.stop(res) or min_count > count) and max_count > count:
    32     60.0 MiB      0.0 MiB         652               if self.batch != 0:
    33     60.0 MiB      0.0 MiB         652                   ran = get_n_random(self.batch, len(x))
    34     60.0 MiB      0.0 MiB         652                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     60.0 MiB      0.0 MiB         652               grad = 0
    40     60.0 MiB      0.0 MiB        1304               for n in ran:
    41                                                         # just like a normal derivative
    42     60.0 MiB      0.0 MiB         652                   grad += self.loss_der(self.weights, x[n], y[n])
    43     60.0 MiB      0.0 MiB         652               grad /= size
    44
    45     60.0 MiB      0.0 MiB       48496               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     60.0 MiB      0.0 MiB       47844                   ans = 0
    48     60.0 MiB      0.0 MiB       95688                   for n in ran:
    49     60.0 MiB      0.0 MiB       47844                       ans += self.loss(val, x[n], y[n])
    50     60.0 MiB      0.0 MiB       47844                   ans /= size
    51
    52     60.0 MiB      0.0 MiB       47844                   return ans
    53
    54     60.0 MiB      0.0 MiB       48496               def optim_der(val):
    55                                                         # the same as upper
    56     60.0 MiB      0.0 MiB       47844                   ans = 0
    57     60.0 MiB      0.0 MiB       95688                   for n in ran:
    58     60.0 MiB      0.0 MiB       47844                       ans += self.loss_der(val, x[n], y[n])
    59     60.0 MiB      0.0 MiB       47844                   ans /= size
    60
    61     60.0 MiB      0.0 MiB       47844                   return ans
    62
    63     60.0 MiB      0.0 MiB         652               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     60.0 MiB      0.0 MiB         652               res.count_of_function_calls += preres.count_call_func * size
    65     60.0 MiB      0.0 MiB         652               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     60.0 MiB      0.0 MiB         652               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     60.0 MiB      0.0 MiB         652               self.weights = preres.res
    69     60.0 MiB      0.0 MiB         652               res.add_guess(self.weights)
    70     60.0 MiB      0.0 MiB         652               count += 1
    71
    72     60.0 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     60.0 MiB      0.0 MiB           1           res.success = True
    74     60.0 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:34:11,566] Trial 83 finished with value: 5446026677.720737 and parameters: {'l_1': 0.0013884963101507813, 'l_2': 0.009075054532360004, 'batch': 1, 'min_count': 347}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     60.0 MiB     60.0 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     60.0 MiB      0.0 MiB           1           res = common.StateResult()
    27     60.0 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     60.0 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     60.0 MiB      0.0 MiB           1           count = 0
    31     60.0 MiB      0.0 MiB         456           while (not self.stop(res) or min_count > count) and max_count > count:
    32     60.0 MiB      0.0 MiB         455               if self.batch != 0:
    33     60.0 MiB      0.0 MiB         455                   ran = get_n_random(self.batch, len(x))
    34     60.0 MiB      0.0 MiB         455                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     60.0 MiB      0.0 MiB         455               grad = 0
    40     60.0 MiB      0.0 MiB         910               for n in ran:
    41                                                         # just like a normal derivative
    42     60.0 MiB      0.0 MiB         455                   grad += self.loss_der(self.weights, x[n], y[n])
    43     60.0 MiB      0.0 MiB         455               grad /= size
    44
    45     60.0 MiB      0.0 MiB       34257               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     60.0 MiB      0.0 MiB       33802                   ans = 0
    48     60.0 MiB      0.0 MiB       67604                   for n in ran:
    49     60.0 MiB      0.0 MiB       33802                       ans += self.loss(val, x[n], y[n])
    50     60.0 MiB      0.0 MiB       33802                   ans /= size
    51
    52     60.0 MiB      0.0 MiB       33802                   return ans
    53
    54     60.0 MiB      0.0 MiB       34257               def optim_der(val):
    55                                                         # the same as upper
    56     60.0 MiB      0.0 MiB       33802                   ans = 0
    57     60.0 MiB      0.0 MiB       67604                   for n in ran:
    58     60.0 MiB      0.0 MiB       33802                       ans += self.loss_der(val, x[n], y[n])
    59     60.0 MiB      0.0 MiB       33802                   ans /= size
    60
    61     60.0 MiB      0.0 MiB       33802                   return ans
    62
    63     60.0 MiB      0.0 MiB         455               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     60.0 MiB      0.0 MiB         455               res.count_of_function_calls += preres.count_call_func * size
    65     60.0 MiB      0.0 MiB         455               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     60.0 MiB      0.0 MiB         455               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     60.0 MiB      0.0 MiB         455               self.weights = preres.res
    69     60.0 MiB      0.0 MiB         455               res.add_guess(self.weights)
    70     60.0 MiB      0.0 MiB         455               count += 1
    71
    72     60.0 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     60.0 MiB      0.0 MiB           1           res.success = True
    74     60.0 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:34:25,058] Trial 84 finished with value: 321294771951.98004 and parameters: {'l_1': 0.0014454662968018254, 'l_2': 0.009079392411078874, 'batch': 1, 'min_count': 354}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     60.1 MiB     60.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     60.1 MiB      0.0 MiB           1           res = common.StateResult()
    27     60.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     60.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     60.1 MiB      0.0 MiB           1           count = 0
    31     60.2 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32     60.2 MiB      0.0 MiB        1000               if self.batch != 0:
    33     60.2 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34     60.2 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     60.2 MiB      0.0 MiB        1000               grad = 0
    40     60.2 MiB      0.0 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42     60.2 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43     60.2 MiB      0.0 MiB        1000               grad /= size
    44
    45     60.2 MiB      0.0 MiB       74728               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     60.2 MiB      0.0 MiB       73728                   ans = 0
    48     60.2 MiB      0.0 MiB      147456                   for n in ran:
    49     60.2 MiB      0.0 MiB       73728                       ans += self.loss(val, x[n], y[n])
    50     60.2 MiB      0.0 MiB       73728                   ans /= size
    51
    52     60.2 MiB      0.0 MiB       73728                   return ans
    53
    54     60.2 MiB      0.0 MiB       74728               def optim_der(val):
    55                                                         # the same as upper
    56     60.2 MiB      0.0 MiB       73728                   ans = 0
    57     60.2 MiB      0.0 MiB      147456                   for n in ran:
    58     60.2 MiB      0.0 MiB       73728                       ans += self.loss_der(val, x[n], y[n])
    59     60.2 MiB      0.0 MiB       73728                   ans /= size
    60
    61     60.2 MiB      0.0 MiB       73728                   return ans
    62
    63     60.2 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     60.2 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65     60.2 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     60.2 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     60.2 MiB      0.0 MiB        1000               self.weights = preres.res
    69     60.2 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70     60.2 MiB      0.0 MiB        1000               count += 1
    71
    72     60.2 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     60.2 MiB      0.0 MiB           1           res.success = True
    74     60.2 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:34:54,776] Trial 85 finished with value: 203780107542.71558 and parameters: {'l_1': 0.002040735386349246, 'l_2': 0.009744947186841142, 'batch': 1, 'min_count': 396}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     60.2 MiB     60.2 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     60.2 MiB      0.0 MiB           1           res = common.StateResult()
    27     60.2 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     60.2 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     60.2 MiB      0.0 MiB           1           count = 0
    31     60.2 MiB   -451.1 MiB         635           while (not self.stop(res) or min_count > count) and max_count > count:
    32     60.2 MiB   -450.0 MiB         634               if self.batch != 0:
    33     60.2 MiB   -450.0 MiB         634                   ran = get_n_random(self.batch, len(x))
    34     60.2 MiB   -450.0 MiB         634                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     60.2 MiB   -450.0 MiB         634               grad = 0
    40     60.2 MiB  -1350.1 MiB        1902               for n in ran:
    41                                                         # just like a normal derivative
    42     60.2 MiB   -900.0 MiB        1268                   grad += self.loss_der(self.weights, x[n], y[n])
    43     60.2 MiB   -450.0 MiB         634               grad /= size
    44
    45     60.2 MiB -35268.8 MiB       49622               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     60.2 MiB -34818.8 MiB       48988                   ans = 0
    48     60.2 MiB -104456.8 MiB      146964                   for n in ran:
    49     60.2 MiB -69638.0 MiB       97976                       ans += self.loss(val, x[n], y[n])
    50     60.2 MiB -34819.0 MiB       48988                   ans /= size
    51
    52     60.2 MiB -34819.1 MiB       48988                   return ans
    53
    54     60.2 MiB -35269.7 MiB       49622               def optim_der(val):
    55                                                         # the same as upper
    56     60.2 MiB -34819.8 MiB       48988                   ans = 0
    57     60.2 MiB -104460.3 MiB      146964                   for n in ran:
    58     60.2 MiB -69640.4 MiB       97976                       ans += self.loss_der(val, x[n], y[n])
    59     60.2 MiB -34820.4 MiB       48988                   ans /= size
    60
    61     60.2 MiB -34820.4 MiB       48988                   return ans
    62
    63     60.2 MiB   -451.1 MiB         634               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     60.2 MiB   -451.1 MiB         634               res.count_of_function_calls += preres.count_call_func * size
    65     60.2 MiB   -451.1 MiB         634               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     60.2 MiB   -451.1 MiB         634               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     60.2 MiB   -451.1 MiB         634               self.weights = preres.res
    69     60.2 MiB   -451.1 MiB         634               res.add_guess(self.weights)
    70     60.2 MiB   -451.1 MiB         634               count += 1
    71
    72     59.1 MiB     -1.1 MiB           1           res.add_guess(self.weights)
    73     59.1 MiB      0.0 MiB           1           res.success = True
    74     59.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:35:25,384] Trial 86 finished with value: 22968520582.412052 and parameters: {'l_1': 0.0004370584099913325, 'l_2': 0.008843060809856884, 'batch': 2, 'min_count': 625}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.1 MiB     59.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.1 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.1 MiB      0.0 MiB           1           count = 0
    31     59.1 MiB      0.0 MiB         781           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.1 MiB      0.0 MiB         780               if self.batch != 0:
    33     59.1 MiB      0.0 MiB         780                   ran = get_n_random(self.batch, len(x))
    34     59.1 MiB      0.0 MiB         780                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.1 MiB      0.0 MiB         780               grad = 0
    40     59.1 MiB      0.0 MiB        1560               for n in ran:
    41                                                         # just like a normal derivative
    42     59.1 MiB      0.0 MiB         780                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.1 MiB      0.0 MiB         780               grad /= size
    44
    45     59.1 MiB      0.0 MiB       58590               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.1 MiB      0.0 MiB       57810                   ans = 0
    48     59.1 MiB      0.0 MiB      115620                   for n in ran:
    49     59.1 MiB      0.0 MiB       57810                       ans += self.loss(val, x[n], y[n])
    50     59.1 MiB      0.0 MiB       57810                   ans /= size
    51
    52     59.1 MiB      0.0 MiB       57810                   return ans
    53
    54     59.1 MiB      0.0 MiB       58590               def optim_der(val):
    55                                                         # the same as upper
    56     59.1 MiB      0.0 MiB       57810                   ans = 0
    57     59.1 MiB      0.0 MiB      115620                   for n in ran:
    58     59.1 MiB      0.0 MiB       57810                       ans += self.loss_der(val, x[n], y[n])
    59     59.1 MiB      0.0 MiB       57810                   ans /= size
    60
    61     59.1 MiB      0.0 MiB       57810                   return ans
    62
    63     59.1 MiB      0.0 MiB         780               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.1 MiB      0.0 MiB         780               res.count_of_function_calls += preres.count_call_func * size
    65     59.1 MiB      0.0 MiB         780               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.1 MiB      0.0 MiB         780               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.1 MiB      0.0 MiB         780               self.weights = preres.res
    69     59.1 MiB      0.0 MiB         780               res.add_guess(self.weights)
    70     59.1 MiB      0.0 MiB         780               count += 1
    71
    72     59.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.1 MiB      0.0 MiB           1           res.success = True
    74     59.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:35:50,590] Trial 87 finished with value: 7179027907.490434 and parameters: {'l_1': 6.625159767429181e-05, 'l_2': 0.008530096212014813, 'batch': 1, 'min_count': 445}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.1 MiB     59.1 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.1 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.1 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.1 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.1 MiB      0.0 MiB           1           count = 0
    31     59.1 MiB      0.0 MiB         550           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.1 MiB      0.0 MiB         549               if self.batch != 0:
    33     59.1 MiB      0.0 MiB         549                   ran = get_n_random(self.batch, len(x))
    34     59.1 MiB      0.0 MiB         549                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.1 MiB      0.0 MiB         549               grad = 0
    40     59.1 MiB      0.0 MiB        1098               for n in ran:
    41                                                         # just like a normal derivative
    42     59.1 MiB      0.0 MiB         549                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.1 MiB      0.0 MiB         549               grad /= size
    44
    45     59.1 MiB      0.0 MiB       41465               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.1 MiB      0.0 MiB       40916                   ans = 0
    48     59.1 MiB      0.0 MiB       81832                   for n in ran:
    49     59.1 MiB      0.0 MiB       40916                       ans += self.loss(val, x[n], y[n])
    50     59.1 MiB      0.0 MiB       40916                   ans /= size
    51
    52     59.1 MiB      0.0 MiB       40916                   return ans
    53
    54     59.1 MiB      0.0 MiB       41465               def optim_der(val):
    55                                                         # the same as upper
    56     59.1 MiB      0.0 MiB       40916                   ans = 0
    57     59.1 MiB      0.0 MiB       81832                   for n in ran:
    58     59.1 MiB      0.0 MiB       40916                       ans += self.loss_der(val, x[n], y[n])
    59     59.1 MiB      0.0 MiB       40916                   ans /= size
    60
    61     59.1 MiB      0.0 MiB       40916                   return ans
    62
    63     59.1 MiB      0.0 MiB         549               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.1 MiB      0.0 MiB         549               res.count_of_function_calls += preres.count_call_func * size
    65     59.1 MiB      0.0 MiB         549               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.1 MiB      0.0 MiB         549               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.1 MiB      0.0 MiB         549               self.weights = preres.res
    69     59.1 MiB      0.0 MiB         549               res.add_guess(self.weights)
    70     59.1 MiB      0.0 MiB         549               count += 1
    71
    72     59.1 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.1 MiB      0.0 MiB           1           res.success = True
    74     59.1 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:36:08,136] Trial 88 finished with value: 5924821418.86771 and parameters: {'l_1': 0.00042498370047164307, 'l_2': 0.009112030220459817, 'batch': 1, 'min_count': 436}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.2 MiB     59.2 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.2 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.2 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.2 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.2 MiB      0.0 MiB           1           count = 0
    31     59.2 MiB      0.0 MiB         457           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.2 MiB      0.0 MiB         456               if self.batch != 0:
    33     59.2 MiB      0.0 MiB         456                   ran = get_n_random(self.batch, len(x))
    34     59.2 MiB      0.0 MiB         456                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.2 MiB      0.0 MiB         456               grad = 0
    40     59.2 MiB      0.0 MiB        1368               for n in ran:
    41                                                         # just like a normal derivative
    42     59.2 MiB      0.0 MiB         912                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.2 MiB      0.0 MiB         456               grad /= size
    44
    45     59.2 MiB      0.0 MiB       35428               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.2 MiB      0.0 MiB       34972                   ans = 0
    48     59.2 MiB      0.0 MiB      104916                   for n in ran:
    49     59.2 MiB      0.0 MiB       69944                       ans += self.loss(val, x[n], y[n])
    50     59.2 MiB      0.0 MiB       34972                   ans /= size
    51
    52     59.2 MiB      0.0 MiB       34972                   return ans
    53
    54     59.2 MiB      0.0 MiB       35428               def optim_der(val):
    55                                                         # the same as upper
    56     59.2 MiB      0.0 MiB       34972                   ans = 0
    57     59.2 MiB      0.0 MiB      104916                   for n in ran:
    58     59.2 MiB      0.0 MiB       69944                       ans += self.loss_der(val, x[n], y[n])
    59     59.2 MiB      0.0 MiB       34972                   ans /= size
    60
    61     59.2 MiB      0.0 MiB       34972                   return ans
    62
    63     59.2 MiB      0.0 MiB         456               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.2 MiB      0.0 MiB         456               res.count_of_function_calls += preres.count_call_func * size
    65     59.2 MiB      0.0 MiB         456               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.2 MiB      0.0 MiB         456               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.2 MiB      0.0 MiB         456               self.weights = preres.res
    69     59.2 MiB      0.0 MiB         456               res.add_guess(self.weights)
    70     59.2 MiB      0.0 MiB         456               count += 1
    71
    72     59.2 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.2 MiB      0.0 MiB           1           res.success = True
    74     59.2 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:36:30,040] Trial 89 finished with value: 59847511517.47715 and parameters: {'l_1': 0.0001460767890656355, 'l_2': 0.009166935059015047, 'batch': 2, 'min_count': 441}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.2 MiB     59.2 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.2 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.2 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.2 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.2 MiB      0.0 MiB           1           count = 0
    31     59.2 MiB      0.0 MiB         672           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.2 MiB      0.0 MiB         671               if self.batch != 0:
    33     59.2 MiB      0.0 MiB         671                   ran = get_n_random(self.batch, len(x))
    34     59.2 MiB      0.0 MiB         671                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.2 MiB      0.0 MiB         671               grad = 0
    40     59.2 MiB      0.0 MiB        1342               for n in ran:
    41                                                         # just like a normal derivative
    42     59.2 MiB      0.0 MiB         671                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.2 MiB      0.0 MiB         671               grad /= size
    44
    45     59.2 MiB      0.0 MiB       50641               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.2 MiB      0.0 MiB       49970                   ans = 0
    48     59.2 MiB      0.0 MiB       99940                   for n in ran:
    49     59.2 MiB      0.0 MiB       49970                       ans += self.loss(val, x[n], y[n])
    50     59.2 MiB      0.0 MiB       49970                   ans /= size
    51
    52     59.2 MiB      0.0 MiB       49970                   return ans
    53
    54     59.2 MiB      0.0 MiB       50641               def optim_der(val):
    55                                                         # the same as upper
    56     59.2 MiB      0.0 MiB       49970                   ans = 0
    57     59.2 MiB      0.0 MiB       99940                   for n in ran:
    58     59.2 MiB      0.0 MiB       49970                       ans += self.loss_der(val, x[n], y[n])
    59     59.2 MiB      0.0 MiB       49970                   ans /= size
    60
    61     59.2 MiB      0.0 MiB       49970                   return ans
    62
    63     59.2 MiB      0.0 MiB         671               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.2 MiB      0.0 MiB         671               res.count_of_function_calls += preres.count_call_func * size
    65     59.2 MiB      0.0 MiB         671               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.2 MiB      0.0 MiB         671               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.2 MiB      0.0 MiB         671               self.weights = preres.res
    69     59.2 MiB      0.0 MiB         671               res.add_guess(self.weights)
    70     59.2 MiB      0.0 MiB         671               count += 1
    71
    72     59.2 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.2 MiB      0.0 MiB           1           res.success = True
    74     59.2 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:36:50,309] Trial 90 finished with value: 14825579880.82864 and parameters: {'l_1': 0.0003484907130879143, 'l_2': 0.008509840669395755, 'batch': 1, 'min_count': 557}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.2 MiB     59.2 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.2 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.2 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.2 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.2 MiB      0.0 MiB           1           count = 0
    31     59.2 MiB      0.0 MiB         512           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.2 MiB      0.0 MiB         511               if self.batch != 0:
    33     59.2 MiB      0.0 MiB         511                   ran = get_n_random(self.batch, len(x))
    34     59.2 MiB      0.0 MiB         511                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.2 MiB      0.0 MiB         511               grad = 0
    40     59.2 MiB      0.0 MiB        1022               for n in ran:
    41                                                         # just like a normal derivative
    42     59.2 MiB      0.0 MiB         511                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.2 MiB      0.0 MiB         511               grad /= size
    44
    45     59.2 MiB      0.0 MiB       38321               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.2 MiB      0.0 MiB       37810                   ans = 0
    48     59.2 MiB      0.0 MiB       75620                   for n in ran:
    49     59.2 MiB      0.0 MiB       37810                       ans += self.loss(val, x[n], y[n])
    50     59.2 MiB      0.0 MiB       37810                   ans /= size
    51
    52     59.2 MiB      0.0 MiB       37810                   return ans
    53
    54     59.2 MiB      0.0 MiB       38321               def optim_der(val):
    55                                                         # the same as upper
    56     59.2 MiB      0.0 MiB       37810                   ans = 0
    57     59.2 MiB      0.0 MiB       75620                   for n in ran:
    58     59.2 MiB      0.0 MiB       37810                       ans += self.loss_der(val, x[n], y[n])
    59     59.2 MiB      0.0 MiB       37810                   ans /= size
    60
    61     59.2 MiB      0.0 MiB       37810                   return ans
    62
    63     59.2 MiB      0.0 MiB         511               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.2 MiB      0.0 MiB         511               res.count_of_function_calls += preres.count_call_func * size
    65     59.2 MiB      0.0 MiB         511               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.2 MiB      0.0 MiB         511               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.2 MiB      0.0 MiB         511               self.weights = preres.res
    69     59.2 MiB      0.0 MiB         511               res.add_guess(self.weights)
    70     59.2 MiB      0.0 MiB         511               count += 1
    71
    72     59.2 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.2 MiB      0.0 MiB           1           res.success = True
    74     59.2 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:37:05,498] Trial 91 finished with value: 84061729402.993 and parameters: {'l_1': 0.0013163119127774271, 'l_2': 0.00948225077061295, 'batch': 1, 'min_count': 373}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.3 MiB     59.3 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.3 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.3 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.3 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.3 MiB      0.0 MiB           1           count = 0
    31     59.3 MiB      0.0 MiB         701           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.3 MiB      0.0 MiB         700               if self.batch != 0:
    33     59.3 MiB      0.0 MiB         700                   ran = get_n_random(self.batch, len(x))
    34     59.3 MiB      0.0 MiB         700                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.3 MiB      0.0 MiB         700               grad = 0
    40     59.3 MiB      0.0 MiB        1400               for n in ran:
    41                                                         # just like a normal derivative
    42     59.3 MiB      0.0 MiB         700                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.3 MiB      0.0 MiB         700               grad /= size
    44
    45     59.3 MiB      0.0 MiB       52902               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.3 MiB      0.0 MiB       52202                   ans = 0
    48     59.3 MiB      0.0 MiB      104404                   for n in ran:
    49     59.3 MiB      0.0 MiB       52202                       ans += self.loss(val, x[n], y[n])
    50     59.3 MiB      0.0 MiB       52202                   ans /= size
    51
    52     59.3 MiB      0.0 MiB       52202                   return ans
    53
    54     59.3 MiB      0.0 MiB       52902               def optim_der(val):
    55                                                         # the same as upper
    56     59.3 MiB      0.0 MiB       52202                   ans = 0
    57     59.3 MiB      0.0 MiB      104404                   for n in ran:
    58     59.3 MiB      0.0 MiB       52202                       ans += self.loss_der(val, x[n], y[n])
    59     59.3 MiB      0.0 MiB       52202                   ans /= size
    60
    61     59.3 MiB      0.0 MiB       52202                   return ans
    62
    63     59.3 MiB      0.0 MiB         700               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.3 MiB      0.0 MiB         700               res.count_of_function_calls += preres.count_call_func * size
    65     59.3 MiB      0.0 MiB         700               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.3 MiB      0.0 MiB         700               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.3 MiB      0.0 MiB         700               self.weights = preres.res
    69     59.3 MiB      0.0 MiB         700               res.add_guess(self.weights)
    70     59.3 MiB      0.0 MiB         700               count += 1
    71
    72     59.3 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.3 MiB      0.0 MiB           1           res.success = True
    74     59.3 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:37:26,353] Trial 92 finished with value: 7054085604.498283 and parameters: {'l_1': 0.0007237730428319026, 'l_2': 0.008051757189715816, 'batch': 1, 'min_count': 504}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.4 MiB     59.4 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.4 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.4 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.4 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.4 MiB      0.0 MiB           1           count = 0
    31     59.5 MiB      0.0 MiB         975           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.5 MiB      0.0 MiB         974               if self.batch != 0:
    33     59.5 MiB      0.0 MiB         974                   ran = get_n_random(self.batch, len(x))
    34     59.5 MiB      0.0 MiB         974                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.5 MiB      0.0 MiB         974               grad = 0
    40     59.5 MiB      0.0 MiB        1948               for n in ran:
    41                                                         # just like a normal derivative
    42     59.5 MiB      0.0 MiB         974                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.5 MiB      0.0 MiB         974               grad /= size
    44
    45     59.5 MiB      0.0 MiB       72228               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.5 MiB      0.0 MiB       71254                   ans = 0
    48     59.5 MiB      0.0 MiB      142508                   for n in ran:
    49     59.5 MiB      0.0 MiB       71254                       ans += self.loss(val, x[n], y[n])
    50     59.5 MiB      0.0 MiB       71254                   ans /= size
    51
    52     59.5 MiB      0.0 MiB       71254                   return ans
    53
    54     59.5 MiB      0.0 MiB       72228               def optim_der(val):
    55                                                         # the same as upper
    56     59.5 MiB      0.0 MiB       71254                   ans = 0
    57     59.5 MiB      0.0 MiB      142508                   for n in ran:
    58     59.5 MiB      0.1 MiB       71254                       ans += self.loss_der(val, x[n], y[n])
    59     59.5 MiB      0.0 MiB       71254                   ans /= size
    60
    61     59.5 MiB      0.0 MiB       71254                   return ans
    62
    63     59.5 MiB      0.0 MiB         974               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.5 MiB      0.0 MiB         974               res.count_of_function_calls += preres.count_call_func * size
    65     59.5 MiB      0.0 MiB         974               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.5 MiB      0.0 MiB         974               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.5 MiB      0.0 MiB         974               self.weights = preres.res
    69     59.5 MiB      0.0 MiB         974               res.add_guess(self.weights)
    70     59.5 MiB      0.0 MiB         974               count += 1
    71
    72     59.5 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.5 MiB      0.0 MiB           1           res.success = True
    74     59.5 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:37:54,901] Trial 93 finished with value: 76922041714.1727 and parameters: {'l_1': 0.0006028894207222883, 'l_2': 0.009789263359635056, 'batch': 1, 'min_count': 506}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.5 MiB     59.5 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.5 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.5 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.5 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.5 MiB      0.0 MiB           1           count = 0
    31     59.5 MiB      0.0 MiB         492           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.5 MiB      0.0 MiB         491               if self.batch != 0:
    33     59.5 MiB      0.0 MiB         491                   ran = get_n_random(self.batch, len(x))
    34     59.5 MiB      0.0 MiB         491                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.5 MiB      0.0 MiB         491               grad = 0
    40     59.5 MiB      0.0 MiB         982               for n in ran:
    41                                                         # just like a normal derivative
    42     59.5 MiB      0.0 MiB         491                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.5 MiB      0.0 MiB         491               grad /= size
    44
    45     59.5 MiB      0.0 MiB       37011               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.5 MiB      0.0 MiB       36520                   ans = 0
    48     59.5 MiB      0.0 MiB       73040                   for n in ran:
    49     59.5 MiB      0.0 MiB       36520                       ans += self.loss(val, x[n], y[n])
    50     59.5 MiB      0.0 MiB       36520                   ans /= size
    51
    52     59.5 MiB      0.0 MiB       36520                   return ans
    53
    54     59.5 MiB      0.0 MiB       37011               def optim_der(val):
    55                                                         # the same as upper
    56     59.5 MiB      0.0 MiB       36520                   ans = 0
    57     59.5 MiB      0.0 MiB       73040                   for n in ran:
    58     59.5 MiB      0.0 MiB       36520                       ans += self.loss_der(val, x[n], y[n])
    59     59.5 MiB      0.0 MiB       36520                   ans /= size
    60
    61     59.5 MiB      0.0 MiB       36520                   return ans
    62
    63     59.5 MiB      0.0 MiB         491               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.5 MiB      0.0 MiB         491               res.count_of_function_calls += preres.count_call_func * size
    65     59.5 MiB      0.0 MiB         491               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.5 MiB      0.0 MiB         491               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.5 MiB      0.0 MiB         491               self.weights = preres.res
    69     59.5 MiB      0.0 MiB         491               res.add_guess(self.weights)
    70     59.5 MiB      0.0 MiB         491               count += 1
    71
    72     59.5 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.5 MiB      0.0 MiB           1           res.success = True
    74     59.5 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:38:09,512] Trial 94 finished with value: 6022701260.6362505 and parameters: {'l_1': 0.00012093870979926582, 'l_2': 0.008166183307250174, 'batch': 1, 'min_count': 458}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.5 MiB     59.5 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.5 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.5 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.5 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.5 MiB      0.0 MiB           1           count = 0
    31     59.5 MiB      0.0 MiB        1001           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.5 MiB      0.0 MiB        1000               if self.batch != 0:
    33     59.5 MiB      0.0 MiB        1000                   ran = get_n_random(self.batch, len(x))
    34     59.5 MiB      0.0 MiB        1000                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.5 MiB      0.0 MiB        1000               grad = 0
    40     59.5 MiB      0.0 MiB        2000               for n in ran:
    41                                                         # just like a normal derivative
    42     59.5 MiB      0.0 MiB        1000                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.5 MiB      0.0 MiB        1000               grad /= size
    44
    45     59.5 MiB      0.0 MiB       75402               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.5 MiB      0.0 MiB       74402                   ans = 0
    48     59.5 MiB      0.0 MiB      148804                   for n in ran:
    49     59.5 MiB      0.0 MiB       74402                       ans += self.loss(val, x[n], y[n])
    50     59.5 MiB      0.0 MiB       74402                   ans /= size
    51
    52     59.5 MiB      0.0 MiB       74402                   return ans
    53
    54     59.5 MiB      0.0 MiB       75402               def optim_der(val):
    55                                                         # the same as upper
    56     59.5 MiB      0.0 MiB       74402                   ans = 0
    57     59.5 MiB      0.0 MiB      148804                   for n in ran:
    58     59.5 MiB      0.0 MiB       74402                       ans += self.loss_der(val, x[n], y[n])
    59     59.5 MiB      0.0 MiB       74402                   ans /= size
    60
    61     59.5 MiB      0.0 MiB       74402                   return ans
    62
    63     59.5 MiB      0.0 MiB        1000               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.5 MiB      0.0 MiB        1000               res.count_of_function_calls += preres.count_call_func * size
    65     59.5 MiB      0.0 MiB        1000               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.5 MiB      0.0 MiB        1000               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.5 MiB      0.0 MiB        1000               self.weights = preres.res
    69     59.5 MiB      0.0 MiB        1000               res.add_guess(self.weights)
    70     59.5 MiB      0.0 MiB        1000               count += 1
    71
    72     59.5 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.5 MiB      0.0 MiB           1           res.success = True
    74     59.5 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:38:39,413] Trial 95 finished with value: 7621382707.554024 and parameters: {'l_1': 0.0008143931329948929, 'l_2': 0.0080611730003876, 'batch': 1, 'min_count': 589}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.6 MiB     59.6 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.6 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.6 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.6 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.6 MiB      0.0 MiB           1           count = 0
    31     59.6 MiB      0.0 MiB         659           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.6 MiB      0.0 MiB         658               if self.batch != 0:
    33     59.6 MiB      0.0 MiB         658                   ran = get_n_random(self.batch, len(x))
    34     59.6 MiB      0.0 MiB         658                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.6 MiB      0.0 MiB         658               grad = 0
    40     59.6 MiB      0.0 MiB        1974               for n in ran:
    41                                                         # just like a normal derivative
    42     59.6 MiB      0.0 MiB        1316                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.6 MiB      0.0 MiB         658               grad /= size
    44
    45     59.6 MiB      0.0 MiB       51464               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.6 MiB      0.0 MiB       50806                   ans = 0
    48     59.6 MiB      0.0 MiB      152418                   for n in ran:
    49     59.6 MiB      0.0 MiB      101612                       ans += self.loss(val, x[n], y[n])
    50     59.6 MiB      0.0 MiB       50806                   ans /= size
    51
    52     59.6 MiB      0.0 MiB       50806                   return ans
    53
    54     59.6 MiB      0.0 MiB       51464               def optim_der(val):
    55                                                         # the same as upper
    56     59.6 MiB      0.0 MiB       50806                   ans = 0
    57     59.6 MiB      0.0 MiB      152418                   for n in ran:
    58     59.6 MiB      0.0 MiB      101612                       ans += self.loss_der(val, x[n], y[n])
    59     59.6 MiB      0.0 MiB       50806                   ans /= size
    60
    61     59.6 MiB      0.0 MiB       50806                   return ans
    62
    63     59.6 MiB      0.0 MiB         658               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.6 MiB      0.0 MiB         658               res.count_of_function_calls += preres.count_call_func * size
    65     59.6 MiB      0.0 MiB         658               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.6 MiB      0.0 MiB         658               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.6 MiB      0.0 MiB         658               self.weights = preres.res
    69     59.6 MiB      0.0 MiB         658               res.add_guess(self.weights)
    70     59.6 MiB      0.0 MiB         658               count += 1
    71
    72     59.6 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.6 MiB      0.0 MiB           1           res.success = True
    74     59.6 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:39:09,523] Trial 96 finished with value: 25749250427.73452 and parameters: {'l_1': 0.0016055522069978927, 'l_2': 0.00903502201104208, 'batch': 2, 'min_count': 485}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.7 MiB     59.7 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.7 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.7 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.7 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.7 MiB      0.0 MiB           1           count = 0
    31     59.7 MiB      0.0 MiB         409           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.7 MiB      0.0 MiB         408               if self.batch != 0:
    33     59.7 MiB      0.0 MiB         408                   ran = get_n_random(self.batch, len(x))
    34     59.7 MiB      0.0 MiB         408                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.7 MiB      0.0 MiB         408               grad = 0
    40     59.7 MiB      0.0 MiB         816               for n in ran:
    41                                                         # just like a normal derivative
    42     59.7 MiB      0.0 MiB         408                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.7 MiB      0.0 MiB         408               grad /= size
    44
    45     59.7 MiB      0.0 MiB       31158               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.7 MiB      0.0 MiB       30750                   ans = 0
    48     59.7 MiB      0.0 MiB       61500                   for n in ran:
    49     59.7 MiB      0.0 MiB       30750                       ans += self.loss(val, x[n], y[n])
    50     59.7 MiB      0.0 MiB       30750                   ans /= size
    51
    52     59.7 MiB      0.0 MiB       30750                   return ans
    53
    54     59.7 MiB      0.0 MiB       31158               def optim_der(val):
    55                                                         # the same as upper
    56     59.7 MiB      0.0 MiB       30750                   ans = 0
    57     59.7 MiB      0.0 MiB       61500                   for n in ran:
    58     59.7 MiB      0.0 MiB       30750                       ans += self.loss_der(val, x[n], y[n])
    59     59.7 MiB      0.0 MiB       30750                   ans /= size
    60
    61     59.7 MiB      0.0 MiB       30750                   return ans
    62
    63     59.7 MiB      0.0 MiB         408               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.7 MiB      0.0 MiB         408               res.count_of_function_calls += preres.count_call_func * size
    65     59.7 MiB      0.0 MiB         408               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.7 MiB      0.0 MiB         408               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.7 MiB      0.0 MiB         408               self.weights = preres.res
    69     59.7 MiB      0.0 MiB         408               res.add_guess(self.weights)
    70     59.7 MiB      0.0 MiB         408               count += 1
    71
    72     59.7 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.7 MiB      0.0 MiB           1           res.success = True
    74     59.7 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:39:21,891] Trial 97 finished with value: 5114092258.680867 and parameters: {'l_1': 0.00836402103387409, 'l_2': 0.00821072323204212, 'batch': 1, 'min_count': 405}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.7 MiB     59.7 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.7 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.7 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.7 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.7 MiB      0.0 MiB           1           count = 0
    31     59.7 MiB      0.0 MiB         474           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.7 MiB      0.0 MiB         473               if self.batch != 0:
    33     59.7 MiB      0.0 MiB         473                   ran = get_n_random(self.batch, len(x))
    34     59.7 MiB      0.0 MiB         473                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.7 MiB      0.0 MiB         473               grad = 0
    40     59.7 MiB      0.0 MiB        1892               for n in ran:
    41                                                         # just like a normal derivative
    42     59.7 MiB      0.0 MiB        1419                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.7 MiB      0.0 MiB         473               grad /= size
    44
    45     59.7 MiB      0.0 MiB       36973               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.7 MiB      0.0 MiB       36500                   ans = 0
    48     59.7 MiB      0.0 MiB      146000                   for n in ran:
    49     59.7 MiB      0.0 MiB      109500                       ans += self.loss(val, x[n], y[n])
    50     59.7 MiB      0.0 MiB       36500                   ans /= size
    51
    52     59.7 MiB      0.0 MiB       36500                   return ans
    53
    54     59.7 MiB      0.0 MiB       36973               def optim_der(val):
    55                                                         # the same as upper
    56     59.7 MiB      0.0 MiB       36500                   ans = 0
    57     59.7 MiB      0.0 MiB      146000                   for n in ran:
    58     59.7 MiB      0.0 MiB      109500                       ans += self.loss_der(val, x[n], y[n])
    59     59.7 MiB      0.0 MiB       36500                   ans /= size
    60
    61     59.7 MiB      0.0 MiB       36500                   return ans
    62
    63     59.7 MiB      0.0 MiB         473               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.7 MiB      0.0 MiB         473               res.count_of_function_calls += preres.count_call_func * size
    65     59.7 MiB      0.0 MiB         473               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.7 MiB      0.0 MiB         473               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.7 MiB      0.0 MiB         473               self.weights = preres.res
    69     59.7 MiB      0.0 MiB         473               res.add_guess(self.weights)
    70     59.7 MiB      0.0 MiB         473               count += 1
    71
    72     59.7 MiB      0.0 MiB           1           res.add_guess(self.weights)
    73     59.7 MiB      0.0 MiB           1           res.success = True
    74     59.7 MiB      0.0 MiB           1           return res


[I 2025-05-29 00:39:50,934] Trial 98 finished with value: 23537090856.451683 and parameters: {'l_1': 0.008303011787397889, 'l_2': 0.008177700217052239, 'batch': 3, 'min_count': 466}. Best is trial 68 with value: 3002477570.026244.
Filename: C:\Users\rinnothing\Code\pug-optimization-1\optimize\sgd.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    24     59.7 MiB     59.7 MiB           1       @profile
    25                                             def fit(self, x, y, min_count=1, max_count=100):
    26     59.7 MiB      0.0 MiB           1           res = common.StateResult()
    27     59.7 MiB      0.0 MiB           1           if len(res.guesses) == 0:
    28     59.7 MiB      0.0 MiB           1               res.add_guess(self.weights)
    29
    30     59.7 MiB      0.0 MiB           1           count = 0
    31     59.7 MiB    -33.1 MiB         574           while (not self.stop(res) or min_count > count) and max_count > count:
    32     59.7 MiB    -32.8 MiB         573               if self.batch != 0:
    33     59.7 MiB    -32.8 MiB         573                   ran = get_n_random(self.batch, len(x))
    34     59.7 MiB    -32.8 MiB         573                   size = self.batch
    35                                                     else:
    36                                                         ran = range(0, len(x))
    37                                                         size = len(x)
    38
    39     59.7 MiB    -32.8 MiB         573               grad = 0
    40     59.7 MiB    -65.6 MiB        1146               for n in ran:
    41                                                         # just like a normal derivative
    42     59.7 MiB    -32.8 MiB         573                   grad += self.loss_der(self.weights, x[n], y[n])
    43     59.7 MiB    -32.8 MiB         573               grad /= size
    44
    45     59.7 MiB  -2395.5 MiB       42759               def optim_fun(val):
    46                                                         # I don't know, maybe it's better to call it for every value
    47     59.7 MiB  -2362.8 MiB       42186                   ans = 0
    48     59.7 MiB  -4725.6 MiB       84372                   for n in ran:
    49     59.7 MiB  -2362.8 MiB       42186                       ans += self.loss(val, x[n], y[n])
    50     59.7 MiB  -2362.8 MiB       42186                   ans /= size
    51
    52     59.7 MiB  -2362.8 MiB       42186                   return ans
    53
    54     59.7 MiB  -2395.8 MiB       42759               def optim_der(val):
    55                                                         # the same as upper
    56     59.7 MiB  -2363.0 MiB       42186                   ans = 0
    57     59.7 MiB  -4726.0 MiB       84372                   for n in ran:
    58     59.7 MiB  -2363.0 MiB       42186                       ans += self.loss_der(val, x[n], y[n])
    59     59.7 MiB  -2363.0 MiB       42186                   ans /= size
    60
    61     59.7 MiB  -2363.1 MiB       42186                   return ans
    62
    63     59.7 MiB    -33.1 MiB         573               preres = self.get_next(res, optim_fun, optim_der, -grad, self.weights)
    64     59.7 MiB    -33.1 MiB         573               res.count_of_function_calls += preres.count_call_func * size
    65     59.7 MiB    -33.1 MiB         573               res.count_of_gradient_calls += preres.count_call_grad * size + size
    66     59.7 MiB    -33.1 MiB         573               res.count_of_hessian_calls += preres.count_call_hess * size
    67
    68     59.7 MiB    -33.1 MiB         573               self.weights = preres.res
    69     59.7 MiB    -33.1 MiB         573               res.add_guess(self.weights)
    70     59.7 MiB    -33.1 MiB         573               count += 1
    71
    72     59.4 MiB     -0.3 MiB           1           res.add_guess(self.weights)
    73     59.4 MiB      0.0 MiB           1           res.success = True
    74     59.4 MiB      0.0 MiB           1           return res


 : {'l_1': 0.0011155790464793206, 'l_2': 0.008327823447788912, 'batch': 1, 'min_count': 188}
  : 3002477570.026244
[I 2025-05-29 00:40:07,724] Trial 99 finished with value: 68203332094.12353 and parameters: {'l_1': 0.007761312019585125, 'l_2': 0.008373203833121741, 'batch': 1, 'min_count': 530}. Best is trial 68 with value: 3002477570.026244.

Process finished with exit code 0
